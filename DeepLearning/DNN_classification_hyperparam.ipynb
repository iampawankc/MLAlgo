{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/Users/pawankumarkc/Documents/vscode_workspace/MLAlgo/datasets/Churn_Modelling.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Exited'].value_counts()\n",
    "\n",
    "#Imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.iloc[:,3:]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.get_dummies(dataset, columns=['Geography','Gender'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the data into dep and ind variable\n",
    "\n",
    "x = dataset.drop(['Exited'], axis=1) \n",
    "y = dataset[['Exited']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the data\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE()\n",
    "x_smote, y_smote = smote.fit_resample(x, y)\n",
    "print(y.value_counts())\n",
    "print(y_smote.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_smote, y_smote, test_size=0.25, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparam tuning\n",
    "\n",
    "#1. How many no. of hidden layer we should have?\n",
    "#2. How many no. of neurons we should have in each hidden layer?\n",
    "#3. Which activation to use in each hidden layer?\n",
    "#4. What is the best learning rate?\n",
    "\n",
    "from kerastuner.tuners import RandomSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi layer preceptron\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu',input_dim=11))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    #Assign hyperparam tuning - how many hidden layer??\n",
    "    counter=0\n",
    "    for i in range(hp.Int('no_hidden_layer', min_value=1, max_value=10)):\n",
    "        if counter==0:\n",
    "            model.add(Dense(hp.Int('no_nuerons'+ str(i), min_value=8, max_value=128, step=8), \n",
    "                                   activation=hp.Choice('activation_func'+str(i), values=['relu','leaky_relu']), input_dim=11))\n",
    "            model.add(Dropout(hp.Choice(\"Dropout\"+str(i), values=[0.1,0.2,0.3,0.4,0.5])))\n",
    "        else:\n",
    "            model.add(Dense(hp.Int('no_nuerons'+ str(i), min_value=8, max_value=128, step=8), \n",
    "                        activation=hp.Choice('activation_func'+str(i), values=['relu','leaky_relu'])))\n",
    "            model.add(Dropout(hp.Choice(\"Dropout\"+str(i), values=[0.1,0.2,0.3,0.4,0.5])))\n",
    "        counter+=1\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid')) # output layer, binary class= sigmoid\n",
    "    model.compile(optimizer=hp.Choice(\"Optimizer\", values=['adam', 'rmsprop','sgd']),\n",
    "                   loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerastuner.tuners import RandomSearch\n",
    "import kerastuner as kt\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    directory='my_own_dir', project_name='HyperParam'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(x_train, y_train, epochs=5, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.get_best_hyperparameters()[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, batch_size=32, epochs=100, initial_epoch=5, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn    #neural network\n",
    "from torch.nn import functional as f    #functional api\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "#Brest cancer datset - classification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from tqdm import tqdm #Progress package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.has_mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading dataset\n",
    "\n",
    "data = load_breast_cancer()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['data']\n",
    "y = data['target']\n",
    "\n",
    "print(f\"Size x = {x.shape}\")\n",
    "print(f\"Size y =  {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(x).head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y).value_counts()  #Data is balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.25, random_state=42, stratify=y)\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparameters():\n",
    "    def __init__(self):\n",
    "        self.number_of_epochs = 50\n",
    "        self.batch_size=8\n",
    "        self.learning_rate=0.01\n",
    "        self.shuffle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train data\n",
    "\n",
    "class TrainData(Dataset):\n",
    "    def __init__(self, x_train, y_train):\n",
    "        self.x_train = torch.tensor(x_train, dtype=torch.float32)\n",
    "        self.y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_train[index], self.y_train[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_train)\n",
    "    \n",
    "train_data = TrainData(x_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test data\n",
    "\n",
    "class TestData(Dataset):\n",
    "    def __init__(self, x_test):\n",
    "        self.x_train = torch.tensor(x_test, dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_test[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_test)\n",
    "    \n",
    "test_data = TestData(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declaring the train and test dataset loaders\n",
    "\n",
    "hyperparameters = Hyperparameters()\n",
    "train_loader = DataLoader(dataset= train_data, batch_size=hyperparameters.batch_size, \n",
    "                          shuffle=hyperparameters.shuffle)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model\n",
    "\n",
    "class BrestCancerClassification(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(BrestCancerClassification, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_shape, 64)\n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the GPU\n",
    "!nvidia -smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building model\n",
    "model = BrestCancerClassification(input_shape=x_train.shape[1])\n",
    "#declaring the Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr = hyperparameters.learning_rate)\n",
    "#declaring the loss function\n",
    "criterian = nn.BCELoss()\n",
    "#moving model to device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is responsible for calculating the accuracy for a given batch\n",
    "\n",
    "def binary_acc(y_pred, y):\n",
    "    results = torch.round(y_pred)\n",
    "    correct_results_sum = (results == y).sum().float()\n",
    "    acc = correct_results_sum / y.shape[0]\n",
    "    acc = torch.round(acc*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model \n",
    "losses = []\n",
    "accuracies = []\n",
    "model.train()\n",
    "\n",
    "#putting the model in train mode\n",
    "for i in range(1, hyperparameters.number_of_epochs+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    for data in train_loader:\n",
    "        x_train, y_train = data\n",
    "        x_train = x_train.to(device)    #Moving the input features to device\n",
    "        y_train = y_train.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        #Zeroing the gradinets so that they don't accumulate the next batch\n",
    "\n",
    "        y_pred = model(x_train)\n",
    "        loss = criterian(y_pred, y_train.reshape(-1,1))\n",
    "\n",
    "        acc = binary_acc(y_pred, y_train.reshape(-1,1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_accuracy += acc.item()\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        print(f'Epoch {i+0.3}: | Loss: {epoch_loss / len(train_loader):.5f} | Acc: {epoch_accuracy/len(train_loader):.3f }')\n",
    "        losses.append(epoch_loss/len(train_loader))\n",
    "        accuracies.append(epoch_loss/len(train_loader))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the losses\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

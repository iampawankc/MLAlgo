{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK - Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happier : happy\n"
     ]
    }
   ],
   "source": [
    "#NLTK - Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#Creating lemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happier : happy\n",
      "dancing : dancing\n",
      "Happiest : happy\n"
     ]
    }
   ],
   "source": [
    "#Use lower case for lemmatization\n",
    "print('Happier :', lemmatizer.lemmatize(\"happier\", pos=\"a\"))\n",
    "print('dancing :', lemmatizer.lemmatize(\"dancing\", pos=\"a\"))\n",
    "print('Happiest :', lemmatizer.lemmatize(\"happiest\", pos=\"a\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happier : happier\n",
      "dancing : dancing\n",
      "Happiest : happiest\n"
     ]
    }
   ],
   "source": [
    "print('Happier :', lemmatizer.lemmatize(\"happier\"))\n",
    "print('dancing :', lemmatizer.lemmatize(\"dancing\"))\n",
    "print('Happiest :', lemmatizer.lemmatize(\"happiest\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK - Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "#Alternate SnowballStemmer\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helping : help\n",
      "caring : care\n"
     ]
    }
   ],
   "source": [
    "print('helping :', ps.stem(\"helping\"))\n",
    "print('caring :', ps.stem(\"caring\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy : happi\n"
     ]
    }
   ],
   "source": [
    "#Sometimes does not provide meaningful words\n",
    "print('happy :', ps.stem(\"happy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "s_stemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happi\n"
     ]
    }
   ],
   "source": [
    "print(s_stemmer.stem('happy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK - tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragrapth=\"\"\"Our founder, who comes from an automotive family, a car enthusiast by heart and an automobile engineer by qualification - Rachit Hirani has been a renowned name in the auto-industry for a while now who's goal in to simplify a buyers decision making process by providing in-depth information & experience on all cars available in the Indian auto industry. What you get in this call is a direct access to what MotorOctane's founder would have bought if he were in your shoes\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Our founder, who comes from an automotive family, a car enthusiast by heart and an automobile engineer by qualification - Rachit Hirani has been a renowned name in the auto-industry for a while now who's goal in to simplify a buyers decision making process by providing in-depth information & experience on all cars available in the Indian auto industry. What you get in this call is a direct access to what MotorOctane's founder would have bought if he were in your shoes\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragrapth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(paragrapth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Our',\n",
       " 'founder',\n",
       " ',',\n",
       " 'who',\n",
       " 'comes',\n",
       " 'from',\n",
       " 'an',\n",
       " 'automotive',\n",
       " 'family',\n",
       " ',',\n",
       " 'a',\n",
       " 'car',\n",
       " 'enthusiast',\n",
       " 'by',\n",
       " 'heart',\n",
       " 'and',\n",
       " 'an',\n",
       " 'automobile',\n",
       " 'engineer',\n",
       " 'by',\n",
       " 'qualification',\n",
       " '-',\n",
       " 'Rachit',\n",
       " 'Hirani',\n",
       " 'has',\n",
       " 'been',\n",
       " 'a',\n",
       " 'renowned',\n",
       " 'name',\n",
       " 'in',\n",
       " 'the',\n",
       " 'auto-industry',\n",
       " 'for',\n",
       " 'a',\n",
       " 'while',\n",
       " 'now',\n",
       " 'who',\n",
       " \"'s\",\n",
       " 'goal',\n",
       " 'in',\n",
       " 'to',\n",
       " 'simplify',\n",
       " 'a',\n",
       " 'buyers',\n",
       " 'decision',\n",
       " 'making',\n",
       " 'process',\n",
       " 'by',\n",
       " 'providing',\n",
       " 'in-depth',\n",
       " 'information',\n",
       " '&',\n",
       " 'experience',\n",
       " 'on',\n",
       " 'all',\n",
       " 'cars',\n",
       " 'available',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Indian',\n",
       " 'auto',\n",
       " 'industry',\n",
       " '.',\n",
       " 'What',\n",
       " 'you',\n",
       " 'get',\n",
       " 'in',\n",
       " 'this',\n",
       " 'call',\n",
       " 'is',\n",
       " 'a',\n",
       " 'direct',\n",
       " 'access',\n",
       " 'to',\n",
       " 'what',\n",
       " 'MotorOctane',\n",
       " \"'s\",\n",
       " 'founder',\n",
       " 'would',\n",
       " 'have',\n",
       " 'bought',\n",
       " 'if',\n",
       " 'he',\n",
       " 'were',\n",
       " 'in',\n",
       " 'your',\n",
       " 'shoes']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK - Tokenization (Whitespace tokenisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Our',\n",
       " 'founder,',\n",
       " 'who',\n",
       " 'comes',\n",
       " 'from',\n",
       " 'an',\n",
       " 'automotive',\n",
       " 'family,',\n",
       " 'a',\n",
       " 'car',\n",
       " 'enthusiast',\n",
       " 'by',\n",
       " 'heart',\n",
       " 'and',\n",
       " 'an',\n",
       " 'automobile',\n",
       " 'engineer',\n",
       " 'by',\n",
       " 'qualification',\n",
       " '-',\n",
       " 'Rachit',\n",
       " 'Hirani',\n",
       " 'has',\n",
       " 'been',\n",
       " 'a',\n",
       " 'renowned',\n",
       " 'name',\n",
       " 'in',\n",
       " 'the',\n",
       " 'auto-industry',\n",
       " 'for',\n",
       " 'a',\n",
       " 'while',\n",
       " 'now',\n",
       " \"who's\",\n",
       " 'goal',\n",
       " 'in',\n",
       " 'to',\n",
       " 'simplify',\n",
       " 'a',\n",
       " 'buyers',\n",
       " 'decision',\n",
       " 'making',\n",
       " 'process',\n",
       " 'by',\n",
       " 'providing',\n",
       " 'in-depth',\n",
       " 'information',\n",
       " '&',\n",
       " 'experience',\n",
       " 'on',\n",
       " 'all',\n",
       " 'cars',\n",
       " 'available',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Indian',\n",
       " 'auto',\n",
       " 'industry.',\n",
       " 'What',\n",
       " 'you',\n",
       " 'get',\n",
       " 'in',\n",
       " 'this',\n",
       " 'call',\n",
       " 'is',\n",
       " 'a',\n",
       " 'direct',\n",
       " 'access',\n",
       " 'to',\n",
       " 'what',\n",
       " \"MotorOctane's\",\n",
       " 'founder',\n",
       " 'would',\n",
       " 'have',\n",
       " 'bought',\n",
       " 'if',\n",
       " 'he',\n",
       " 'were',\n",
       " 'in',\n",
       " 'your',\n",
       " 'shoes']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For getting words without punctuations\n",
    "\n",
    "white_token = nltk.tokenize.WhitespaceTokenizer()\n",
    "white_words = white_token.tokenize(paragrapth)\n",
    "white_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK - tokenisation(Punctuation Tokenisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Our',\n",
       " 'founder',\n",
       " ',',\n",
       " 'who',\n",
       " 'comes',\n",
       " 'from',\n",
       " 'an',\n",
       " 'automotive',\n",
       " 'family',\n",
       " ',',\n",
       " 'a',\n",
       " 'car',\n",
       " 'enthusiast',\n",
       " 'by',\n",
       " 'heart',\n",
       " 'and',\n",
       " 'an',\n",
       " 'automobile',\n",
       " 'engineer',\n",
       " 'by',\n",
       " 'qualification',\n",
       " '-',\n",
       " 'Rachit',\n",
       " 'Hirani',\n",
       " 'has',\n",
       " 'been',\n",
       " 'a',\n",
       " 'renowned',\n",
       " 'name',\n",
       " 'in',\n",
       " 'the',\n",
       " 'auto',\n",
       " '-',\n",
       " 'industry',\n",
       " 'for',\n",
       " 'a',\n",
       " 'while',\n",
       " 'now',\n",
       " 'who',\n",
       " \"'\",\n",
       " 's',\n",
       " 'goal',\n",
       " 'in',\n",
       " 'to',\n",
       " 'simplify',\n",
       " 'a',\n",
       " 'buyers',\n",
       " 'decision',\n",
       " 'making',\n",
       " 'process',\n",
       " 'by',\n",
       " 'providing',\n",
       " 'in',\n",
       " '-',\n",
       " 'depth',\n",
       " 'information',\n",
       " '&',\n",
       " 'experience',\n",
       " 'on',\n",
       " 'all',\n",
       " 'cars',\n",
       " 'available',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Indian',\n",
       " 'auto',\n",
       " 'industry',\n",
       " '.',\n",
       " 'What',\n",
       " 'you',\n",
       " 'get',\n",
       " 'in',\n",
       " 'this',\n",
       " 'call',\n",
       " 'is',\n",
       " 'a',\n",
       " 'direct',\n",
       " 'access',\n",
       " 'to',\n",
       " 'what',\n",
       " 'MotorOctane',\n",
       " \"'\",\n",
       " 's',\n",
       " 'founder',\n",
       " 'would',\n",
       " 'have',\n",
       " 'bought',\n",
       " 'if',\n",
       " 'he',\n",
       " 'were',\n",
       " 'in',\n",
       " 'your',\n",
       " 'shoes']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_punc = nltk.tokenize.WordPunctTokenizer()\n",
    "newwords = word_punc.tokenize(paragrapth)\n",
    "newwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK - tokenisation(Sentence Tokenisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Our founder, who comes from an automotive family, a car enthusiast by heart and an automobile engineer by qualification - Rachit Hirani has been a renowned name in the auto-industry for a while now who's goal in to simplify a buyers decision making process by providing in-depth information & experience on all cars available in the Indian auto industry.\",\n",
       " \"What you get in this call is a direct access to what MotorOctane's founder would have bought if he were in your shoes\"]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = nltk.sent_tokenize(paragrapth)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK - Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No such file or directory: '/Users/pawankumarkc/nltk_data/corpora/stopwords/hindi'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/Users/pawankumarkc/Documents/vscode_workspace/MLAlgo/NLP/nlp_intro.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/pawankumarkc/Documents/vscode_workspace/MLAlgo/NLP/nlp_intro.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(stopwords\u001b[39m.\u001b[39;49mwords(\u001b[39m'\u001b[39;49m\u001b[39mhindi\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/nltk/corpus/reader/wordlist.py:21\u001b[0m, in \u001b[0;36mWordListCorpusReader.words\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwords\u001b[39m(\u001b[39mself\u001b[39m, fileids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, ignore_lines_startswith\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m     20\u001b[0m         line\n\u001b[0;32m---> 21\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m line_tokenize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw(fileids))\n\u001b[1;32m     22\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m line\u001b[39m.\u001b[39mstartswith(ignore_lines_startswith)\n\u001b[1;32m     23\u001b[0m     ]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/nltk/corpus/reader/api.py:218\u001b[0m, in \u001b[0;36mCorpusReader.raw\u001b[0;34m(self, fileids)\u001b[0m\n\u001b[1;32m    216\u001b[0m contents \u001b[39m=\u001b[39m []\n\u001b[1;32m    217\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m fileids:\n\u001b[0;32m--> 218\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mopen(f) \u001b[39mas\u001b[39;00m fp:\n\u001b[1;32m    219\u001b[0m         contents\u001b[39m.\u001b[39mappend(fp\u001b[39m.\u001b[39mread())\n\u001b[1;32m    220\u001b[0m \u001b[39mreturn\u001b[39;00m concat(contents)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/nltk/corpus/reader/api.py:231\u001b[0m, in \u001b[0;36mCorpusReader.open\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[39mReturn an open stream that can be used to read the given file.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[39mIf the file's encoding is not None, then the stream will\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39m:param file: The file identifier of the file to read.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    230\u001b[0m encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding(file)\n\u001b[0;32m--> 231\u001b[0m stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_root\u001b[39m.\u001b[39;49mjoin(file)\u001b[39m.\u001b[39mopen(encoding)\n\u001b[1;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m stream\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/nltk/data.py:334\u001b[0m, in \u001b[0;36mFileSystemPathPointer.join\u001b[0;34m(self, fileid)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mjoin\u001b[39m(\u001b[39mself\u001b[39m, fileid):\n\u001b[1;32m    333\u001b[0m     _path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path, fileid)\n\u001b[0;32m--> 334\u001b[0m     \u001b[39mreturn\u001b[39;00m FileSystemPathPointer(_path)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/nltk/compat.py:41\u001b[0m, in \u001b[0;36mpy3_data.<locals>._decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_decorator\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     40\u001b[0m     args \u001b[39m=\u001b[39m (args[\u001b[39m0\u001b[39m], add_py3_data(args[\u001b[39m1\u001b[39m])) \u001b[39m+\u001b[39m args[\u001b[39m2\u001b[39m:]\n\u001b[0;32m---> 41\u001b[0m     \u001b[39mreturn\u001b[39;00m init_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/nltk/data.py:312\u001b[0m, in \u001b[0;36mFileSystemPathPointer.__init__\u001b[0;34m(self, _path)\u001b[0m\n\u001b[1;32m    310\u001b[0m _path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(_path)\n\u001b[1;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(_path):\n\u001b[0;32m--> 312\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo such file or directory: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m _path)\n\u001b[1;32m    313\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path \u001b[39m=\u001b[39m _path\n",
      "\u001b[0;31mOSError\u001b[0m: No such file or directory: '/Users/pawankumarkc/nltk_data/corpora/stopwords/hindi'"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('hindi'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setmming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/pawankumarkc/Documents/vscode_workspace/MLAlgo/NLP/nlp_intro.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/pawankumarkc/Documents/vscode_workspace/MLAlgo/NLP/nlp_intro.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39;49m(nltk\u001b[39m.\u001b[39;49msent_tokenize(paragrapth)):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pawankumarkc/Documents/vscode_workspace/MLAlgo/NLP/nlp_intro.ipynb#X36sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     words \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mword_tokenize(sentence[i])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pawankumarkc/Documents/vscode_workspace/MLAlgo/NLP/nlp_intro.ipynb#X36sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     words \u001b[39m=\u001b[39m [ps\u001b[39m.\u001b[39mstem(word) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mset\u001b[39m(stopwords\u001b[39m.\u001b[39mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m))]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentence)):\n",
    "    words = nltk.word_tokenize(sentence[i])\n",
    "    words = [ps.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentence[i] = ''.join(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"ourfound,comeautomotfamili,carenthusiastheartautomobilenginqualif-rachithiranirenownnameauto-industri'sgoalsimplifibuyerdecismakeprocessprovidin-depthinform&expericaravailindianautoindustri.\",\n",
       " \"whatgetcalldirectaccessmotoroctan'sfounderwouldboughtsho\"]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK - Processing raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_input = 'Python is a programming language that is interpreted and high-level language. It is also called OOPS language'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text in lower case:  python is a programming language that is interpreted and high-level language. it is also called oops language\n"
     ]
    }
   ],
   "source": [
    "print(\"Text in lower case: \", my_input.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_input1 = \"Team A has 6 batsmen and 5 bowlers, while team b has 5 batsman and 6 bowlers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing numbers and replacing with spaces:  Team A has  batsmen and  bowlers, while team b has  batsman and  bowlers\n"
     ]
    }
   ],
   "source": [
    "# Regular expression\n",
    "import re\n",
    "#removing all digits\n",
    "output = re.sub(r\"\\d+\",\"\",my_input1)\n",
    "print('Removing numbers and replacing with spaces: ',output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sentance. having. string with. Punctuation?'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_str = \"Sentance. having. string with. Punctuation?\"\n",
    "inp_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove punctuation\n",
    "\n",
    "res = re.sub('[%s]' %re.escape(string.punctuation), '', inp_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'text':['This meal is very tasty and affordable','This meal is not tasty and is affordable','This meal is delicious and cheap','Meal is tasty and meal tastes good'],\n",
    "                   'output':[1,0,1,1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This meal is very tasty and affordable</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This meal is not tasty and is affordable</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This meal is delicious and cheap</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Meal is tasty and meal tastes good</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       text  output\n",
       "0    This meal is very tasty and affordable       1\n",
       "1  This meal is not tasty and is affordable       0\n",
       "2          This meal is delicious and cheap       1\n",
       "3        Meal is tasty and meal tastes good       1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4 entries, 0 to 3\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    4 non-null      object\n",
      " 1   output  4 non-null      int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 192.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction - Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(binary=False)  #Frequency BOW\n",
    "cv1 = CountVectorizer(binary=True)  #Binary BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW = cv.fit_transform(df['text'])\n",
    "BOW1 = cv1.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 10, 'meal': 6, 'is': 5, 'very': 11, 'tasty': 9, 'and': 1, 'affordable': 0, 'not': 7, 'delicious': 3, 'cheap': 2, 'tastes': 8, 'good': 4}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 10, 'meal': 6, 'is': 5, 'very': 11, 'tasty': 9, 'and': 1, 'affordable': 0, 'not': 7, 'delicious': 3, 'cheap': 2, 'tastes': 8, 'good': 4}\n"
     ]
    }
   ],
   "source": [
    "print(cv1.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x12 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 26 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1],\n",
       "       [1, 1, 0, 0, 0, 2, 1, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 1, 1, 2, 0, 1, 1, 0, 0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOW.toarray()\n",
    "#Here values more than 1 count can be seen, counting number of occurances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1],\n",
       "       [1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOW1.toarray()\n",
    "#Here only binary values are there, no count. Only presence and absence of words are denoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(2,2), binary=True) #Defining bi-gram\n",
    "bow = cv.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this meal': 15, 'meal is': 10, 'is very': 9, 'very tasty': 16, 'tasty and': 14, 'and affordable': 0, 'is not': 7, 'not tasty': 12, 'and is': 2, 'is affordable': 5, 'is delicious': 6, 'delicious and': 4, 'and cheap': 1, 'is tasty': 8, 'and meal': 3, 'meal tastes': 11, 'tastes good': 13}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this meal': 15, 'meal is': 10, 'is very': 9, 'very tasty': 16, 'tasty and': 14, 'and affordable': 0, 'is not': 7, 'not tasty': 12, 'and is': 2, 'is affordable': 5, 'is delicious': 6, 'delicious and': 4, 'and cheap': 1, 'is tasty': 8, 'and meal': 3, 'meal tastes': 11, 'tastes good': 13}\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(2,2), binary=True) #Defining both unigram and bi-gram\n",
    "bow = cv.fit_transform(df['text'])\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this meal is': 16, 'meal is very': 10, 'is very tasty': 6, 'very tasty and': 17, 'tasty and affordable': 13, 'meal is not': 8, 'is not tasty': 4, 'not tasty and': 12, 'tasty and is': 14, 'and is affordable': 0, 'meal is delicious': 7, 'is delicious and': 3, 'delicious and cheap': 2, 'meal is tasty': 9, 'is tasty and': 5, 'tasty and meal': 15, 'and meal tastes': 1, 'meal tastes good': 11}\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(3,3), binary=True) #Defining tri-gram\n",
    "bow = cv.fit_transform(df['text'])\n",
    "print(cv.vocabulary_)\n",
    "print(len(cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 41, 'meal': 23, 'is': 13, 'very': 44, 'tasty': 36, 'and': 1, 'affordable': 0, 'this meal': 42, 'meal is': 24, 'is very': 21, 'very tasty': 45, 'tasty and': 37, 'and affordable': 2, 'this meal is': 43, 'meal is very': 28, 'is very tasty': 22, 'very tasty and': 46, 'tasty and affordable': 38, 'not': 31, 'is not': 17, 'not tasty': 32, 'and is': 4, 'is affordable': 14, 'meal is not': 26, 'is not tasty': 18, 'not tasty and': 33, 'tasty and is': 39, 'and is affordable': 5, 'delicious': 9, 'cheap': 8, 'is delicious': 15, 'delicious and': 10, 'and cheap': 3, 'meal is delicious': 25, 'is delicious and': 16, 'delicious and cheap': 11, 'tastes': 34, 'good': 12, 'is tasty': 19, 'and meal': 6, 'meal tastes': 29, 'tastes good': 35, 'meal is tasty': 27, 'is tasty and': 20, 'tasty and meal': 40, 'and meal tastes': 7, 'meal tastes good': 30}\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,3), binary=True) #Defining unigram, bi-gram and tri-gram\n",
    "bow = cv.fit_transform(df['text'])\n",
    "print(cv.vocabulary_)\n",
    "print(len(cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 0 0 0 0 0\n",
      "  1 1 1 0 0 1 1 1 1 1 1]\n",
      " [1 1 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 1 0 0\n",
      "  1 1 0 1 0 1 1 1 0 0 0]\n",
      " [0 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 1 1 1 0 0 0]\n",
      " [0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 0 1 1\n",
      "  1 1 0 0 1 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   ...  37  38  39  40  41  42  43  \\\n",
       "0   1   1   1   0   0   0   0   0   0   0  ...   1   1   0   0   1   1   1   \n",
       "1   1   1   0   0   1   1   0   0   0   0  ...   1   0   1   0   1   1   1   \n",
       "2   0   1   0   1   0   0   0   0   1   1  ...   0   0   0   0   1   1   1   \n",
       "3   0   1   0   0   0   0   1   1   0   0  ...   1   0   0   1   0   0   0   \n",
       "\n",
       "   44  45  46  \n",
       "0   1   1   1  \n",
       "1   0   0   0  \n",
       "2   0   0   0  \n",
       "3   0   0   0  \n",
       "\n",
       "[4 rows x 47 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(bow.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'want', 'to')\n",
      "('want', 'to', 'ngramize')\n",
      "('to', 'ngramize', 'the')\n",
      "('ngramize', 'the', 'given')\n",
      "('the', 'given', 'sentences')\n"
     ]
    }
   ],
   "source": [
    "user_input = 'I want to ngramize the given sentences'\n",
    "n=3\n",
    "\n",
    "sixgrams = ngrams(sequence=user_input.split(), n=n)\n",
    "\n",
    "for ngrams in sixgrams:\n",
    "    print(ngrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'want')\n",
      "('want', 'to')\n",
      "('to', 'ngramize')\n",
      "('ngramize', 'the')\n",
      "('the', 'given')\n",
      "('given', 'sentences')\n"
     ]
    }
   ],
   "source": [
    "user_input = 'I want to ngramize the given sentences'\n",
    "n=2\n",
    "\n",
    "sixgrams = ngrams(sequence=user_input.split(), n=n)\n",
    "\n",
    "for ngrams in sixgrams:\n",
    "    print(ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"I want to ngramize the given sentences\")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input = TextBlob('I want to ngramize the given sentences')\n",
    "user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['I']),\n",
       " WordList(['want']),\n",
       " WordList(['to']),\n",
       " WordList(['ngramize']),\n",
       " WordList(['the']),\n",
       " WordList(['given']),\n",
       " WordList(['sentences'])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input.ngrams(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['I', 'want', 'to']),\n",
       " WordList(['want', 'to', 'ngramize']),\n",
       " WordList(['to', 'ngramize', 'the']),\n",
       " WordList(['ngramize', 'the', 'given']),\n",
       " WordList(['the', 'given', 'sentences'])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input.ngrams(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['I', 'want', 'to', 'ngramize']),\n",
       " WordList(['want', 'to', 'ngramize', 'the']),\n",
       " WordList(['to', 'ngramize', 'the', 'given']),\n",
       " WordList(['ngramize', 'the', 'given', 'sentences'])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input.ngrams(n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_lang = TextBlob('众所皆知，如同石油般，半导体无法逃避区域政治。美国在几年前发起针对中国半导体产业进行制裁的贸易战，几乎是在全方位展开。从2019年要求台积电等公司禁止销售晶片给中国科技龙头华为公司之后，后者手机生产线被腰斩，至今全面停摆。2022年美国施行的晶片法案，要求拥有美国绿卡及国籍者，不能在中国半导体公司中任职，引发中国强烈抗议。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"众所皆知，如同石油般，半导体无法逃避区域政治。美国在几年前发起针对中国半导体产业进行制裁的贸易战，几乎是在全方位展开。从2019年要求台积电等公司禁止销售晶片给中国科技龙头华为公司之后，后者手机生产线被腰斩，至今全面停摆。2022年美国施行的晶片法案，要求拥有美国绿卡及国籍者，不能在中国半导体公司中任职，引发中国强烈抗议。\")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chinese_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Everyone knows that like oil, semiconductor cannot escape regional politics. The United States launched a trade war on sanctions against the Chinese semiconductor industry a few years ago, which was almost fully launched. Since TSMC and other companies are required to ban the sales of chips to China technology leader Huawei in 2019, the latter ’s mobile phone production line has been cut off and has been fully stopped so far. The chip bill implemented in the United States in 2022 requires those who have American green cards and nationality, and they cannot serve among Chinese semiconductor companies, causing strong protests in China.\")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chinese_lang.translate(from_lang='zh-CN', to='en-US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"हर कोई जानता है कि तेल की तरह, अर्धचालक क्षेत्रीय राजनीति से बच नहीं सकता है। संयुक्त राज्य अमेरिका ने कुछ साल पहले चीनी सेमीकंडक्टर उद्योग के खिलाफ प्रतिबंधों पर एक व्यापार युद्ध शुरू किया था, जो लगभग पूरी तरह से शुरू किया गया था। चूंकि TSMC और अन्य कंपनियों को 2019 में चीन प्रौद्योगिकी नेता Huawei को चिप्स की बिक्री पर प्रतिबंध लगाने की आवश्यकता है, इसलिए बाद की मोबाइल फोन उत्पादन लाइन को काट दिया गया है और अब तक पूरी तरह से बंद कर दिया गया है। 2022 में संयुक्त राज्य अमेरिका में लागू चिप बिल को उन लोगों की आवश्यकता होती है जिनके पास अमेरिकी ग्रीन कार्ड और राष्ट्रीयता है, और वे चीनी अर्धचालक कंपनियों के बीच सेवा नहीं कर सकते हैं, जिससे चीन में मजबूत विरोध प्रदर्शन होता है।\")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chinese_lang.translate(from_lang='zh-CN', to='hi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have to saaay tat you hav a good knowleege in Englis\n",
      "I have to away tat you had a good knowledge in English\n"
     ]
    }
   ],
   "source": [
    "spell_checks = TextBlob(\"I have to saaay tat you hav a good knowleege in Englis\")\n",
    "print(spell_checks)\n",
    "print(spell_checks.correct())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF - Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset = extract_text('/Users/pawankumarkc/Documents/Learning/Kumar StatsML DeepLearning/Deep Learning/E-Books/2-Aurélien-Géron-Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-Tensorflow_-Concepts-Tools-and-Techniques-to-Build.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\f\fSECOND EDITION\n",
      "\n",
      "Hands-on Machine Learning with\n",
      "Scikit-Learn, Keras, and\n",
      "TensorFlow\n",
      "Concepts, Tools, and Techniques to\n",
      "Build Intelligent Systems\n",
      "\n",
      "Aurélien Géron\n",
      "\n",
      "Beijing\n",
      "Beijing\n",
      "\n",
      "Boston\n",
      "Boston\n",
      "\n",
      "Farnham Sebastopol\n",
      "Farnham Sebastopol\n",
      "\n",
      "Tokyo\n",
      "Tokyo\n",
      "\n",
      "\fHands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow\n",
      "by Aurélien Géron\n",
      "\n",
      "Copyright © 2019 Aurélien Géron. All rights reserved.\n",
      "\n",
      "Printed in the United States of America.\n",
      "\n",
      "Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n",
      "\n",
      "O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\n",
      "also available for most titles (http://oreilly.com). For more information, contact our corporate/institutional\n",
      "sales department: 800-998-9938 or corporate@oreilly.com.\n",
      "\n",
      "Editor: Nicole Tache\n",
      "Interior Designer: David Futato\n",
      "\n",
      "Cover Designer: Karen Montgomery\n",
      "Illustrator: Rebecca Demarest\n",
      "\n",
      "June 2019:\n",
      "\n",
      " Second Edition\n",
      "\n",
      "Revision History for the Early Release\n",
      "2018-11-05:  First Release\n",
      "2019-01-24:  Second Release\n",
      "2019-03-07:  Third Release\n",
      "2019-03-29:  Fourth Release\n",
      "2019-04-22:  Fifth Release\n",
      "\n",
      "See http://oreilly.com/catalog/errata.csp?isbn=9781492032649 for release details.\n",
      "\n",
      "The  O’Reilly  logo  is  a  registered  trademark  of  O’Reilly  Media,  Inc.  Hands-on  Machine  Learning  with\n",
      "Scikit-Learn, Keras, and TensorFlow, the cover image, and related trade dress are trademarks of O’Reilly\n",
      "Media, Inc.\n",
      "\n",
      "While  the  publisher  and  the  author  have  used  good  faith  efforts  to  ensure  that  the  information  and\n",
      "instructions contained in this work are accurate, the publisher and the author disclaim all responsibility\n",
      "for errors or omissions, including without limitation responsibility for damages resulting from the use of\n",
      "or reliance on this work. Use of the information and instructions contained in this work is at your own\n",
      "risk.  If  any  code  samples  or  other  technology  this  work  contains  or  describes  is  subject  to  open  source\n",
      "licenses  or  the  intellectual  property  rights  of  others,  it  is  your  responsibility  to  ensure  that  your  use\n",
      "thereof complies with such licenses and/or rights.\n",
      "\n",
      "978-1-492-03264-9\n",
      "\n",
      "[LSI]\n",
      "\n",
      "\fTable of Contents\n",
      "\n",
      "Preface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   xi\n",
      "\n",
      "Part I. \n",
      "\n",
      "The Fundamentals of Machine Learning\n",
      "\n",
      "1. The Machine Learning Landscape. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   3\n",
      "What Is Machine Learning?                                                                                            4\n",
      "Why Use Machine Learning?                                                                                          4\n",
      "Types of Machine Learning Systems                                                                              8\n",
      "Supervised/Unsupervised Learning                                                                           8\n",
      "Batch and Online Learning                                                                                       15\n",
      "Instance-Based Versus Model-Based Learning                                                      18\n",
      "Main Challenges of Machine Learning                                                                       24\n",
      "Insufficient Quantity of Training Data                                                                    24\n",
      "Nonrepresentative Training Data                                                                             26\n",
      "Poor-Quality Data                                                                                                       27\n",
      "Irrelevant Features                                                                                                      27\n",
      "Overfitting the Training Data                                                                                   28\n",
      "Underfitting the Training Data                                                                                 30\n",
      "Stepping Back                                                                                                              30\n",
      "Testing and Validating                                                                                                   31\n",
      "Hyperparameter Tuning and Model Selection                                                       32\n",
      "Data Mismatch                                                                                                            33\n",
      "Exercises                                                                                                                          34\n",
      "\n",
      "2. End-to-End Machine Learning Project. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   37\n",
      "Working with Real Data                                                                                                38\n",
      "Look at the Big Picture                                                                                                  39\n",
      "\n",
      "iii\n",
      "\n",
      "\fFrame the Problem                                                                                                     39\n",
      "Select a Performance Measure                                                                                  42\n",
      "Check the Assumptions                                                                                             45\n",
      "Get the Data                                                                                                                    45\n",
      "Create the Workspace                                                                                                 45\n",
      "Download the Data                                                                                                     49\n",
      "Take a Quick Look at the Data Structure                                                                50\n",
      "Create a Test Set                                                                                                          54\n",
      "Discover and Visualize the Data to Gain Insights                                                     58\n",
      "Visualizing Geographical Data                                                                                 59\n",
      "Looking for Correlations                                                                                           62\n",
      "Experimenting with Attribute Combinations                                                        65\n",
      "Prepare the Data for Machine Learning Algorithms                                                66\n",
      "Data Cleaning                                                                                                              67\n",
      "Handling Text and Categorical Attributes                                                              69\n",
      "Custom Transformers                                                                                                71\n",
      "Feature Scaling                                                                                                            72\n",
      "Transformation Pipelines                                                                                          73\n",
      "Select and Train a Model                                                                                               75\n",
      "Training and Evaluating on the Training Set                                                          75\n",
      "Better Evaluation Using Cross-Validation                                                              76\n",
      "Fine-Tune Your Model                                                                                                   79\n",
      "Grid Search                                                                                                                  79\n",
      "Randomized Search                                                                                                    81\n",
      "Ensemble Methods                                                                                                     82\n",
      "Analyze the Best Models and Their Errors                                                             82\n",
      "Evaluate Your System on the Test Set                                                                       83\n",
      "Launch, Monitor, and Maintain Your System                                                            84\n",
      "Try It Out!                                                                                                                        85\n",
      "Exercises                                                                                                                          85\n",
      "\n",
      "3. Classification. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   87\n",
      "MNIST                                                                                                                             87\n",
      "Training a Binary Classifier                                                                                          90\n",
      "Performance Measures                                                                                                  90\n",
      "Measuring Accuracy Using Cross-Validation                                                         91\n",
      "Confusion Matrix                                                                                                       92\n",
      "Precision and Recall                                                                                                   94\n",
      "Precision/Recall Tradeoff                                                                                           95\n",
      "The ROC Curve                                                                                                          99\n",
      "Multiclass Classification                                                                                              102\n",
      "Error Analysis                                                                                                               104\n",
      "\n",
      "iv \n",
      "\n",
      "| \n",
      "\n",
      "Table of Contents\n",
      "\n",
      "\fMultilabel Classification                                                                                              108\n",
      "Multioutput Classification                                                                                          109\n",
      "Exercises                                                                                                                        110\n",
      "\n",
      "4. Training Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  113\n",
      "Linear Regression                                                                                                         114\n",
      "The Normal Equation                                                                                              116\n",
      "Computational Complexity                                                                                     119\n",
      "Gradient Descent                                                                                                          119\n",
      "Batch Gradient Descent                                                                                           123\n",
      "Stochastic Gradient Descent                                                                                   126\n",
      "Mini-batch Gradient Descent                                                                                 129\n",
      "Polynomial Regression                                                                                                130\n",
      "Learning Curves                                                                                                           132\n",
      "Regularized Linear Models                                                                                         136\n",
      "Ridge Regression                                                                                                       137\n",
      "Lasso Regression                                                                                                       139\n",
      "Elastic Net                                                                                                                  142\n",
      "Early Stopping                                                                                                           142\n",
      "Logistic Regression                                                                                                      144\n",
      "Estimating Probabilities                                                                                           144\n",
      "Training and Cost Function                                                                                    145\n",
      "Decision Boundaries                                                                                                146\n",
      "Softmax Regression                                                                                                  149\n",
      "Exercises                                                                                                                        153\n",
      "\n",
      "5. Support Vector Machines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   155\n",
      "Linear SVM Classification                                                                                          155\n",
      "Soft Margin Classification                                                                                       156\n",
      "Nonlinear SVM Classification                                                                                    159\n",
      "Polynomial Kernel                                                                                                    160\n",
      "Adding Similarity Features                                                                                      161\n",
      "Gaussian RBF Kernel                                                                                               162\n",
      "Computational Complexity                                                                                     163\n",
      "SVM Regression                                                                                                           164\n",
      "Under the Hood                                                                                                            166\n",
      "Decision Function and Predictions                                                                        166\n",
      "Training Objective                                                                                                    167\n",
      "Quadratic Programming                                                                                         169\n",
      "The Dual Problem                                                                                                    170\n",
      "Kernelized SVM                                                                                                        171\n",
      "Online SVMs                                                                                                             174\n",
      "\n",
      "Table of Contents \n",
      "\n",
      "| \n",
      "\n",
      "v\n",
      "\n",
      "\fExercises                                                                                                                        175\n",
      "\n",
      "6. Decision Trees. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   177\n",
      "Training and Visualizing a Decision Tree                                                                 177\n",
      "Making Predictions                                                                                                      179\n",
      "Estimating Class Probabilities                                                                                    181\n",
      "The CART Training Algorithm                                                                                  182\n",
      "Computational Complexity                                                                                        183\n",
      "Gini Impurity or Entropy?                                                                                          183\n",
      "Regularization Hyperparameters                                                                               184\n",
      "Regression                                                                                                                     185\n",
      "Instability                                                                                                                       188\n",
      "Exercises                                                                                                                        189\n",
      "\n",
      "7. Ensemble Learning and Random Forests. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  191\n",
      "Voting Classifiers                                                                                                          192\n",
      "Bagging and Pasting                                                                                                     195\n",
      "Bagging and Pasting in Scikit-Learn                                                                      196\n",
      "Out-of-Bag Evaluation                                                                                             197\n",
      "Random Patches and Random Subspaces                                                                198\n",
      "Random Forests                                                                                                            199\n",
      "Extra-Trees                                                                                                                 200\n",
      "Feature Importance                                                                                                  200\n",
      "Boosting                                                                                                                         201\n",
      "AdaBoost                                                                                                                    202\n",
      "Gradient Boosting                                                                                                    205\n",
      "Stacking                                                                                                                          210\n",
      "Exercises                                                                                                                        213\n",
      "\n",
      "8. Dimensionality Reduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   215\n",
      "The Curse of Dimensionality                                                                                     216\n",
      "Main Approaches for Dimensionality Reduction                                                   218\n",
      "Projection                                                                                                                   218\n",
      "Manifold Learning                                                                                                    220\n",
      "PCA                                                                                                                                222\n",
      "Preserving the Variance                                                                                           222\n",
      "Principal Components                                                                                             223\n",
      "Projecting Down to d Dimensions                                                                         224\n",
      "Using Scikit-Learn                                                                                                    224\n",
      "Explained Variance Ratio                                                                                        225\n",
      "Choosing the Right Number of Dimensions                                                        225\n",
      "PCA for Compression                                                                                              226\n",
      "\n",
      "vi \n",
      "\n",
      "| \n",
      "\n",
      "Table of Contents\n",
      "\n",
      "\fRandomized PCA                                                                                                     227\n",
      "Incremental PCA                                                                                                      227\n",
      "Kernel PCA                                                                                                                   228\n",
      "Selecting a Kernel and Tuning Hyperparameters                                                229\n",
      "LLE                                                                                                                                 232\n",
      "Other Dimensionality Reduction Techniques                                                         234\n",
      "Exercises                                                                                                                        235\n",
      "\n",
      "9. Unsupervised Learning Techniques. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   237\n",
      "Clustering                                                                                                                      238\n",
      "K-Means                                                                                                                     240\n",
      "Limits of K-Means                                                                                                    250\n",
      "Using clustering for image segmentation                                                              251\n",
      "Using Clustering for Preprocessing                                                                       252\n",
      "Using Clustering for Semi-Supervised Learning                                                 254\n",
      "DBSCAN                                                                                                                    256\n",
      "Other Clustering Algorithms                                                                                  259\n",
      "Gaussian Mixtures                                                                                                        260\n",
      "Anomaly Detection using Gaussian Mixtures                                                      266\n",
      "Selecting the Number of Clusters                                                                           267\n",
      "Bayesian Gaussian Mixture Models                                                                       270\n",
      "Other Anomaly Detection and Novelty Detection Algorithms                         274\n",
      "\n",
      "Part II.  Neural Networks and Deep Learning\n",
      "\n",
      "10.\n",
      "\n",
      "Introduction to Artificial Neural Networks with Keras. . . . . . . . . . . . . . . . . . . . . . . . . .  277\n",
      "From Biological to Artificial Neurons                                                                       278\n",
      "Biological Neurons                                                                                                   279\n",
      "Logical Computations with Neurons                                                                     281\n",
      "The Perceptron                                                                                                          281\n",
      "Multi-Layer Perceptron and Backpropagation                                                     286\n",
      "Regression MLPs                                                                                                      289\n",
      "Classification MLPs                                                                                                  290\n",
      "Implementing MLPs with Keras                                                                                292\n",
      "Installing TensorFlow 2                                                                                           293\n",
      "Building an Image Classifier Using the Sequential API                                      294\n",
      "Building a Regression MLP Using the Sequential API                                        303\n",
      "Building Complex Models Using the Functional API                                         304\n",
      "Building Dynamic Models Using the Subclassing API                                       309\n",
      "Saving and Restoring a Model                                                                                311\n",
      "Using Callbacks                                                                                                         311\n",
      "\n",
      "Table of Contents \n",
      "\n",
      "| \n",
      "\n",
      "vii\n",
      "\n",
      "\fVisualization Using TensorBoard                                                                           313\n",
      "Fine-Tuning Neural Network Hyperparameters                                                     315\n",
      "Number of Hidden Layers                                                                                       319\n",
      "Number of Neurons per Hidden Layer                                                                 320\n",
      "Learning Rate, Batch Size and Other Hyperparameters                                     320\n",
      "Exercises                                                                                                                        322\n",
      "\n",
      "11. Training Deep Neural Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   325\n",
      "Vanishing/Exploding Gradients Problems                                                               326\n",
      "Glorot and He Initialization                                                                                    327\n",
      "Nonsaturating Activation Functions                                                                     329\n",
      "Batch Normalization                                                                                                333\n",
      "Gradient Clipping                                                                                                     338\n",
      "Reusing Pretrained Layers                                                                                          339\n",
      "Transfer Learning With Keras                                                                                341\n",
      "Unsupervised Pretraining                                                                                       343\n",
      "Pretraining on an Auxiliary Task                                                                           344\n",
      "Faster Optimizers                                                                                                         344\n",
      "Momentum Optimization                                                                                       345\n",
      "Nesterov Accelerated Gradient                                                                               346\n",
      "AdaGrad                                                                                                                     347\n",
      "RMSProp                                                                                                                    349\n",
      "Adam and Nadam Optimization                                                                            349\n",
      "Learning Rate Scheduling                                                                                        352\n",
      "Avoiding Overfitting Through Regularization                                                        356\n",
      "ℓ1 and ℓ2 Regularization                                                                                           356\n",
      "Dropout                                                                                                                      357\n",
      "Monte-Carlo (MC) Dropout                                                                                   360\n",
      "Max-Norm Regularization                                                                                      362\n",
      "Summary and Practical Guidelines                                                                           363\n",
      "Exercises                                                                                                                        364\n",
      "\n",
      "12. Custom Models and Training with TensorFlow. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   367\n",
      "A Quick Tour of TensorFlow                                                                                      368\n",
      "Using TensorFlow like NumPy                                                                                   371\n",
      "Tensors and Operations                                                                                           371\n",
      "Tensors and NumPy                                                                                                 373\n",
      "Type Conversions                                                                                                     374\n",
      "Variables                                                                                                                     374\n",
      "Other Data Structures                                                                                              375\n",
      "Customizing Models and Training Algorithms                                                       376\n",
      "Custom Loss Functions                                                                                           376\n",
      "\n",
      "viii \n",
      "\n",
      "| \n",
      "\n",
      "Table of Contents\n",
      "\n",
      "\fSaving and Loading Models That Contain Custom Components                     377\n",
      "Custom Activation Functions, Initializers, Regularizers, and Constraints      379\n",
      "Custom Metrics                                                                                                         380\n",
      "Custom Layers                                                                                                           383\n",
      "Custom Models                                                                                                         386\n",
      "Losses and Metrics Based on Model Internals                                                      388\n",
      "Computing Gradients Using Autodiff                                                                   389\n",
      "Custom Training Loops                                                                                           393\n",
      "TensorFlow Functions and Graphs                                                                            396\n",
      "Autograph and Tracing                                                                                            398\n",
      "TF Function Rules                                                                                                    400\n",
      "\n",
      "13. Loading and Preprocessing Data with TensorFlow. . . . . . . . . . . . . . . . . . . . . . . . . . . . .  403\n",
      "The Data API                                                                                                                404\n",
      "Chaining Transformations                                                                                      405\n",
      "Shuffling the Data                                                                                                     406\n",
      "Preprocessing the Data                                                                                            409\n",
      "Putting Everything Together                                                                                   410\n",
      "Prefetching                                                                                                                 411\n",
      "Using the Dataset With tf.keras                                                                              413\n",
      "The TFRecord Format                                                                                                 414\n",
      "Compressed TFRecord Files                                                                                   415\n",
      "A Brief Introduction to Protocol Buffers                                                              415\n",
      "TensorFlow Protobufs                                                                                              416\n",
      "Loading and Parsing Examples                                                                               418\n",
      "Handling Lists of Lists Using the SequenceExample Protobuf                          419\n",
      "The Features API                                                                                                          420\n",
      "Categorical Features                                                                                                 421\n",
      "Crossed Categorical Features                                                                                  421\n",
      "Encoding Categorical Features Using One-Hot Vectors                                     422\n",
      "Encoding Categorical Features Using Embeddings                                             423\n",
      "Using Feature Columns for Parsing                                                                       426\n",
      "Using Feature Columns in Your Models                                                               426\n",
      "TF Transform                                                                                                                428\n",
      "The TensorFlow Datasets (TFDS) Project                                                                429\n",
      "\n",
      "14. Deep Computer Vision Using Convolutional Neural Networks. . . . . . . . . . . . . . . . . . .   431\n",
      "The Architecture of the Visual Cortex                                                                      432\n",
      "Convolutional Layer                                                                                                    434\n",
      "Filters                                                                                                                          436\n",
      "Stacking Multiple Feature Maps                                                                             437\n",
      "TensorFlow Implementation                                                                                   439\n",
      "\n",
      "Table of Contents \n",
      "\n",
      "| \n",
      "\n",
      "ix\n",
      "\n",
      "\fMemory Requirements                                                                                            441\n",
      "Pooling Layer                                                                                                                442\n",
      "TensorFlow Implementation                                                                                   444\n",
      "CNN Architectures                                                                                                      446\n",
      "LeNet-5                                                                                                                       449\n",
      "AlexNet                                                                                                                      450\n",
      "GoogLeNet                                                                                                                 452\n",
      "VGGNet                                                                                                                     456\n",
      "ResNet                                                                                                                        457\n",
      "Xception                                                                                                                     459\n",
      "SENet                                                                                                                          461\n",
      "Implementing a ResNet-34 CNN Using Keras                                                        464\n",
      "Using Pretrained Models From Keras                                                                       465\n",
      "Pretrained Models for Transfer Learning                                                                 467\n",
      "Classification and Localization                                                                                   469\n",
      "Object Detection                                                                                                          471\n",
      "Fully Convolutional Networks (FCNs)                                                                  473\n",
      "You Only Look Once (YOLO)                                                                                475\n",
      "Semantic Segmentation                                                                                               478\n",
      "Exercises                                                                                                                        482\n",
      "\n",
      "x \n",
      "\n",
      "| \n",
      "\n",
      "Table of Contents\n",
      "\n",
      "\fPreface\n",
      "\n",
      "The Machine Learning Tsunami\n",
      "In 2006, Geoffrey Hinton et al. published a paper1 showing how to train a deep neural\n",
      "network  capable  of  recognizing  handwritten  digits  with  state-of-the-art  precision\n",
      "(>98%).  They  branded  this  technique  “Deep  Learning.”  Training  a  deep  neural  net\n",
      "was widely considered impossible at the time,2 and most researchers had abandoned\n",
      "the idea since the 1990s. This paper revived the interest of the scientific community\n",
      "and  before  long  many  new  papers  demonstrated  that  Deep  Learning  was  not  only\n",
      "possible, but capable of mind-blowing achievements that no other Machine Learning\n",
      "(ML) technique could hope to match (with the help of tremendous computing power\n",
      "and great amounts of data). This enthusiasm soon extended to many other areas of\n",
      "Machine Learning.\n",
      "\n",
      "Fast-forward 10 years and Machine Learning has conquered the industry: it is now at\n",
      "the  heart  of  much  of  the  magic  in  today’s  high-tech  products,  ranking  your  web\n",
      "search results, powering your smartphone’s speech recognition, recommending vid‐\n",
      "eos, and beating the world champion at the game of Go. Before you know it, it will be\n",
      "driving your car.\n",
      "\n",
      "Machine Learning in Your Projects\n",
      "So naturally you are excited about Machine Learning and you would love to join the\n",
      "party!\n",
      "\n",
      "Perhaps you would like to give your homemade robot a brain of its own? Make it rec‐\n",
      "ognize faces? Or learn to walk around?\n",
      "\n",
      "1 Available on Hinton’s home page at http://www.cs.toronto.edu/~hinton/.\n",
      "\n",
      "2 Despite the fact that Yann Lecun’s deep convolutional neural networks had worked well for image recognition\n",
      "\n",
      "since the 1990s, although they were not as general purpose.\n",
      "\n",
      "xi\n",
      "\n",
      "\fOr maybe your company has tons of data (user logs, financial data, production data,\n",
      "machine sensor data, hotline stats, HR reports, etc.), and more than likely you could\n",
      "unearth some hidden gems if you just knew where to look; for example:\n",
      "\n",
      "• Segment customers and find the best marketing strategy for each group\n",
      "\n",
      "• Recommend products for each client based on what similar clients bought\n",
      "\n",
      "• Detect which transactions are likely to be fraudulent\n",
      "\n",
      "• Forecast next year’s revenue\n",
      "\n",
      "• And more\n",
      "\n",
      "Whatever the reason, you have decided to learn Machine Learning and implement it\n",
      "in your projects. Great idea!\n",
      "\n",
      "Objective and Approach\n",
      "This book assumes that you know close to nothing about Machine Learning. Its goal\n",
      "is to give you the concepts, the intuitions, and the tools you need to actually imple‐\n",
      "ment programs capable of learning from data.\n",
      "\n",
      "We will cover a large number of techniques, from the simplest and most commonly\n",
      "used (such as linear regression) to some of the Deep Learning techniques that regu‐\n",
      "larly win competitions.\n",
      "\n",
      "Rather than implementing our own toy versions of each algorithm, we will be using\n",
      "actual production-ready Python frameworks:\n",
      "\n",
      "• Scikit-Learn is very easy to use, yet it implements many Machine Learning algo‐\n",
      "rithms efficiently, so it makes for a great entry point to learn Machine Learning.\n",
      "\n",
      "• TensorFlow is a more complex library for distributed numerical computation. It\n",
      "makes it possible to train and run very large neural networks efficiently by dis‐\n",
      "tributing  the  computations  across  potentially  hundreds  of  multi-GPU  servers.\n",
      "TensorFlow  was  created  at  Google  and  supports  many  of  their  large-scale\n",
      "Machine Learning applications. It was open sourced in November 2015.\n",
      "\n",
      "• Keras  is  a  high  level  Deep  Learning  API  that  makes  it  very  simple  to  train  and\n",
      "run neural networks. It can run on top of either TensorFlow, Theano or Micro‐\n",
      "soft  Cognitive  Toolkit  (formerly  known  as  CNTK).  TensorFlow  comes  with  its\n",
      "own implementation of this API, called tf.keras, which provides support for some\n",
      "advanced TensorFlow features (e.g., to efficiently load data).\n",
      "\n",
      "The  book  favors  a  hands-on  approach,  growing  an  intuitive  understanding  of\n",
      "Machine Learning through concrete working examples and just a little bit of theory.\n",
      "While you can read this book without picking up your laptop, we highly recommend\n",
      "\n",
      "xii \n",
      "\n",
      "|  Preface\n",
      "\n",
      "\fyou  experiment  with  the  code  examples  available  online  as  Jupyter  notebooks  at\n",
      "https://github.com/ageron/handson-ml2.\n",
      "\n",
      "Prerequisites\n",
      "This book assumes that you have some Python programming experience and that you\n",
      "are familiar with Python’s main scientific libraries, in particular NumPy, Pandas, and\n",
      "Matplotlib.\n",
      "\n",
      "Also, if you care about what’s under the hood you should have a reasonable under‐\n",
      "standing of college-level math as well (calculus, linear algebra, probabilities, and sta‐\n",
      "tistics).\n",
      "\n",
      "If you don’t know Python yet, http://learnpython.org/ is a great place to start. The offi‐\n",
      "cial tutorial on python.org is also quite good.\n",
      "\n",
      "If you have never used Jupyter, Chapter 2 will guide you through installation and the\n",
      "basics: it is a great tool to have in your toolbox.\n",
      "\n",
      "If  you  are  not  familiar  with  Python’s  scientific  libraries,  the  provided  Jupyter  note‐\n",
      "books include a few tutorials. There is also a quick math tutorial for linear algebra.\n",
      "\n",
      "Roadmap\n",
      "This book is organized in two parts. Part I, The Fundamentals of Machine Learning,\n",
      "covers the following topics:\n",
      "\n",
      "• What  is  Machine  Learning?  What  problems  does  it  try  to  solve?  What  are  the\n",
      "\n",
      "main categories and fundamental concepts of Machine Learning systems?\n",
      "\n",
      "• The main steps in a typical Machine Learning project.\n",
      "\n",
      "• Learning by fitting a model to data.\n",
      "\n",
      "• Optimizing a cost function.\n",
      "\n",
      "• Handling, cleaning, and preparing data.\n",
      "\n",
      "• Selecting and engineering features.\n",
      "\n",
      "• Selecting a model and tuning hyperparameters using cross-validation.\n",
      "\n",
      "• The main challenges of Machine Learning, in particular underfitting and overfit‐\n",
      "\n",
      "ting (the bias/variance tradeoff).\n",
      "\n",
      "• Reducing the dimensionality of the training data to fight the curse of dimension‐\n",
      "\n",
      "ality.\n",
      "\n",
      "• Other unsupervised learning techniques, including clustering, density estimation\n",
      "\n",
      "and anomaly detection.\n",
      "\n",
      "Preface \n",
      "\n",
      "| \n",
      "\n",
      "xiii\n",
      "\n",
      "\f• The  most  common  learning  algorithms:  Linear  and  Polynomial  Regression,\n",
      "Logistic  Regression,  k-Nearest  Neighbors,  Support  Vector  Machines,  Decision\n",
      "Trees, Random Forests, and Ensemble methods.\n",
      "\n",
      "xiv \n",
      "\n",
      "|  Preface\n",
      "\n",
      "\fPart II, Neural Networks and Deep Learning, covers the following topics:\n",
      "\n",
      "• What are neural nets? What are they good for?\n",
      "\n",
      "• Building and training neural nets using TensorFlow and Keras.\n",
      "\n",
      "• The most important neural net architectures: feedforward neural nets, convolu‐\n",
      "tional nets, recurrent nets, long short-term memory (LSTM) nets, autoencoders\n",
      "and generative adversarial networks (GANs).\n",
      "\n",
      "• Techniques for training deep neural nets.\n",
      "\n",
      "• Scaling neural networks for large datasets.\n",
      "\n",
      "• Learning strategies with Reinforcement Learning.\n",
      "\n",
      "• Handling uncertainty with Bayesian Deep Learning.\n",
      "\n",
      "The first part is based mostly on Scikit-Learn while the second part uses TensorFlow\n",
      "and Keras.\n",
      "\n",
      "Don’t jump into deep waters too hastily: while Deep Learning is no\n",
      "doubt  one  of  the  most  exciting  areas  in  Machine  Learning,  you\n",
      "should  master  the  fundamentals  first.  Moreover,  most  problems\n",
      "can be solved quite well using simpler techniques such as Random\n",
      "Forests and Ensemble methods (discussed in Part I). Deep Learn‐\n",
      "ing is best suited for complex problems such as image recognition,\n",
      "speech  recognition,  or  natural  language  processing,  provided  you\n",
      "have enough data, computing power, and patience.\n",
      "\n",
      "Other Resources\n",
      "Many  resources  are  available  to  learn  about  Machine  Learning.  Andrew  Ng’s  ML\n",
      "course  on  Coursera  and  Geoffrey  Hinton’s  course  on  neural  networks  and  Deep\n",
      "Learning  are  amazing,  although  they  both  require  a  significant  time  investment\n",
      "(think months).\n",
      "\n",
      "There  are  also  many  interesting  websites  about  Machine  Learning,  including  of\n",
      "course  Scikit-Learn’s  exceptional  User  Guide.  You  may  also  enjoy  Dataquest,  which\n",
      "provides very nice interactive tutorials, and ML blogs such as those listed on Quora.\n",
      "Finally, the Deep Learning website has a good list of resources to learn more.\n",
      "\n",
      "Of course there are also many other introductory books about Machine Learning, in\n",
      "particular:\n",
      "\n",
      "• Joel  Grus,  Data  Science  from  Scratch  (O’Reilly).  This  book  presents  the  funda‐\n",
      "mentals  of  Machine  Learning,  and  implements  some  of  the  main  algorithms  in\n",
      "pure Python (from scratch, as the name suggests).\n",
      "\n",
      "Preface \n",
      "\n",
      "| \n",
      "\n",
      "xv\n",
      "\n",
      "\f• Stephen Marsland, Machine Learning: An Algorithmic Perspective (Chapman and\n",
      "Hall).  This  book  is  a  great  introduction  to  Machine  Learning,  covering  a  wide\n",
      "range  of  topics  in  depth,  with  code  examples  in  Python  (also  from  scratch,  but\n",
      "using NumPy).\n",
      "\n",
      "• Sebastian  Raschka,  Python  Machine  Learning  (Packt  Publishing).  Also  a  great\n",
      "introduction to Machine Learning, this book leverages Python open source libra‐\n",
      "ries (Pylearn 2 and Theano).\n",
      "\n",
      "• François  Chollet,  Deep  Learning  with  Python  (Manning).  A  very  practical  book\n",
      "that covers a large range of topics in a clear and concise way, as you might expect\n",
      "from the author of the excellent Keras library. It favors code examples over math‐\n",
      "ematical theory.\n",
      "\n",
      "• Yaser S. Abu-Mostafa, Malik Magdon-Ismail, and Hsuan-Tien Lin, Learning from\n",
      "Data (AMLBook). A rather theoretical approach to ML, this book provides deep\n",
      "insights, in particular on the bias/variance tradeoff (see Chapter 4).\n",
      "\n",
      "• Stuart  Russell  and  Peter  Norvig,  Artificial  Intelligence:  A  Modern  Approach,  3rd\n",
      "Edition (Pearson). This is a great (and huge) book covering an incredible amount\n",
      "of topics, including Machine Learning. It helps put ML into perspective.\n",
      "\n",
      "Finally, a great way to learn is to join ML competition websites such as Kaggle.com\n",
      "this  will  allow  you  to  practice  your  skills  on  real-world  problems,  with  help  and\n",
      "insights from some of the best ML professionals out there.\n",
      "\n",
      "Conventions Used in This Book\n",
      "The following typographical conventions are used in this book:\n",
      "\n",
      "Italic\n",
      "\n",
      "Indicates new terms, URLs, email addresses, filenames, and file extensions.\n",
      "\n",
      "Constant width\n",
      "\n",
      "Used for program listings, as well as within paragraphs to refer to program ele‐\n",
      "ments  such  as  variable  or  function  names,  databases,  data  types,  environment\n",
      "variables, statements and keywords.\n",
      "\n",
      "Constant width bold\n",
      "\n",
      "Shows commands or other text that should be typed literally by the user.\n",
      "\n",
      "Constant width italic\n",
      "\n",
      "Shows text that should be replaced with user-supplied values or by values deter‐\n",
      "mined by context.\n",
      "\n",
      "xvi \n",
      "\n",
      "|  Preface\n",
      "\n",
      "\fThis element signifies a tip or suggestion.\n",
      "\n",
      "This element signifies a general note.\n",
      "\n",
      "This element indicates a warning or caution.\n",
      "\n",
      "Code Examples\n",
      "Supplemental  material  (code  examples,  exercises,  etc.)  is  available  for  download  at\n",
      "https://github.com/ageron/handson-ml2. It is mostly composed of Jupyter notebooks.\n",
      "\n",
      "Some of the code examples in the book leave out some repetitive sections, or details\n",
      "that  are  obvious  or  unrelated  to  Machine  Learning.  This  keeps  the  focus  on  the\n",
      "important parts of the code, and it saves space to cover more topics. However, if you\n",
      "want the full code examples, they are all available in the Jupyter notebooks.\n",
      "\n",
      "Note  that  when  the  code  examples  display  some  outputs,  then  these  code  examples\n",
      "are shown with Python prompts (>>> and ...), as in a Python shell, to clearly distin‐\n",
      "guish the code from the outputs. For example, this code defines the square() func‐\n",
      "tion then it computes and displays the square of 3:\n",
      "\n",
      ">>> def square(x):\n",
      "...     return x ** 2\n",
      "...\n",
      ">>> result = square(3)\n",
      ">>> result\n",
      "9\n",
      "\n",
      "When code does not display anything, prompts are not used. However, the result may\n",
      "sometimes be shown as a comment like this:\n",
      "\n",
      "def square(x):\n",
      "    return x ** 2\n",
      "\n",
      "result = square(3)  # result is 9\n",
      "\n",
      "Preface \n",
      "\n",
      "| \n",
      "\n",
      "xvii\n",
      "\n",
      "\fUsing Code Examples\n",
      "This book is here to help you get your job done. In general, if example code is offered\n",
      "with  this  book,  you  may  use  it  in  your  programs  and  documentation.  You  do  not\n",
      "need to contact us for permission unless you’re reproducing a significant portion of\n",
      "the code. For example, writing a program that uses several chunks of code from this\n",
      "book  does  not  require  permission.  Selling  or  distributing  a  CD-ROM  of  examples\n",
      "from  O’Reilly  books  does  require  permission.  Answering  a  question  by  citing  this\n",
      "book and quoting example code does not require permission. Incorporating a signifi‐\n",
      "cant amount of example code from this book into your product’s documentation does\n",
      "require permission.\n",
      "\n",
      "We  appreciate,  but  do  not  require,  attribution.  An  attribution  usually  includes  the\n",
      "title,  author,  publisher,  and  ISBN.  For  example:  “Hands-On  Machine  Learning  with\n",
      "Scikit-Learn,  Keras  and  TensorFlow  by  Aurélien  Géron  (O’Reilly).  Copyright  2019\n",
      "Aurélien Géron, 978-1-492-03264-9.” If you feel your use of code examples falls out‐\n",
      "side  fair  use  or  the  permission  given  above,  feel  free  to  contact  us  at  permis‐\n",
      "sions@oreilly.com.\n",
      "\n",
      "O’Reilly Safari\n",
      "\n",
      "Safari (formerly Safari Books Online) is a membership-based\n",
      "training  and  reference  platform  for  enterprise,  government,\n",
      "educators, and individuals.\n",
      "\n",
      "Members have access to thousands of books, training videos, Learning Paths, interac‐\n",
      "tive  tutorials,  and  curated  playlists  from  over  250  publishers,  including  O’Reilly\n",
      "Media, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Profes‐\n",
      "sional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press,\n",
      "John  Wiley  &  Sons,  Syngress,  Morgan  Kaufmann,  IBM  Redbooks,  Packt,  Adobe\n",
      "Press,  FT  Press,  Apress,  Manning,  New  Riders,  McGraw-Hill,  Jones  &  Bartlett,  and\n",
      "Course Technology, among others.\n",
      "\n",
      "For more information, please visit http://oreilly.com/safari.\n",
      "\n",
      "How to Contact Us\n",
      "Please address comments and questions concerning this book to the publisher:\n",
      "\n",
      "O’Reilly Media, Inc.\n",
      "1005 Gravenstein Highway North\n",
      "Sebastopol, CA 95472\n",
      "800-998-9938 (in the United States or Canada)\n",
      "\n",
      "xviii \n",
      "\n",
      "|  Preface\n",
      "\n",
      "\f707-829-0515 (international or local)\n",
      "707-829-0104 (fax)\n",
      "\n",
      "We have a web page for this book, where we list errata, examples, and any additional\n",
      "information.  You  can  access  this  page  at  http://bit.ly/hands-on-machine-learning-\n",
      "with-scikit-learn-and-tensorflow or https://homl.info/oreilly.\n",
      "\n",
      "To  comment  or  ask  technical  questions  about  this  book,  send  email  to  bookques‐\n",
      "tions@oreilly.com.\n",
      "\n",
      "For more information about our books, courses, conferences, and news, see our web‐\n",
      "site at http://www.oreilly.com.\n",
      "\n",
      "Find us on Facebook: http://facebook.com/oreilly\n",
      "\n",
      "Follow us on Twitter: http://twitter.com/oreillymedia\n",
      "\n",
      "Watch us on YouTube: http://www.youtube.com/oreillymedia\n",
      "\n",
      "Changes in the Second Edition\n",
      "This second edition has five main objectives:\n",
      "\n",
      "1. Cover additional topics: additional unsupervised learning techniques (including\n",
      "clustering,  anomaly  detection,  density  estimation  and  mixture  models),  addi‐\n",
      "tional  techniques  for  training  deep  nets  (including  self-normalized  networks),\n",
      "additional  computer  vision  techniques  (including  the  Xception,  SENet,  object\n",
      "detection  with  YOLO,  and  semantic  segmentation  using  R-CNN),  handling\n",
      "sequences using CNNs (including WaveNet), natural language processing using\n",
      "RNNs, CNNs and Transformers, generative adversarial networks, deploying Ten‐\n",
      "sorFlow models, and more.\n",
      "\n",
      "2. Update  the  book  to  mention  some  of  the  latest  results  from  Deep  Learning\n",
      "\n",
      "research.\n",
      "\n",
      "3. Migrate  all  TensorFlow  chapters  to  TensorFlow  2,  and  use  TensorFlow’s  imple‐\n",
      "mentation  of  the  Keras  API  (called  tf.keras)  whenever  possible,  to  simplify  the\n",
      "code examples.\n",
      "\n",
      "4. Update the code examples to use the latest version of Scikit-Learn, NumPy, Pan‐\n",
      "\n",
      "das, Matplotlib and other libraries.\n",
      "\n",
      "5. Clarify  some  sections  and  fix  some  errors,  thanks  to  plenty  of  great  feedback\n",
      "\n",
      "from readers.\n",
      "\n",
      "Some chapters were added, others were rewritten and a few were reordered. Table P-1\n",
      "shows the mapping between the 1st edition chapters and the 2nd edition chapters:\n",
      "\n",
      "Preface \n",
      "\n",
      "| \n",
      "\n",
      "xix\n",
      "\n",
      "\fTable P-1. Chapter mapping between 1st and 2nd edition\n",
      "\n",
      "1st Ed. chapter 2nd Ed. Chapter % Changes\n",
      "1\n",
      "\n",
      "<10%\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "8\n",
      "\n",
      "N/A\n",
      "\n",
      "10\n",
      "\n",
      "11\n",
      "\n",
      "9\n",
      "\n",
      "Part of 12\n",
      "\n",
      "13\n",
      "\n",
      "Part of 14\n",
      "\n",
      "Part of 14\n",
      "\n",
      "15\n",
      "\n",
      "16\n",
      "\n",
      "Part of 12\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "8\n",
      "\n",
      "9\n",
      "\n",
      "10\n",
      "\n",
      "11\n",
      "\n",
      "12\n",
      "\n",
      "13\n",
      "\n",
      "14\n",
      "\n",
      "15\n",
      "\n",
      "16\n",
      "\n",
      "17\n",
      "\n",
      "18\n",
      "\n",
      "19\n",
      "\n",
      "2nd Ed. Title\n",
      "The Machine Learning Landscape\n",
      "\n",
      "End-to-End Machine Learning Project\n",
      "\n",
      "Classification\n",
      "\n",
      "Training Models\n",
      "\n",
      "Support Vector Machines\n",
      "\n",
      "Decision Trees\n",
      "\n",
      "Ensemble Learning and Random Forests\n",
      "\n",
      "Dimensionality Reduction\n",
      "\n",
      "<10%\n",
      "\n",
      "<10%\n",
      "\n",
      "<10%\n",
      "\n",
      "<10%\n",
      "\n",
      "<10%\n",
      "\n",
      "<10%\n",
      "\n",
      "<10%\n",
      "\n",
      "100% new\n",
      "\n",
      "Unsupervised Learning Techniques\n",
      "\n",
      "~75%\n",
      "\n",
      "~50%\n",
      "\n",
      "Introduction to Artificial Neural Networks with Keras\n",
      "\n",
      "Training Deep Neural Networks\n",
      "\n",
      "100% rewritten Custom Models and Training with TensorFlow\n",
      "\n",
      "100% rewritten Loading and Preprocessing Data with TensorFlow\n",
      "\n",
      "~50%\n",
      "\n",
      "~75%\n",
      "\n",
      "~90%\n",
      "\n",
      "~75%\n",
      "\n",
      "~75%\n",
      "\n",
      "Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "Processing Sequences Using RNNs and CNNs\n",
      "\n",
      "Natural Language Processing with RNNs and Attention\n",
      "\n",
      "Autoencoders and GANs\n",
      "\n",
      "Reinforcement Learning\n",
      "\n",
      "100% rewritten Deploying your TensorFlow Models\n",
      "\n",
      "More specifically, here are the main changes for each 2nd edition chapter (other than\n",
      "clarifications, corrections and code updates):\n",
      "\n",
      "• Chapter 1\n",
      "\n",
      "— Added a section on handling mismatch between the training set and the vali‐\n",
      "\n",
      "dation & test sets.\n",
      "\n",
      "• Chapter 2\n",
      "\n",
      "— Added how to compute a confidence interval.\n",
      "\n",
      "— Improved the installation instructions (e.g., for Windows).\n",
      "— Introduced the upgraded OneHotEncoder and the new ColumnTransformer.\n",
      "\n",
      "• Chapter 4\n",
      "\n",
      "— Explained  the  need  for  training  instances  to  be  Independent  and  Identically\n",
      "\n",
      "Distributed (IID).\n",
      "\n",
      "• Chapter 7\n",
      "\n",
      "— Added a short section about XGBoost.\n",
      "\n",
      "xx \n",
      "\n",
      "|  Preface\n",
      "\n",
      "\f• Chapter 9 – new chapter including:\n",
      "\n",
      "— Clustering with K-Means, how to choose the number of clusters, how to use it\n",
      "for dimensionality reduction, semi-supervised learning, image segmentation,\n",
      "and more.\n",
      "\n",
      "— The DBSCAN clustering algorithm and an overview of other clustering algo‐\n",
      "\n",
      "rithms available in Scikit-Learn.\n",
      "\n",
      "— Gaussian  mixture  models,  the  Expectation-Maximization  (EM)  algorithm,\n",
      "Bayesian variational inference, and how mixture models can be used for clus‐\n",
      "tering, density estimation, anomaly detection and novelty detection.\n",
      "\n",
      "— Overview of other anomaly detection and novelty detection algorithms.\n",
      "\n",
      "• Chapter 10 (mostly new)\n",
      "\n",
      "— Added  an  introduction  to  the  Keras  API,  including  all  its  APIs  (Sequential,\n",
      "Functional and Subclassing), persistence and callbacks (including the Tensor\n",
      "Board callback).\n",
      "\n",
      "• Chapter 11 (many changes)\n",
      "\n",
      "— Introduced  self-normalizing  nets,  the  SELU  activation  function  and  Alpha\n",
      "\n",
      "Dropout.\n",
      "\n",
      "— Introduced self-supervised learning.\n",
      "\n",
      "— Added Nadam optimization.\n",
      "\n",
      "— Added Monte-Carlo Dropout.\n",
      "\n",
      "— Added a note about the risks of adaptive optimization methods.\n",
      "\n",
      "— Updated the practical guidelines.\n",
      "\n",
      "• Chapter 12 – completely rewritten chapter, including:\n",
      "\n",
      "— A tour of TensorFlow 2\n",
      "\n",
      "— TensorFlow’s lower-level Python API\n",
      "\n",
      "— Writing custom loss functions, metrics, layers, models\n",
      "\n",
      "— Using auto-differentiation and creating custom training algorithms.\n",
      "\n",
      "— TensorFlow Functions and graphs (including tracing and autograph).\n",
      "\n",
      "• Chapter 13 – new chapter, including:\n",
      "\n",
      "— The Data API\n",
      "\n",
      "— Loading/Storing data efficiently using TFRecords\n",
      "\n",
      "— The Features API (including an introduction to embeddings).\n",
      "\n",
      "— An overview of TF Transform and TF Datasets\n",
      "\n",
      "— Moved the low-level implementation of the neural network to the exercises.\n",
      "\n",
      "Preface \n",
      "\n",
      "| \n",
      "\n",
      "xxi\n",
      "\n",
      "\f— Removed  details  about  queues  and  readers  that  are  now  superseded  by  the\n",
      "\n",
      "Data API.\n",
      "\n",
      "• Chapter 14\n",
      "\n",
      "— Added Xception and SENet architectures.\n",
      "\n",
      "— Added a Keras implementation of ResNet-34.\n",
      "\n",
      "— Showed how to use pretrained models using Keras.\n",
      "\n",
      "— Added an end-to-end transfer learning example.\n",
      "\n",
      "— Added classification and localization.\n",
      "\n",
      "— Introduced Fully Convolutional Networks (FCNs).\n",
      "\n",
      "— Introduced object detection using the YOLO architecture.\n",
      "\n",
      "— Introduced semantic segmentation using R-CNN.\n",
      "\n",
      "• Chapter 15\n",
      "\n",
      "— Added an introduction to Wavenet.\n",
      "\n",
      "— Moved the Encoder–Decoder architecture and Bidirectional RNNs to Chapter\n",
      "\n",
      "16.\n",
      "\n",
      "• Chapter 16\n",
      "\n",
      "— Explained how to use the Data API to handle sequential data.\n",
      "\n",
      "— Showed  an  end-to-end  example  of  text  generation  using  a  Character  RNN,\n",
      "\n",
      "using both a stateless and a stateful RNN.\n",
      "\n",
      "— Showed an end-to-end example of sentiment analysis using an LSTM.\n",
      "\n",
      "— Explained masking in Keras.\n",
      "\n",
      "— Showed how to reuse pretrained embeddings using TF Hub.\n",
      "\n",
      "— Showed  how  to  build  an  Encoder–Decoder  for  Neural  Machine  Translation\n",
      "\n",
      "using TensorFlow Addons/seq2seq.\n",
      "\n",
      "— Introduced beam search.\n",
      "\n",
      "— Explained attention mechanisms.\n",
      "\n",
      "— Added a short overview of visual attention and a note on explainability.\n",
      "\n",
      "— Introduced the fully attention-based Transformer architecture, including posi‐\n",
      "\n",
      "tional embeddings and multi-head attention.\n",
      "\n",
      "— Added an overview of recent language models (2018).\n",
      "\n",
      "• Chapters 17, 18 and 19: coming soon.\n",
      "\n",
      "xxii \n",
      "\n",
      "|  Preface\n",
      "\n",
      "\fAcknowledgments\n",
      "Never in my wildest dreams did I imagine that the first edition of this book would get\n",
      "such a large audience. I received so many messages from readers, many asking ques‐\n",
      "tions,  some  kindly  pointing  out  errata,  and  most  sending  me  encouraging  words.  I\n",
      "cannot express how grateful I am to all these readers for their tremendous support.\n",
      "Thank you all so very much! Please do not hesitate to file issues on github if you find\n",
      "errors in the code examples (or just to ask questions), or to submit errata if you find\n",
      "errors in the text. Some readers also shared how this book helped them get their first\n",
      "job,  or  how  it  helped  them  solve  a  concrete  problem  they  were  working  on:  I  find\n",
      "such feedback incredibly motivating. If you find this book helpful, I would love it if\n",
      "you could share your story with me, either privately (e.g., via LinkedIn) or publicly\n",
      "(e.g., in an Amazon review).\n",
      "\n",
      "I  am  also  incredibly  thankful  to  all  the  amazing  people  who  took  time  out  of  their\n",
      "busy lives to review my book with such care. In particular, I would like to thank Fran‐\n",
      "çois Chollet for reviewing all the chapters based on Keras & TensorFlow, and giving\n",
      "me some great, in-depth feedback. Since Keras is one of the main additions to this 2nd\n",
      "edition, having its author review the book was invaluable. I highly recommend Fran‐\n",
      "çois’s excellent book Deep Learning with Python3: it has the conciseness, clarity and\n",
      "depth  of  the  Keras  library  itself.  Big  thanks  as  well  to  Ankur  Patel,  who  reviewed\n",
      "every chapter of this 2nd edition and gave me excellent feedback.\n",
      "\n",
      "This book also benefited from plenty of help from members of the TensorFlow team,\n",
      "in particular Martin Wicke, who tirelessly answered dozens of my questions and dis‐\n",
      "patched the rest to the right people, including Alexandre Passos, Allen Lavoie, André\n",
      "Susano  Pinto,  Anna  Revinskaya,  Anthony  Platanios,  Clemens  Mewald,  Dan  Moldo‐\n",
      "van, Daniel Dobson, Dustin Tran, Edd Wilder-James, Goldie Gadde, Jiri Simsa, Kar‐\n",
      "mel Allison, Nick Felt, Paige Bailey, Pete Warden (who also reviewed the 1st edition),\n",
      "Ryan  Sepassi,  Sandeep  Gupta,  Sean  Morgan,  Todd  Wang,  Tom  O’Malley,  William\n",
      "Chargin, and Yuefeng Zhou, all of whom were tremendously helpful. A huge thank\n",
      "you to all of you, and to all other members of the TensorFlow team. Not just for your\n",
      "help, but also for making such a great library.\n",
      "\n",
      "Big thanks to Haesun Park, who gave me plenty of excellent feedback and caught sev‐\n",
      "eral errors while he was writing the Korean translation of the 1st edition of this book.\n",
      "He  also  translated  the  Jupyter  notebooks  to  Korean,  not  to  mention  TensorFlow’s\n",
      "documentation. I do not speak Korean, but judging by the quality of his feedback, all\n",
      "his translations must be truly excellent! Moreover, he kindly contributed some of the\n",
      "solutions to the exercises in this book.\n",
      "\n",
      "3 “Deep Learning with Python,” François Chollet (2017).\n",
      "\n",
      "Preface \n",
      "\n",
      "| \n",
      "\n",
      "xxiii\n",
      "\n",
      "\fMany thanks as well to O’Reilly’s fantastic staff, in particular Nicole Tache, who gave\n",
      "me insightful feedback, always cheerful, encouraging, and helpful: I could not dream\n",
      "of a better editor. Big thanks to Michele Cronin as well, who was very helpful (and\n",
      "patient) at the start of this 2nd edition. Thanks to Marie Beaugureau, Ben Lorica, Mike\n",
      "Loukides,  and  Laurel  Ruma  for  believing  in  this  project  and  helping  me  define  its\n",
      "scope. Thanks to Matt Hacker and all of the Atlas team for answering all my technical\n",
      "questions  regarding  formatting,  asciidoc,  and  LaTeX,  and  thanks  to  Rachel  Mona‐\n",
      "ghan,  Nick  Adams,  and  all  of  the  production  team  for  their  final  review  and  their\n",
      "hundreds of corrections.\n",
      "\n",
      "I  would  also  like  to  thank  my  former  Google  colleagues,  in  particular  the  YouTube\n",
      "video classification team, for teaching me so much about Machine Learning. I could\n",
      "never have started the first edition without them. Special thanks to my personal ML\n",
      "gurus:  Clément  Courbet,  Julien  Dubois,  Mathias  Kende,  Daniel  Kitachewsky,  James\n",
      "Pack,  Alexander  Pak,  Anosh  Raj,  Vitor  Sessak,  Wiktor  Tomczak,  Ingrid  von  Glehn,\n",
      "Rich Washington, and everyone I worked with at YouTube and in the amazing Goo‐\n",
      "gle research teams in Mountain View. All these people are just as nice and helpful as\n",
      "they are bright, and that’s saying a lot.\n",
      "\n",
      "I will never forget the kind people who reviewed the 1st edition of this book, including\n",
      "David  Andrzejewski,  Eddy  Hung,  Grégoire  Mesnil,  Iain  Smears,  Ingrid  von  Glehn,\n",
      "Justin Francis, Karim Matrah, Lukas Biewald, Michel Tessier, Salim Sémaoune, Vin‐\n",
      "cent Guilbeau and of course my dear brother Sylvain.\n",
      "\n",
      "Last but not least, I am infinitely grateful to my beloved wife, Emmanuelle, and to our\n",
      "three  wonderful  children,  Alexandre,  Rémi,  and  Gabrielle,  for  encouraging  me  to\n",
      "work hard on this book, as well as for their insatiable curiosity: explaining some of\n",
      "the most difficult concepts in this book to my wife and children helped me clarify my\n",
      "thoughts and directly improved many parts of this book. Plus, they keep bringing me\n",
      "cookies and coffee! What more can one dream of?\n",
      "\n",
      "xxiv \n",
      "\n",
      "|  Preface\n",
      "\n",
      "\fPART I\n",
      "The Fundamentals of\n",
      "Machine Learning\n",
      "\n",
      "\f\fCHAPTER 1\n",
      "The Machine Learning Landscape\n",
      "\n",
      "With  Early  Release  ebooks,  you  get  books  in  their  earliest  form—\n",
      "the author’s raw and unedited content as he or she writes—so you\n",
      "can  take  advantage  of  these  technologies  long  before  the  official\n",
      "release of these titles. The following will be Chapter 1 in the final\n",
      "release of the book.\n",
      "\n",
      "When most people hear “Machine Learning,” they picture a robot: a dependable but‐\n",
      "ler or a deadly Terminator depending on who you ask. But Machine Learning is not\n",
      "just  a  futuristic  fantasy,  it’s  already  here.  In  fact,  it  has  been  around  for  decades  in\n",
      "some specialized applications, such as Optical Character Recognition (OCR). But the\n",
      "first ML application that really became mainstream, improving the lives of hundreds\n",
      "of  millions  of  people,  took  over  the  world  back  in  the  1990s:  it  was  the  spam  filter.\n",
      "Not exactly a self-aware Skynet, but it does technically qualify as Machine Learning\n",
      "(it  has  actually  learned  so  well  that  you  seldom  need  to  flag  an  email  as  spam  any‐\n",
      "more). It was followed by hundreds of ML applications that now quietly power hun‐\n",
      "dreds of products and features that you use regularly, from better recommendations\n",
      "to voice search.\n",
      "\n",
      "Where  does  Machine  Learning  start  and  where  does  it  end?  What  exactly  does  it\n",
      "mean for a machine to learn something? If I download a copy of Wikipedia, has my\n",
      "computer really “learned” something? Is it suddenly smarter? In this chapter we will\n",
      "start by clarifying what Machine Learning is and why you may want to use it.\n",
      "\n",
      "Then,  before  we  set  out  to  explore  the  Machine  Learning  continent,  we  will  take  a\n",
      "look at the map and learn about the main regions and the most notable landmarks:\n",
      "supervised  versus  unsupervised  learning,  online  versus  batch  learning,  instance-\n",
      "based versus model-based learning. Then we will look at the workflow of a typical ML\n",
      "project,  discuss  the  main  challenges  you  may  face,  and  cover  how  to  evaluate  and\n",
      "fine-tune a Machine Learning system.\n",
      "\n",
      "3\n",
      "\n",
      "\fThis  chapter  introduces  a  lot  of  fundamental  concepts  (and  jargon)  that  every  data\n",
      "scientist  should  know  by  heart.  It  will  be  a  high-level  overview  (the  only  chapter\n",
      "without  much  code),  all  rather  simple,  but  you  should  make  sure  everything  is\n",
      "crystal-clear to you before continuing to the rest of the book. So grab a coffee and let’s\n",
      "get started!\n",
      "\n",
      "If you already know all the Machine Learning basics, you may want\n",
      "to skip directly to Chapter 2. If you are not sure, try to answer all\n",
      "the questions listed at the end of the chapter before moving on.\n",
      "\n",
      "What Is Machine Learning?\n",
      "Machine  Learning  is  the  science  (and  art)  of  programming  computers  so  they  can\n",
      "learn from data.\n",
      "\n",
      "Here is a slightly more general definition:\n",
      "\n",
      "[Machine  Learning  is  the]  field  of  study  that  gives  computers  the  ability  to  learn\n",
      "without being explicitly programmed.\n",
      "\n",
      "—Arthur Samuel, 1959\n",
      "\n",
      "And a more engineering-oriented one:\n",
      "\n",
      "A  computer  program  is  said  to  learn  from  experience  E  with  respect  to  some  task  T\n",
      "and some performance measure P, if its performance on T, as measured by P, improves\n",
      "with experience E.\n",
      "\n",
      "—Tom Mitchell, 1997\n",
      "\n",
      "For example, your spam filter is a Machine Learning program that can learn to flag\n",
      "spam given examples of spam emails (e.g., flagged by users) and examples of regular\n",
      "(nonspam, also called “ham”) emails. The examples that the system uses to learn are\n",
      "called the training set. Each training example is called a training instance (or sample).\n",
      "In this case, the task T is to flag spam for new emails, the experience E is the training\n",
      "data, and the performance measure P needs to be defined; for example, you can use\n",
      "the ratio of correctly classified emails. This particular performance measure is called\n",
      "accuracy and it is often used in classification tasks.\n",
      "\n",
      "If you just download a copy of Wikipedia, your computer has a lot more data, but it is\n",
      "not suddenly better at any task. Thus, it is not Machine Learning.\n",
      "\n",
      "Why Use Machine Learning?\n",
      "Consider how you would write a spam filter using traditional programming techni‐\n",
      "ques (Figure 1-1):\n",
      "\n",
      "4 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 1: The Machine Learning Landscape\n",
      "\n",
      "\f1. First  you  would  look  at  what  spam  typically  looks  like.  You  might  notice  that\n",
      "some words or phrases (such as “4U,” “credit card,” “free,” and “amazing”) tend to\n",
      "come up a lot in the subject. Perhaps you would also notice a few other patterns\n",
      "in the sender’s name, the email’s body, and so on.\n",
      "\n",
      "2. You would write a detection algorithm for each of the patterns that you noticed,\n",
      "and  your  program  would  flag  emails  as  spam  if  a  number  of  these  patterns  are\n",
      "detected.\n",
      "\n",
      "3. You would test your program, and repeat steps 1 and 2 until it is good enough.\n",
      "\n",
      "Figure 1-1. The traditional approach\n",
      "\n",
      "Since the problem is not trivial, your program will likely become a long list of com‐\n",
      "plex rules—pretty hard to maintain.\n",
      "\n",
      "In contrast, a spam filter based on Machine Learning techniques automatically learns\n",
      "which  words  and  phrases  are  good  predictors  of  spam  by  detecting  unusually  fre‐\n",
      "quent  patterns  of  words  in  the  spam  examples  compared  to  the  ham  examples\n",
      "(Figure 1-2). The program is much shorter, easier to maintain, and most likely more\n",
      "accurate.\n",
      "\n",
      "Why Use Machine Learning? \n",
      "\n",
      "| \n",
      "\n",
      "5\n",
      "\n",
      "\fFigure 1-2. Machine Learning approach\n",
      "\n",
      "Moreover, if spammers notice that all their emails containing “4U” are blocked, they\n",
      "might  start  writing  “For  U”  instead.  A  spam  filter  using  traditional  programming\n",
      "techniques would need to be updated to flag “For U” emails. If spammers keep work‐\n",
      "ing around your spam filter, you will need to keep writing new rules forever.\n",
      "\n",
      "In contrast, a spam filter based on Machine Learning techniques automatically noti‐\n",
      "ces that “For U” has become unusually frequent in spam flagged by users, and it starts\n",
      "flagging them without your intervention (Figure 1-3).\n",
      "\n",
      "Figure 1-3. Automatically adapting to change\n",
      "\n",
      "Another area where Machine Learning shines is for problems that either are too com‐\n",
      "plex for traditional approaches or have no known algorithm. For example, consider \n",
      "speech recognition: say you want to start simple and write a program capable of dis‐\n",
      "tinguishing  the  words  “one”  and  “two.”  You  might  notice  that  the  word  “two”  starts\n",
      "with  a  high-pitch  sound  (“T”),  so  you  could  hardcode  an  algorithm  that  measures\n",
      "high-pitch sound intensity and use that to distinguish ones and twos. Obviously this\n",
      "technique  will  not  scale  to  thousands  of  words  spoken  by  millions  of  very  different\n",
      "\n",
      "6 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 1: The Machine Learning Landscape\n",
      "\n",
      "\fpeople in noisy environments and in dozens of languages. The best solution (at least\n",
      "today) is to write an algorithm that learns by itself, given many example recordings\n",
      "for each word.\n",
      "\n",
      "Finally, Machine Learning can help humans learn (Figure 1-4): ML algorithms can be\n",
      "inspected  to  see  what  they  have  learned  (although  for  some  algorithms  this  can  be\n",
      "tricky).  For  instance,  once  the  spam  filter  has  been  trained  on  enough  spam,  it  can\n",
      "easily  be  inspected  to  reveal  the  list  of  words  and  combinations  of  words  that  it\n",
      "believes are the best predictors of spam. Sometimes this will reveal unsuspected cor‐\n",
      "relations or new trends, and thereby lead to a better understanding of the problem.\n",
      "\n",
      "Applying ML techniques to dig into large amounts of data can help discover patterns\n",
      "that were not immediately apparent. This is called data mining.\n",
      "\n",
      "Figure 1-4. Machine Learning can help humans learn\n",
      "\n",
      "To summarize, Machine Learning is great for:\n",
      "\n",
      "• Problems for which existing solutions require a lot of hand-tuning or long lists of\n",
      "rules: one Machine Learning algorithm can often simplify code and perform bet‐\n",
      "ter.\n",
      "\n",
      "• Complex problems for which there is no good solution at all using a traditional\n",
      "\n",
      "approach: the best Machine Learning techniques can find a solution.\n",
      "\n",
      "• Fluctuating environments: a Machine Learning system can adapt to new data.\n",
      "\n",
      "• Getting insights about complex problems and large amounts of data.\n",
      "\n",
      "Why Use Machine Learning? \n",
      "\n",
      "| \n",
      "\n",
      "7\n",
      "\n",
      "\fTypes of Machine Learning Systems\n",
      "There  are  so  many  different  types  of  Machine  Learning  systems  that  it  is  useful  to\n",
      "classify them in broad categories based on:\n",
      "\n",
      "• Whether or not they are trained with human supervision (supervised, unsuper‐\n",
      "\n",
      "vised, semisupervised, and Reinforcement Learning)\n",
      "\n",
      "• Whether  or  not  they  can  learn  incrementally  on  the  fly  (online  versus  batch\n",
      "\n",
      "learning)\n",
      "\n",
      "• Whether they work by simply comparing new data points to known data points,\n",
      "or instead detect patterns in the training data and build a predictive model, much\n",
      "like scientists do (instance-based versus model-based learning)\n",
      "\n",
      "These  criteria  are  not  exclusive;  you  can  combine  them  in  any  way  you  like.  For\n",
      "example, a state-of-the-art spam filter may learn on the fly using a deep neural net‐\n",
      "work model trained using examples of spam and ham; this makes it an online, model-\n",
      "based, supervised learning system.\n",
      "\n",
      "Let’s look at each of these criteria a bit more closely.\n",
      "\n",
      "Supervised/Unsupervised Learning\n",
      "Machine  Learning  systems  can  be  classified  according  to  the  amount  and  type  of\n",
      "supervision  they  get  during  training.  There  are  four  major  categories:  supervised\n",
      "learning, unsupervised learning, semisupervised learning, and Reinforcement Learn‐\n",
      "ing.\n",
      "\n",
      "Supervised learning\n",
      "\n",
      "In supervised learning, the training data you feed to the algorithm includes the desired\n",
      "solutions, called labels (Figure 1-5).\n",
      "\n",
      "Figure 1-5. A labeled training set for supervised learning (e.g., spam classification)\n",
      "\n",
      "8 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 1: The Machine Learning Landscape\n",
      "\n",
      "\fA typical supervised learning task is classification. The spam filter is a good example\n",
      "of this: it is trained with many example emails along with their class (spam or ham),\n",
      "and it must learn how to classify new emails.\n",
      "\n",
      "Another  typical  task  is  to  predict  a  target  numeric  value,  such  as  the  price  of  a  car,\n",
      "given a set of features (mileage, age, brand, etc.) called predictors. This sort of task is \n",
      "called regression (Figure 1-6).1 To train the system, you need to give it many examples\n",
      "of cars, including both their predictors and their labels (i.e., their prices).\n",
      "\n",
      "In  Machine  Learning  an  attribute  is  a  data  type  (e.g.,  “Mileage”),\n",
      "while a feature has several meanings depending on the context, but\n",
      "generally  means  an  attribute  plus  its  value  (e.g.,  “Mileage  =\n",
      "15,000”).  Many  people  use  the  words  attribute  and  feature  inter‐\n",
      "changeably, though.\n",
      "\n",
      "Figure 1-6. Regression\n",
      "\n",
      "Note that some regression algorithms can be used for classification as well, and vice\n",
      "versa. For example, Logistic Regression is commonly used for classification, as it can\n",
      "output a value that corresponds to the probability of belonging to a given class (e.g.,\n",
      "20% chance of being spam).\n",
      "\n",
      "1 Fun fact: this odd-sounding name is a statistics term introduced by Francis Galton while he was studying the\n",
      "fact that the children of tall people tend to be shorter than their parents. Since children were shorter, he called\n",
      "this regression to the mean. This name was then applied to the methods he used to analyze correlations\n",
      "between variables.\n",
      "\n",
      "Types of Machine Learning Systems \n",
      "\n",
      "| \n",
      "\n",
      "9\n",
      "\n",
      "\fHere are some of the most important supervised learning algorithms (covered in this\n",
      "book):\n",
      "\n",
      "• k-Nearest Neighbors\n",
      "\n",
      "• Linear Regression\n",
      "\n",
      "• Logistic Regression\n",
      "\n",
      "• Support Vector Machines (SVMs)\n",
      "\n",
      "• Decision Trees and Random Forests\n",
      "• Neural networks2\n",
      "\n",
      "Unsupervised learning\n",
      "\n",
      "In  unsupervised  learning,  as  you  might  guess,  the  training  data  is  unlabeled\n",
      "(Figure 1-7). The system tries to learn without a teacher.\n",
      "\n",
      "Figure 1-7. An unlabeled training set for unsupervised learning\n",
      "\n",
      "Here  are  some  of  the  most  important  unsupervised  learning  algorithms  (most  of\n",
      "these are covered in Chapter 8 and Chapter 9):\n",
      "\n",
      "• Clustering\n",
      "\n",
      "— K-Means\n",
      "\n",
      "— DBSCAN\n",
      "\n",
      "— Hierarchical Cluster Analysis (HCA)\n",
      "\n",
      "• Anomaly detection and novelty detection\n",
      "\n",
      "— One-class SVM\n",
      "\n",
      "— Isolation Forest\n",
      "\n",
      "2 Some neural network architectures can be unsupervised, such as autoencoders and restricted Boltzmann\n",
      "\n",
      "machines. They can also be semisupervised, such as in deep belief networks and unsupervised pretraining.\n",
      "\n",
      "10 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 1: The Machine Learning Landscape\n",
      "\n",
      "\f• Visualization and dimensionality reduction\n",
      "\n",
      "— Principal Component Analysis (PCA)\n",
      "\n",
      "— Kernel PCA\n",
      "\n",
      "— Locally-Linear Embedding (LLE)\n",
      "\n",
      "— t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
      "\n",
      "• Association rule learning\n",
      "\n",
      "— Apriori\n",
      "\n",
      "— Eclat\n",
      "\n",
      "For example, say you have a lot of data about your blog’s visitors. You may want to\n",
      "run a clustering algorithm to try to detect groups of similar visitors (Figure 1-8). At\n",
      "no  point  do  you  tell  the  algorithm  which  group  a  visitor  belongs  to:  it  finds  those\n",
      "connections without your help. For example, it might notice that 40% of your visitors\n",
      "are males who love comic books and generally read your blog in the evening, while\n",
      "20%  are  young  sci-fi  lovers  who  visit  during  the  weekends,  and  so  on.  If  you  use  a\n",
      "hierarchical  clustering  algorithm,  it  may  also  subdivide  each  group  into  smaller\n",
      "groups. This may help you target your posts for each group.\n",
      "\n",
      "Figure 1-8. Clustering\n",
      "\n",
      "Visualization algorithms are also good examples of unsupervised learning algorithms:\n",
      "you feed them a lot of complex and unlabeled data, and they output a 2D or 3D rep‐\n",
      "resentation of your data that can easily be plotted (Figure 1-9). These algorithms try\n",
      "to preserve as much structure as they can (e.g., trying to keep separate clusters in the\n",
      "input  space  from  overlapping  in  the  visualization),  so  you  can  understand  how  the\n",
      "data is organized and perhaps identify unsuspected patterns.\n",
      "\n",
      "Types of Machine Learning Systems \n",
      "\n",
      "| \n",
      "\n",
      "11\n",
      "\n",
      "\fFigure 1-9. Example of a t-SNE visualization highlighting semantic clusters3\n",
      "\n",
      "A  related  task  is  dimensionality  reduction,  in  which  the  goal  is  to  simplify  the  data\n",
      "without losing too much information. One way to do this is to merge several correla‐\n",
      "ted features into one. For example, a car’s mileage may be very correlated with its age,\n",
      "so the dimensionality reduction algorithm will merge them into one feature that rep‐\n",
      "resents the car’s wear and tear. This is called feature extraction.\n",
      "\n",
      "It is often a good idea to try to reduce the dimension of your train‐\n",
      "ing  data  using  a  dimensionality  reduction  algorithm  before  you\n",
      "feed  it  to  another  Machine  Learning  algorithm  (such  as  a  super‐\n",
      "vised learning algorithm). It will run much faster, the data will take\n",
      "up less disk and memory space, and in some cases it may also per‐\n",
      "form better.\n",
      "\n",
      "Yet another important unsupervised task is anomaly detection—for example, detect‐\n",
      "ing unusual credit card transactions to prevent fraud, catching manufacturing defects,\n",
      "or automatically removing outliers from a dataset before feeding it to another learn‐\n",
      "ing  algorithm.  The  system  is  shown  mostly  normal  instances  during  training,  so  it\n",
      "learns to recognize them and when it sees a new instance it can tell whether it looks\n",
      "\n",
      "3 Notice how animals are rather well separated from vehicles, how horses are close to deer but far from birds,\n",
      "\n",
      "and so on. Figure reproduced with permission from Socher, Ganjoo, Manning, and Ng (2013), “T-SNE visual‐\n",
      "ization of the semantic word space.”\n",
      "\n",
      "12 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 1: The Machine Learning Landscape\n",
      "\n",
      "\flike a normal one or whether it is likely an anomaly (see Figure 1-10). A very similar\n",
      "task is novelty detection: the difference is that novelty detection algorithms expect to\n",
      "see only normal data during training, while anomaly detection algorithms are usually\n",
      "more tolerant, they can often perform well even with a small percentage of outliers in\n",
      "the training set.\n",
      "\n",
      "Figure 1-10. Anomaly detection\n",
      "\n",
      "Finally, another common unsupervised task is association rule learning, in which the\n",
      "goal  is  to  dig  into  large  amounts  of  data  and  discover  interesting  relations  between\n",
      "attributes. For example, suppose you own a supermarket. Running an association rule\n",
      "on  your  sales  logs  may  reveal  that  people  who  purchase  barbecue  sauce  and  potato\n",
      "chips also tend to buy steak. Thus, you may want to place these items close to each \n",
      "other.\n",
      "\n",
      "Semisupervised learning\n",
      "\n",
      "Some  algorithms  can  deal  with  partially  labeled  training  data,  usually  a  lot  of  unla‐\n",
      "beled  data  and  a  little  bit  of  labeled  data.  This  is  called  semisupervised  learning\n",
      "(Figure 1-11).\n",
      "\n",
      "Some photo-hosting services, such as Google Photos, are good examples of this. Once\n",
      "you upload all your family photos to the service, it automatically recognizes that the\n",
      "same person A shows up in photos 1, 5, and 11, while another person B shows up in\n",
      "photos 2, 5, and 7. This is the unsupervised part of the algorithm (clustering). Now all\n",
      "the system needs is for you to tell it who these people are. Just one label per person,4\n",
      "and it is able to name everyone in every photo, which is useful for searching photos.\n",
      "\n",
      "4 That’s when the system works perfectly. In practice it often creates a few clusters per person, and sometimes\n",
      "mixes up two people who look alike, so you need to provide a few labels per person and manually clean up\n",
      "some clusters.\n",
      "\n",
      "Types of Machine Learning Systems \n",
      "\n",
      "| \n",
      "\n",
      "13\n",
      "\n",
      "\fFigure 1-11. Semisupervised learning\n",
      "\n",
      "Most  semisupervised  learning  algorithms  are  combinations  of  unsupervised  and\n",
      "supervised algorithms. For example, deep belief networks (DBNs) are based on unsu‐\n",
      "pervised components called restricted Boltzmann machines (RBMs) stacked on top of\n",
      "one another. RBMs are trained sequentially in an unsupervised manner, and then the\n",
      "whole system is fine-tuned using supervised learning techniques.\n",
      "\n",
      "Reinforcement Learning\n",
      "\n",
      "Reinforcement Learning is a very different beast. The learning system, called an agent\n",
      "in  this  context,  can  observe  the  environment,  select  and  perform  actions,  and  get\n",
      "rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It\n",
      "must  then  learn  by  itself  what  is  the  best  strategy,  called  a  policy,  to  get  the  most\n",
      "reward over time. A policy defines what action the agent should choose when it is in a\n",
      "given situation.\n",
      "\n",
      "14 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 1: The Machine Learning Landscape\n",
      "\n",
      "\fFigure 1-12. Reinforcement Learning\n",
      "\n",
      "For  example,  many  robots  implement  Reinforcement  Learning  algorithms  to  learn\n",
      "how to walk. DeepMind’s AlphaGo program is also a good example of Reinforcement\n",
      "Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie\n",
      "at the game of Go. It learned its winning policy by analyzing millions of games, and\n",
      "then playing many games against itself. Note that learning was turned off during the\n",
      "games against the champion; AlphaGo was just applying the policy it had learned.\n",
      "\n",
      "Batch and Online Learning\n",
      "Another  criterion  used  to  classify  Machine  Learning  systems  is  whether  or  not  the\n",
      "system can learn incrementally from a stream of incoming data.\n",
      "\n",
      "Batch learning\n",
      "\n",
      "In batch learning, the system is incapable of learning incrementally: it must be trained\n",
      "using  all  the  available  data.  This  will  generally  take  a  lot  of  time  and  computing\n",
      "resources,  so  it  is  typically  done  offline.  First  the  system  is  trained,  and  then  it  is\n",
      "launched into production and runs without learning anymore; it just applies what it\n",
      "has learned. This is called offline learning.\n",
      "\n",
      "If you want a batch learning system to know about new data (such as a new type of\n",
      "spam), you need to train a new version of the system from scratch on the full dataset\n",
      "(not just the new data, but also the old data), then stop the old system and replace it\n",
      "with the new one.\n",
      "\n",
      "Fortunately,  the  whole  process  of  training,  evaluating,  and  launching  a  Machine\n",
      "Learning  system  can  be  automated  fairly  easily  (as  shown  in  Figure  1-3),  so  even  a\n",
      "\n",
      "Types of Machine Learning Systems \n",
      "\n",
      "| \n",
      "\n",
      "15\n",
      "\n",
      "\fbatch learning system can adapt to change. Simply update the data and train a new\n",
      "version of the system from scratch as often as needed.\n",
      "\n",
      "This solution is simple and often works fine, but training using the full set of data can\n",
      "take many hours, so you would typically train a new system only every 24 hours or\n",
      "even just weekly. If your system needs to adapt to rapidly changing data (e.g., to pre‐\n",
      "dict stock prices), then you need a more reactive solution.\n",
      "\n",
      "Also,  training  on  the  full  set  of  data  requires  a  lot  of  computing  resources  (CPU,\n",
      "memory space, disk space, disk I/O, network I/O, etc.). If you have a lot of data and\n",
      "you automate your system to train from scratch every day, it will end up costing you a\n",
      "lot of money. If the amount of data is huge, it may even be impossible to use a batch\n",
      "learning algorithm.\n",
      "\n",
      "Finally,  if  your  system  needs  to  be  able  to  learn  autonomously  and  it  has  limited\n",
      "resources (e.g., a smartphone application or a rover on Mars), then carrying around\n",
      "large  amounts  of  training  data  and  taking  up  a  lot  of  resources  to  train  for  hours\n",
      "every day is a showstopper.\n",
      "\n",
      "Fortunately, a better option in all these cases is to use algorithms that are capable of\n",
      "learning incrementally.\n",
      "\n",
      "Online learning\n",
      "\n",
      "In  online  learning,  you  train  the  system  incrementally  by  feeding  it  data  instances\n",
      "sequentially, either individually or by small groups called mini-batches. Each learning\n",
      "step is fast and cheap, so the system can learn about new data on the fly, as it arrives\n",
      "(see Figure 1-13).\n",
      "\n",
      "Figure 1-13. Online learning\n",
      "\n",
      "Online learning is great for systems that receive data as a continuous flow (e.g., stock\n",
      "prices) and need to adapt to change rapidly or autonomously. It is also a good option\n",
      "\n",
      "16 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 1: The Machine Learning Landscape\n",
      "\n",
      "\fif you have limited computing resources: once an online learning system has learned\n",
      "about new data instances, it does not need them anymore, so you can discard them\n",
      "(unless you want to be able to roll back to a previous state and “replay” the data). This\n",
      "can save a huge amount of space.\n",
      "\n",
      "Online  learning  algorithms  can  also  be  used  to  train  systems  on  huge  datasets  that\n",
      "cannot  fit  in  one  machine’s  main  memory  (this  is  called  out-of-core  learning).  The\n",
      "algorithm  loads  part  of  the  data,  runs  a  training  step  on  that  data,  and  repeats  the\n",
      "process until it has run on all of the data (see Figure 1-14).\n",
      "\n",
      "Out-of-core  learning  is  usually  done  offline  (i.e.,  not  on  the  live\n",
      "system), so online learning can be a confusing name. Think of it as\n",
      "incremental learning.\n",
      "\n",
      "Figure 1-14. Using online learning to handle huge datasets\n",
      "\n",
      "One important parameter of online learning systems is how fast they should adapt to\n",
      "changing data: this is called the learning rate. If you set a high learning rate, then your\n",
      "system will rapidly adapt to new data, but it will also tend to quickly forget the old\n",
      "data (you don’t want a spam filter to flag only the latest kinds of spam it was shown).\n",
      "Conversely, if you set a low learning rate, the system will have more inertia; that is, it\n",
      "will learn more slowly, but it will also be less sensitive to noise in the new data or to\n",
      "sequences of nonrepresentative data points (outliers).\n",
      "\n",
      "A big challenge with online learning is that if bad data is fed to the system, the sys‐\n",
      "tem’s performance will gradually decline. If we are talking about a live system, your\n",
      "clients will notice. For example, bad data could come from a malfunctioning sensor\n",
      "on a robot, or from someone spamming a search engine to try to rank high in search\n",
      "\n",
      "Types of Machine Learning Systems \n",
      "\n",
      "| \n",
      "\n",
      "17\n",
      "\n",
      "\fresults.  To  reduce  this  risk,  you  need  to  monitor  your  system  closely  and  promptly\n",
      "switch learning off (and possibly revert to a previously working state) if you detect a\n",
      "drop  in  performance.  You  may  also  want  to  monitor  the  input  data  and  react  to\n",
      "abnormal data (e.g., using an anomaly detection algorithm).\n",
      "\n",
      "Instance-Based Versus Model-Based Learning\n",
      "One  more  way  to  categorize  Machine  Learning  systems  is  by  how  they  generalize.\n",
      "Most Machine Learning tasks are about making predictions. This means that given a\n",
      "number of training examples, the system needs to be able to generalize to examples it\n",
      "has  never  seen  before.  Having  a  good  performance  measure  on  the  training  data  is\n",
      "good, but insufficient; the true goal is to perform well on new instances.\n",
      "\n",
      "There  are  two  main  approaches  to  generalization:  instance-based  learning  and\n",
      "model-based learning.\n",
      "\n",
      "Instance-based learning\n",
      "\n",
      "Possibly the most trivial form of learning is simply to learn by heart. If you were to\n",
      "create a spam filter this way, it would just flag all emails that are identical to emails\n",
      "that have already been flagged by users—not the worst solution, but certainly not the\n",
      "best.\n",
      "\n",
      "Instead  of  just  flagging  emails  that  are  identical  to  known  spam  emails,  your  spam\n",
      "filter could be programmed to also flag emails that are very similar to known spam\n",
      "emails. This requires a measure of similarity between two emails. A (very basic) simi‐\n",
      "larity measure between two emails could be to count the number of words they have\n",
      "in common. The system would flag an email as spam if it has many words in com‐\n",
      "mon with a known spam email.\n",
      "\n",
      "This is called instance-based learning: the system learns the examples by heart, then\n",
      "generalizes to new cases by comparing them to the learned examples (or a subset of\n",
      "them),  using  a  similarity  measure.  For  example,  in  Figure  1-15  the  new  instance\n",
      "would  be  classified  as  a  triangle  because  the  majority  of  the  most  similar  instances\n",
      "belong to that class.\n",
      "\n",
      "18 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 1: The Machine Learning Landscape\n",
      "\n",
      "\fFigure 1-15. Instance-based learning\n",
      "\n",
      "Model-based learning\n",
      "\n",
      "Another way to generalize from a set of examples is to build a model of these exam‐\n",
      "ples,  then  use  that  model  to  make  predictions.  This  is  called  model-based  learning\n",
      "(Figure 1-16).\n",
      "\n",
      "Figure 1-16. Model-based learning\n",
      "\n",
      "For example, suppose you want to know if money makes people happy, so you down‐\n",
      "load  the  Better  Life  Index  data  from  the  OECD’s  website  as  well  as  stats  about  GDP\n",
      "per capita from the IMF’s website. Then you join the tables and sort by GDP per cap‐\n",
      "ita. Table 1-1 shows an excerpt of what you get.\n",
      "\n",
      "Types of Machine Learning Systems \n",
      "\n",
      "| \n",
      "\n",
      "19\n",
      "\n",
      "\fTable 1-1. Does money make people happier?\n",
      "\n",
      "GDP per capita (USD)\n",
      "12,240\n",
      "\n",
      "Life satisfaction\n",
      "4.9\n",
      "\n",
      "Country\n",
      "Hungary\n",
      "\n",
      "Korea\n",
      "\n",
      "France\n",
      "\n",
      "Australia\n",
      "\n",
      "27,195\n",
      "\n",
      "37,675\n",
      "\n",
      "50,962\n",
      "\n",
      "United States\n",
      "\n",
      "55,805\n",
      "\n",
      "5.8\n",
      "\n",
      "6.5\n",
      "\n",
      "7.3\n",
      "\n",
      "7.2\n",
      "\n",
      "Let’s plot the data for a few random countries (Figure 1-17).\n",
      "\n",
      "Figure 1-17. Do you see a trend here?\n",
      "\n",
      "There does seem to be a trend here! Although the data is noisy (i.e., partly random), it\n",
      "looks like life satisfaction goes up more or less linearly as the country’s GDP per cap‐\n",
      "ita increases. So you decide to model life satisfaction as a linear function of GDP per\n",
      "capita. This step is called model selection: you selected a linear model of life satisfac‐\n",
      "tion with just one attribute, GDP per capita (Equation 1-1).\n",
      "\n",
      "Equation 1-1. A simple linear model\n",
      "\n",
      "life_satisfaction = θ0 + θ1 × GDP_per_capita\n",
      "\n",
      "This model has two model parameters, θ0 and θ1.5 By tweaking these parameters, you\n",
      "can make your model represent any linear function, as shown in Figure 1-18.\n",
      "\n",
      "5 By convention, the Greek letter θ (theta) is frequently used to represent model parameters.\n",
      "\n",
      "20 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 1: The Machine Learning Landscape\n",
      "\n",
      "\fFigure 1-18. A few possible linear models\n",
      "\n",
      "Before  you  can  use  your  model,  you  need  to  define  the  parameter  values  θ0  and  θ1.\n",
      "How can you know which values will make your model perform best? To answer this\n",
      "question, you need to specify a performance measure. You can either define a utility\n",
      "function (or fitness function) that measures how good your model is, or you can define\n",
      "a  cost  function  that  measures  how  bad  it  is.  For  linear  regression  problems,  people\n",
      "typically  use  a  cost  function  that  measures  the  distance  between  the  linear  model’s\n",
      "predictions and the training examples; the objective is to minimize this distance.\n",
      "\n",
      "This  is  where  the  Linear  Regression  algorithm  comes  in:  you  feed  it  your  training\n",
      "examples and it finds the parameters that make the linear model fit best to your data.\n",
      "This  is  called  training  the  model.  In  our  case  the  algorithm  finds  that  the  optimal\n",
      "parameter values are θ0 = 4.85 and θ1 = 4.91 × 10–5.\n",
      "\n",
      "Now the model fits the training data as closely as possible (for a linear model), as you\n",
      "can see in Figure 1-19.\n",
      "\n",
      "Figure 1-19. The linear model that fits the training data best\n",
      "\n",
      "Types of Machine Learning Systems \n",
      "\n",
      "| \n",
      "\n",
      "21\n",
      "\n",
      "\fYou  are  finally  ready  to  run  the  model  to  make  predictions.  For  example,  say  you\n",
      "want to know how happy Cypriots are, and the OECD data does not have the answer.\n",
      "Fortunately, you can use your model to make a good prediction: you look up Cyprus’s\n",
      "GDP per capita, find $22,587, and then apply your model and find that life satisfac‐\n",
      "tion is likely to be somewhere around 4.85 + 22,587 × 4.91 × 10-5 = 5.96.\n",
      "\n",
      "To whet your appetite, Example 1-1 shows the Python code that loads the data, pre‐\n",
      "pares  it,6  creates  a  scatterplot  for  visualization,  and  then  trains  a  linear  model  and\n",
      "makes a prediction.7\n",
      "\n",
      "Example 1-1. Training and running a linear model using Scikit-Learn\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import sklearn.linear_model\n",
      "\n",
      "# Load the data\n",
      "oecd_bli = pd.read_csv(\"oecd_bli_2015.csv\", thousands=',')\n",
      "gdp_per_capita = pd.read_csv(\"gdp_per_capita.csv\",thousands=',',delimiter='\\t',\n",
      "                             encoding='latin1', na_values=\"n/a\")\n",
      "\n",
      "# Prepare the data\n",
      "country_stats = prepare_country_stats(oecd_bli, gdp_per_capita)\n",
      "X = np.c_[country_stats[\"GDP per capita\"]]\n",
      "y = np.c_[country_stats[\"Life satisfaction\"]]\n",
      "\n",
      "# Visualize the data\n",
      "country_stats.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction')\n",
      "plt.show()\n",
      "\n",
      "# Select a linear model\n",
      "model = sklearn.linear_model.LinearRegression()\n",
      "\n",
      "# Train the model\n",
      "model.fit(X, y)\n",
      "\n",
      "# Make a prediction for Cyprus\n",
      "X_new = [[22587]]  # Cyprus' GDP per capita\n",
      "print(model.predict(X_new)) # outputs [[ 5.96242338]]\n",
      "\n",
      "6 The prepare_country_stats() function’s definition is not shown here (see this chapter’s Jupyter notebook if\n",
      "you want all the gory details). It’s just boring Pandas code that joins the life satisfaction data from the OECD\n",
      "with the GDP per capita data from the IMF.\n",
      "\n",
      "7 It’s okay if you don’t understand all the code yet; we will present Scikit-Learn in the following chapters.\n",
      "\n",
      "22 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 1: The Machine Learning Landscape\n",
      "\n",
      "\fIf you had used an instance-based learning algorithm instead, you\n",
      "would have found that Slovenia has the closest GDP per capita to\n",
      "that  of  Cyprus  ($20,732),  and  since  the  OECD  data  tells  us  that\n",
      "Slovenians’  life  satisfaction  is  5.7,  you  would  have  predicted  a  life\n",
      "satisfaction of 5.7 for Cyprus. If you zoom out a bit and look at the\n",
      "two  next  closest  countries,  you  will  find  Portugal  and  Spain  with\n",
      "life satisfactions of 5.1 and 6.5, respectively. Averaging these three\n",
      "values, you get 5.77, which is pretty close to your model-based pre‐\n",
      "diction. This simple algorithm is called k-Nearest Neighbors regres‐\n",
      "sion (in this example, k = 3).\n",
      "\n",
      "Replacing  the  Linear  Regression  model  with  k-Nearest  Neighbors\n",
      "regression in the previous code is as simple as replacing these two\n",
      "lines:\n",
      "\n",
      "import sklearn.linear_model\n",
      "model = sklearn.linear_model.LinearRegression()\n",
      "\n",
      "with these two:\n",
      "\n",
      "import sklearn.neighbors\n",
      "model = sklearn.neighbors.KNeighborsRegressor(n_neighbors=3)\n",
      "\n",
      "If all went well, your model will make good predictions. If not, you may need to use\n",
      "more attributes (employment rate, health, air pollution, etc.), get more or better qual‐\n",
      "ity training data, or perhaps select a more powerful model (e.g., a Polynomial Regres‐\n",
      "sion model).\n",
      "\n",
      "In summary:\n",
      "\n",
      "• You studied the data.\n",
      "\n",
      "• You selected a model.\n",
      "\n",
      "• You trained it on the training data (i.e., the learning algorithm searched for the\n",
      "\n",
      "model parameter values that minimize a cost function).\n",
      "\n",
      "• Finally,  you  applied  the  model  to  make  predictions  on  new  cases  (this  is  called\n",
      "\n",
      "inference), hoping that this model will generalize well.\n",
      "\n",
      "This  is  what  a  typical  Machine  Learning  project  looks  like.  In  Chapter  2  you  will\n",
      "experience this first-hand by going through an end-to-end project.\n",
      "\n",
      "We  have  covered  a  lot  of  ground  so  far:  you  now  know  what  Machine  Learning  is\n",
      "really about, why it is useful, what some of the most common categories of ML sys‐\n",
      "tems are, and what a typical project workflow looks like. Now let’s look at what can go\n",
      "wrong in learning and prevent you from making accurate predictions.\n",
      "\n",
      "Types of Machine Learning Systems \n",
      "\n",
      "| \n",
      "\n",
      "23\n",
      "\n",
      "\fMain Challenges of Machine Learning\n",
      "In short, since your main task is to select a learning algorithm and train it on some\n",
      "data, the two things that can go wrong are “bad algorithm” and “bad data.” Let’s start\n",
      "with examples of bad data.\n",
      "\n",
      "Insufficient Quantity of Training Data\n",
      "For a toddler to learn what an apple is, all it takes is for you to point to an apple and\n",
      "say “apple” (possibly repeating this procedure a few times). Now the child is able to\n",
      "recognize apples in all sorts of colors and shapes. Genius.\n",
      "\n",
      "Machine Learning is not quite there yet; it takes a lot of data for most Machine Learn‐\n",
      "ing  algorithms  to  work  properly.  Even  for  very  simple  problems  you  typically  need\n",
      "thousands of examples, and for complex problems such as image or speech recogni‐\n",
      "tion  you  may  need  millions  of  examples  (unless  you  can  reuse  parts  of  an  existing\n",
      "model).\n",
      "\n",
      "24 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 1: The Machine Learning Landscape\n",
      "\n",
      "\fThe Unreasonable Effectiveness of Data\n",
      "In a famous paper published in 2001, Microsoft researchers Michele Banko and Eric\n",
      "Brill showed that very different Machine Learning algorithms, including fairly simple\n",
      "ones,  performed  almost  identically  well  on  a  complex  problem  of  natural  language\n",
      "disambiguation8 once they were given enough data (as you can see in Figure 1-20).\n",
      "\n",
      "Figure 1-20. The importance of data versus algorithms9\n",
      "\n",
      "As the authors put it: “these results suggest that we may want to reconsider the trade-\n",
      "off between spending time and money on algorithm development versus spending it\n",
      "on corpus development.”\n",
      "\n",
      "The idea that data matters more than algorithms for complex problems was further\n",
      "popularized by Peter Norvig et al. in a paper titled “The Unreasonable Effectiveness\n",
      "of Data” published in 2009.10 It should be noted, however, that small- and medium-\n",
      "sized  datasets  are  still  very  common,  and  it  is  not  always  easy  or  cheap  to  get  extra\n",
      "training data, so don’t abandon algorithms just yet.\n",
      "\n",
      "8 For example, knowing whether to write “to,” “two,” or “too” depending on the context.\n",
      "\n",
      "9 Figure reproduced with permission from Banko and Brill (2001), “Learning Curves for Confusion Set Disam‐\n",
      "\n",
      "biguation.”\n",
      "\n",
      "10 “The Unreasonable Effectiveness of Data,” Peter Norvig et al. (2009).\n",
      "\n",
      "Main Challenges of Machine Learning \n",
      "\n",
      "| \n",
      "\n",
      "25\n",
      "\n",
      "\fNonrepresentative Training Data\n",
      "In order to generalize well, it is crucial that your training data be representative of the\n",
      "new  cases  you  want  to  generalize  to.  This  is  true  whether  you  use  instance-based\n",
      "learning or model-based learning.\n",
      "\n",
      "For example, the set of countries we used earlier for training the linear model was not\n",
      "perfectly  representative;  a  few  countries  were  missing.  Figure  1-21  shows  what  the\n",
      "data looks like when you add the missing countries.\n",
      "\n",
      "Figure 1-21. A more representative training sample\n",
      "\n",
      "If you train a linear model on this data, you get the solid line, while the old model is\n",
      "represented  by  the  dotted  line.  As  you  can  see,  not  only  does  adding  a  few  missing\n",
      "countries significantly alter the model, but it makes it clear that such a simple linear\n",
      "model is probably never going to work well. It seems that very rich countries are not\n",
      "happier than moderately rich countries (in fact they seem unhappier), and conversely\n",
      "some poor countries seem happier than many rich countries.\n",
      "\n",
      "By using a nonrepresentative training set, we trained a model that is unlikely to make\n",
      "accurate predictions, especially for very poor and very rich countries.\n",
      "\n",
      "It is crucial to use a training set that is representative of the cases you want to general‐\n",
      "ize  to.  This  is  often  harder  than  it  sounds:  if  the  sample  is  too  small,  you  will  have\n",
      "sampling noise (i.e., nonrepresentative data as a result of chance), but even very large\n",
      "samples  can  be  nonrepresentative  if  the  sampling  method  is  flawed.  This  is  called\n",
      "sampling bias.\n",
      "\n",
      "A Famous Example of Sampling Bias\n",
      "Perhaps  the  most  famous  example  of  sampling  bias  happened  during  the  US  presi‐\n",
      "dential  election  in  1936,  which  pitted  Landon  against  Roosevelt:  the  Literary  Digest\n",
      "conducted a very large poll, sending mail to about 10 million people. It got 2.4 million\n",
      "answers, and predicted with high confidence that Landon would get 57% of the votes.\n",
      "\n",
      "26 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 1: The Machine Learning Landscape\n",
      "\n",
      "\fInstead,  Roosevelt  won  with  62%  of  the  votes.  The  flaw  was  in  the  Literary  Digest’s\n",
      "sampling method:\n",
      "\n",
      "• First,  to  obtain  the  addresses  to  send  the  polls  to,  the  Literary  Digest  used  tele‐\n",
      "phone directories, lists of magazine subscribers, club membership lists, and the\n",
      "like. All of these lists tend to favor wealthier people, who are more likely to vote\n",
      "Republican (hence Landon).\n",
      "\n",
      "• Second, less than 25% of the people who received the poll answered. Again, this\n",
      "introduces a sampling bias, by ruling out people who don’t care much about poli‐\n",
      "tics, people who don’t like the Literary Digest, and other key groups. This is a spe‐\n",
      "cial type of sampling bias called nonresponse bias.\n",
      "\n",
      "Here is another example: say you want to build a system to recognize funk music vid‐\n",
      "eos. One way to build your training set is to search “funk music” on YouTube and use\n",
      "the  resulting  videos.  But  this  assumes  that  YouTube’s  search  engine  returns  a  set  of\n",
      "videos that are representative of all the funk music videos on YouTube. In reality, the\n",
      "search results are likely to be biased toward popular artists (and if you live in Brazil\n",
      "you will get a lot of “funk carioca” videos, which sound nothing like James Brown).\n",
      "On the other hand, how else can you get a large training set?\n",
      "\n",
      "Poor-Quality Data\n",
      "Obviously, if your training data is full of errors, outliers, and noise (e.g., due to poor-\n",
      "quality measurements), it will make it harder for the system to detect the underlying\n",
      "patterns, so your system is less likely to perform well. It is often well worth the effort\n",
      "to spend time cleaning up your training data. The truth is, most data scientists spend\n",
      "a significant part of their time doing just that. For example:\n",
      "\n",
      "• If some instances are clearly outliers, it may help to simply discard them or try to\n",
      "\n",
      "fix the errors manually.\n",
      "\n",
      "• If some instances are missing a few features (e.g., 5% of your customers did not\n",
      "specify their age), you must decide whether you want to ignore this attribute alto‐\n",
      "gether,  ignore  these  instances,  fill  in  the  missing  values  (e.g.,  with  the  median\n",
      "age), or train one model with the feature and one model without it, and so on.\n",
      "\n",
      "Irrelevant Features\n",
      "As the saying goes: garbage in, garbage out. Your system will only be capable of learn‐\n",
      "ing if the training data contains enough relevant features and not too many irrelevant\n",
      "ones. A critical part of the success of a Machine Learning project is coming up with a\n",
      "good set of features to train on. This process, called feature engineering, involves:\n",
      "\n",
      "Main Challenges of Machine Learning \n",
      "\n",
      "| \n",
      "\n",
      "27\n",
      "\n",
      "\f• Feature  selection:  selecting  the  most  useful  features  to  train  on  among  existing\n",
      "\n",
      "features.\n",
      "\n",
      "• Feature extraction: combining existing features to produce a more useful one (as\n",
      "\n",
      "we saw earlier, dimensionality reduction algorithms can help).\n",
      "\n",
      "• Creating new features by gathering new data.\n",
      "\n",
      "Now that we have looked at many examples of bad data, let’s look at a couple of exam‐\n",
      "ples of bad algorithms.\n",
      "\n",
      "Overfitting the Training Data\n",
      "Say you are visiting a foreign country and the taxi driver rips you off. You might be\n",
      "tempted  to  say  that  all  taxi  drivers  in  that  country  are  thieves.  Overgeneralizing  is\n",
      "something that we humans do all too often, and unfortunately machines can fall into\n",
      "the same trap if we are not careful. In Machine Learning this is called overfitting: it\n",
      "means that the model performs well on the training data, but it does not generalize\n",
      "well.\n",
      "\n",
      "Figure  1-22  shows  an  example  of  a  high-degree  polynomial  life  satisfaction  model\n",
      "that strongly overfits the training data. Even though it performs much better on the\n",
      "training data than the simple linear model, would you really trust its predictions?\n",
      "\n",
      "Figure 1-22. Overfitting the training data\n",
      "\n",
      "Complex models such as deep neural networks can detect subtle patterns in the data,\n",
      "but if the training set is noisy, or if it is too small (which introduces sampling noise),\n",
      "then the model is likely to detect patterns in the noise itself. Obviously these patterns\n",
      "will not generalize to new instances. For example, say you feed your life satisfaction\n",
      "model  many  more  attributes,  including  uninformative  ones  such  as  the  country’s\n",
      "name. In that case, a complex model may detect patterns like the fact that all coun‐\n",
      "tries in the training data with a w in their name have a life satisfaction greater than 7:\n",
      "New Zealand (7.3), Norway (7.4), Sweden (7.2), and Switzerland (7.5). How confident\n",
      "\n",
      "28 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 1: The Machine Learning Landscape\n",
      "\n",
      "\fare you that the W-satisfaction rule generalizes to Rwanda or Zimbabwe? Obviously\n",
      "this pattern occurred in the training data by pure chance, but the model has no way\n",
      "to tell whether a pattern is real or simply the result of noise in the data.\n",
      "\n",
      "Overfitting happens when the model is too complex relative to the\n",
      "amount  and  noisiness  of  the  training  data.  The  possible  solutions\n",
      "are:\n",
      "\n",
      "• To simplify the model by selecting one with fewer parameters\n",
      "(e.g.,  a  linear  model  rather  than  a  high-degree  polynomial\n",
      "model),  by  reducing  the  number  of  attributes  in  the  training\n",
      "data or by constraining the model\n",
      "\n",
      "• To gather more training data\n",
      "\n",
      "• To  reduce  the  noise  in  the  training  data  (e.g.,  fix  data  errors\n",
      "\n",
      "and remove outliers)\n",
      "\n",
      "Constraining a model to make it simpler and reduce the risk of overfitting is called\n",
      "regularization. For example, the linear model we defined earlier has two parameters,\n",
      "θ0 and θ1. This gives the learning algorithm two degrees of freedom to adapt the model\n",
      "to the training data: it can tweak both the height (θ0) and the slope (θ1) of the line. If\n",
      "we forced θ1 = 0, the algorithm would have only one degree of freedom and would\n",
      "have a much harder time fitting the data properly: all it could do is move the line up\n",
      "or  down  to  get  as  close  as  possible  to  the  training  instances,  so  it  would  end  up\n",
      "around the mean. A very simple model indeed! If we allow the algorithm to modify θ1\n",
      "but we force it to keep it small, then the learning algorithm will effectively have some‐\n",
      "where in between one and two degrees of freedom. It will produce a simpler model\n",
      "than with two degrees of freedom, but more complex than with just one. You want to\n",
      "find  the  right  balance  between  fitting  the  training  data  perfectly  and  keeping  the\n",
      "model simple enough to ensure that it will generalize well.\n",
      "\n",
      "Figure  1-23  shows  three  models:  the  dotted  line  represents  the  original  model  that\n",
      "was trained with a few countries missing, the dashed line is our second model trained\n",
      "with all countries, and the solid line is a linear model trained with the same data as\n",
      "the  first  model  but  with  a  regularization  constraint.  You  can  see  that  regularization\n",
      "forced the model to have a smaller slope, which fits a bit less the training data that the\n",
      "model was trained on, but actually allows it to generalize better to new examples.\n",
      "\n",
      "Main Challenges of Machine Learning \n",
      "\n",
      "| \n",
      "\n",
      "29\n",
      "\n",
      "\fFigure 1-23. Regularization reduces the risk of overfitting\n",
      "\n",
      "The amount of regularization to apply during learning can be controlled by a hyper‐\n",
      "parameter.  A  hyperparameter  is  a  parameter  of  a  learning  algorithm  (not  of  the\n",
      "model). As such, it is not affected by the learning algorithm itself; it must be set prior\n",
      "to training and remains constant during training. If you set the regularization hyper‐\n",
      "parameter  to  a  very  large  value,  you  will  get  an  almost  flat  model  (a  slope  close  to\n",
      "zero); the learning algorithm will almost certainly not overfit the training data, but it\n",
      "will  be  less  likely  to  find  a  good  solution.  Tuning  hyperparameters  is  an  important\n",
      "part  of  building  a  Machine  Learning  system  (you  will  see  a  detailed  example  in  the\n",
      "next chapter).\n",
      "\n",
      "Underfitting the Training Data\n",
      "As  you  might  guess,  underfitting  is  the  opposite  of  overfitting:  it  occurs  when  your\n",
      "model is too simple to learn the underlying structure of the data. For example, a lin‐\n",
      "ear  model  of  life  satisfaction  is  prone  to  underfit;  reality  is  just  more  complex  than\n",
      "the model, so its predictions are bound to be inaccurate, even on the training exam‐\n",
      "ples.\n",
      "\n",
      "The main options to fix this problem are:\n",
      "\n",
      "• Selecting a more powerful model, with more parameters\n",
      "\n",
      "• Feeding better features to the learning algorithm (feature engineering)\n",
      "\n",
      "• Reducing the constraints on the model (e.g., reducing the regularization hyper‐\n",
      "\n",
      "parameter)\n",
      "\n",
      "Stepping Back\n",
      "By now you already know a lot about Machine Learning. However, we went through\n",
      "so many concepts that you may be feeling a little lost, so let’s step back and look at the\n",
      "big picture:\n",
      "\n",
      "30 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 1: The Machine Learning Landscape\n",
      "\n",
      "\f• Machine Learning is about making machines get better at some task by learning\n",
      "\n",
      "from data, instead of having to explicitly code rules.\n",
      "\n",
      "• There are many different types of ML systems: supervised or not, batch or online,\n",
      "\n",
      "instance-based or model-based, and so on.\n",
      "\n",
      "• In a ML project you gather data in a training set, and you feed the training set to\n",
      "a learning algorithm. If the algorithm is model-based it tunes some parameters to\n",
      "fit the model to the training set (i.e., to make good predictions on the training set\n",
      "itself), and then hopefully it will be able to make good predictions on new cases\n",
      "as well. If the algorithm is instance-based, it just learns the examples by heart and\n",
      "generalizes to new instances by comparing them to the learned instances using a\n",
      "similarity measure.\n",
      "\n",
      "• The system will not perform well if your training set is too small, or if the data is\n",
      "not representative, noisy, or polluted with irrelevant features (garbage in, garbage\n",
      "out).  Lastly,  your  model  needs  to  be  neither  too  simple  (in  which  case  it  will\n",
      "underfit) nor too complex (in which case it will overfit).\n",
      "\n",
      "There’s  just  one  last  important  topic  to  cover:  once  you  have  trained  a  model,  you\n",
      "don’t want to just “hope” it generalizes to new cases. You want to evaluate it, and fine-\n",
      "tune it if necessary. Let’s see how.\n",
      "\n",
      "Testing and Validating\n",
      "The only way to know how well a model will generalize to new cases is to actually try\n",
      "it out on new cases. One way to do that is to put your model in production and moni‐\n",
      "tor  how  well  it  performs.  This  works  well,  but  if  your  model  is  horribly  bad,  your\n",
      "users will complain—not the best idea.\n",
      "\n",
      "A better option is to split your data into two sets: the training set and the test set. As\n",
      "these names imply, you train your model using the training set, and you test it using\n",
      "the test set. The error rate on new cases is called the generalization error (or out-of-\n",
      "sample error), and by evaluating your model on the test set, you get an estimate of this\n",
      "error. This value tells you how well your model will perform on instances it has never\n",
      "seen before.\n",
      "\n",
      "If the training error is low (i.e., your model makes few mistakes on the training set)\n",
      "but the generalization error is high, it means that your model is overfitting the train‐\n",
      "ing data.\n",
      "\n",
      "It is common to use 80% of the data for training and hold out 20%\n",
      "for  testing.  However,  this  depends  on  the  size  of  the  dataset:  if  it\n",
      "contains 10 million instances, then holding out 1% means your test\n",
      "set  will  contain  100,000  instances:  that’s  probably  more  than\n",
      "enough to get a good estimate of the generalization error.\n",
      "\n",
      "Testing and Validating \n",
      "\n",
      "| \n",
      "\n",
      "31\n",
      "\n",
      "\fHyperparameter Tuning and Model Selection\n",
      "So evaluating a model is simple enough: just use a test set. Now suppose you are hesi‐\n",
      "tating  between  two  models  (say  a  linear  model  and  a  polynomial  model):  how  can\n",
      "you decide? One option is to train both and compare how well they generalize using\n",
      "the test set.\n",
      "\n",
      "Now  suppose  that  the  linear  model  generalizes  better,  but  you  want  to  apply  some \n",
      "regularization to avoid overfitting. The question is: how do you choose the value of\n",
      "the regularization hyperparameter? One option is to train 100 different models using\n",
      "100 different values for this hyperparameter. Suppose you find the best hyperparame‐\n",
      "ter value that produces a model with the lowest generalization error, say just 5% error.\n",
      "\n",
      "So you launch this model into production, but unfortunately it does not perform as\n",
      "well as expected and produces 15% errors. What just happened?\n",
      "\n",
      "The problem is that you measured the generalization error multiple times on the test\n",
      "set, and you adapted the model and hyperparameters to produce the best model for\n",
      "that particular set. This means that the model is unlikely to perform as well on new\n",
      "data.\n",
      "\n",
      "A common solution to this problem is called holdout validation: you simply hold out\n",
      "part of the training set to evaluate several candidate models and select the best one.\n",
      "The new heldout set is called the validation set (or sometimes the development set, or\n",
      "dev  set).  More  specifically,  you  train  multiple  models  with  various  hyperparameters\n",
      "on  the  reduced  training  set  (i.e.,  the  full  training  set  minus  the  validation  set),  and\n",
      "you select the model that performs best on the validation set. After this holdout vali‐\n",
      "dation process, you train the best model on the full training set (including the valida‐\n",
      "tion set), and this gives you the final model. Lastly, you evaluate this final model on\n",
      "the test set to get an estimate of the generalization error.\n",
      "\n",
      "This solution usually works quite well. However, if the validation set is too small, then\n",
      "model evaluations will be imprecise: you may end up selecting a suboptimal model by\n",
      "mistake. Conversely, if the validation set is too large, then the remaining training set\n",
      "will be much smaller than the full training set. Why is this bad? Well, since the final\n",
      "model  will  be  trained  on  the  full  training  set,  it  is  not  ideal  to  compare  candidate\n",
      "models trained on a much smaller training set. It would be like selecting the fastest\n",
      "sprinter  to  participate  in  a  marathon.  One  way  to  solve  this  problem  is  to  perform\n",
      "repeated  cross-validation,  using  many  small  validation  sets.  Each  model  is  evaluated\n",
      "once per validation set, after it is trained on the rest of the data. By averaging out all\n",
      "the evaluations of a model, we get a much more accurate measure of its performance.\n",
      "However, there is a drawback: the training time is multiplied by the number of valida‐\n",
      "tion sets.\n",
      "\n",
      "32 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 1: The Machine Learning Landscape\n",
      "\n",
      "\fData Mismatch\n",
      "In some cases, it is easy to get a large amount of data for training, but it is not per‐\n",
      "fectly representative of the data that will be used in production. For example, suppose\n",
      "you want to create a mobile app to take pictures of flowers and automatically deter‐\n",
      "mine  their  species.  You  can  easily  download  millions  of  pictures  of  flowers  on  the\n",
      "web,  but  they  won’t  be  perfectly  representative  of  the  pictures  that  will  actually  be\n",
      "taken using the app on a mobile device. Perhaps you only have 10,000 representative\n",
      "pictures  (i.e.,  actually  taken  with  the  app).  In  this  case,  the  most  important  rule  to\n",
      "remember is that the validation set and the test must be as representative as possible\n",
      "of the data you expect to use in production, so they should be composed exclusively\n",
      "of representative pictures: you can shuffle them and put half in the validation set, and\n",
      "half in the test set (making sure that no duplicates or near-duplicates end up in both\n",
      "sets). After training your model on the web pictures, if you observe that the perfor‐\n",
      "mance  of  your  model  on  the  validation  set  is  disappointing,  you  will  not  know\n",
      "whether this is because your model has overfit the training set, or whether this is just\n",
      "due to the mismatch between the web pictures and the mobile app pictures. One sol‐\n",
      "ution is to hold out part of the training pictures (from the web) in yet another set that\n",
      "Andrew Ng calls the train-dev set. After the model is trained (on the training set, not\n",
      "on the train-dev set), you can evaluate it on the train-dev set: if it performs well, then\n",
      "the model is not overfitting the training set, so if performs poorly on the validation\n",
      "set, the problem must come from the data mismatch. You can try to tackle this prob‐\n",
      "lem by preprocessing the web images to make them look more like the pictures that\n",
      "will  be  taken  by  the  mobile  app,  and  then  retraining  the  model.  Conversely,  if  the\n",
      "model  performs  poorly  on  the  train-dev  set,  then  the  model  must  have  overfit  the\n",
      "training set, so you should try to simplify or regularize the model, get more training\n",
      "data and clean up the training data, as discussed earlier.\n",
      "\n",
      "No Free Lunch Theorem\n",
      "A model is a simplified version of the observations. The simplifications are meant to\n",
      "discard the superfluous details that are unlikely to generalize to new instances. How‐\n",
      "ever, to decide what data to discard and what data to keep, you must make assump‐\n",
      "tions.  For  example,  a  linear  model  makes  the  assumption  that  the  data  is\n",
      "fundamentally linear and that the distance between the instances and the straight line\n",
      "is just noise, which can safely be ignored.\n",
      "\n",
      "In  a  famous  1996  paper,11  David  Wolpert  demonstrated  that  if  you  make  absolutely\n",
      "no assumption about the data, then there is no reason to prefer one model over any\n",
      "other.  This  is  called  the  No  Free  Lunch  (NFL)  theorem.  For  some  datasets  the  best\n",
      "\n",
      "11 “The Lack of A Priori Distinctions Between Learning Algorithms,” D. Wolpert (1996).\n",
      "\n",
      "Testing and Validating \n",
      "\n",
      "| \n",
      "\n",
      "33\n",
      "\n",
      "\fmodel is a linear model, while for other datasets it is a neural network. There is no\n",
      "model that is a priori guaranteed to work better (hence the name of the theorem). The\n",
      "only way to know for sure which model is best is to evaluate them all. Since this is not\n",
      "possible, in practice you make some reasonable assumptions about the data and you\n",
      "evaluate only a few reasonable models. For example, for simple tasks you may evalu‐\n",
      "ate linear models with various levels of regularization, and for a complex problem you\n",
      "may evaluate various neural networks.\n",
      "\n",
      "Exercises\n",
      "In  this  chapter  we  have  covered  some  of  the  most  important  concepts  in  Machine\n",
      "Learning. In the next chapters we will dive deeper and write more code, but before we\n",
      "do, make sure you know how to answer the following questions:\n",
      "\n",
      "1. How would you define Machine Learning?\n",
      "\n",
      "2. Can you name four types of problems where it shines?\n",
      "\n",
      "3. What is a labeled training set?\n",
      "\n",
      "4. What are the two most common supervised tasks?\n",
      "\n",
      "5. Can you name four common unsupervised tasks?\n",
      "\n",
      "6. What  type  of  Machine  Learning  algorithm  would  you  use  to  allow  a  robot  to\n",
      "\n",
      "walk in various unknown terrains?\n",
      "\n",
      "7. What type of algorithm would you use to segment your customers into multiple\n",
      "\n",
      "groups?\n",
      "\n",
      "8. Would you frame the problem of spam detection as a supervised learning prob‐\n",
      "\n",
      "lem or an unsupervised learning problem?\n",
      "\n",
      "9. What is an online learning system?\n",
      "\n",
      "10. What is out-of-core learning?\n",
      "\n",
      "11. What type of learning algorithm relies on a similarity measure to make predic‐\n",
      "\n",
      "tions?\n",
      "\n",
      "12. What  is  the  difference  between  a  model  parameter  and  a  learning  algorithm’s\n",
      "\n",
      "hyperparameter?\n",
      "\n",
      "13. What do model-based learning algorithms search for? What is the most common\n",
      "\n",
      "strategy they use to succeed? How do they make predictions?\n",
      "\n",
      "14. Can you name four of the main challenges in Machine Learning?\n",
      "\n",
      "15. If your model performs great on the training data but generalizes poorly to new\n",
      "\n",
      "instances, what is happening? Can you name three possible solutions?\n",
      "\n",
      "16. What is a test set and why would you want to use it?\n",
      "\n",
      "34 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 1: The Machine Learning Landscape\n",
      "\n",
      "\f17. What is the purpose of a validation set?\n",
      "\n",
      "18. What can go wrong if you tune hyperparameters using the test set?\n",
      "\n",
      "19. What is repeated cross-validation and why would you prefer it to using a single\n",
      "\n",
      "validation set?\n",
      "\n",
      "Solutions to these exercises are available in ???.\n",
      "\n",
      "Exercises \n",
      "\n",
      "| \n",
      "\n",
      "35\n",
      "\n",
      "\f\fCHAPTER 2\n",
      "End-to-End Machine Learning Project\n",
      "\n",
      "With  Early  Release  ebooks,  you  get  books  in  their  earliest  form—\n",
      "the author’s raw and unedited content as he or she writes—so you\n",
      "can  take  advantage  of  these  technologies  long  before  the  official\n",
      "release of these titles. The following will be Chapter 2 in the final\n",
      "release of the book.\n",
      "\n",
      "In this chapter, you will go through an example project end to end, pretending to be a\n",
      "recently hired data scientist in a real estate company.1 Here are the main steps you will\n",
      "go through:\n",
      "\n",
      "1. Look at the big picture.\n",
      "\n",
      "2. Get the data.\n",
      "\n",
      "3. Discover and visualize the data to gain insights.\n",
      "\n",
      "4. Prepare the data for Machine Learning algorithms.\n",
      "\n",
      "5. Select a model and train it.\n",
      "\n",
      "6. Fine-tune your model.\n",
      "\n",
      "7. Present your solution.\n",
      "\n",
      "8. Launch, monitor, and maintain your system.\n",
      "\n",
      "1 The example project is completely fictitious; the goal is just to illustrate the main steps of a Machine Learning\n",
      "\n",
      "project, not to learn anything about the real estate business.\n",
      "\n",
      "37\n",
      "\n",
      "\fWorking with Real Data\n",
      "When you are learning about Machine Learning it is best to actually experiment with\n",
      "real-world  data,  not  just  artificial  datasets.  Fortunately,  there  are  thousands  of  open\n",
      "datasets  to  choose  from,  ranging  across  all  sorts  of  domains.  Here  are  a  few  places\n",
      "you can look to get data:\n",
      "\n",
      "• Popular open data repositories:\n",
      "\n",
      "— UC Irvine Machine Learning Repository\n",
      "\n",
      "— Kaggle datasets\n",
      "\n",
      "— Amazon’s AWS datasets\n",
      "\n",
      "• Meta portals (they list open data repositories):\n",
      "\n",
      "— http://dataportals.org/\n",
      "\n",
      "— http://opendatamonitor.eu/\n",
      "\n",
      "— http://quandl.com/\n",
      "\n",
      "• Other pages listing many popular open data repositories:\n",
      "\n",
      "— Wikipedia’s list of Machine Learning datasets\n",
      "\n",
      "— Quora.com question\n",
      "\n",
      "— Datasets subreddit\n",
      "\n",
      "In this chapter we chose the California Housing Prices dataset from the StatLib repos‐\n",
      "itory2 (see Figure 2-1). This dataset was based on data from the 1990 California cen‐\n",
      "sus. It is not exactly recent (you could still afford a nice house in the Bay Area at the\n",
      "time), but it has many qualities for learning, so we will pretend it is recent data. We\n",
      "also added a categorical attribute and removed a few features for teaching purposes.\n",
      "\n",
      "2 The original dataset appeared in R. Kelley Pace and Ronald Barry, “Sparse Spatial Autoregressions,” Statistics\n",
      "\n",
      "& Probability Letters 33, no. 3 (1997): 291–297.\n",
      "\n",
      "38 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "\fFigure 2-1. California housing prices\n",
      "\n",
      "Look at the Big Picture\n",
      "Welcome to Machine Learning Housing Corporation! The first task you are asked to\n",
      "perform is to build a model of housing prices in California using the California cen‐\n",
      "sus data. This data has metrics such as the population, median income, median hous‐\n",
      "ing price, and so on for each block group in California. Block groups are the smallest\n",
      "geographical  unit  for  which  the  US  Census  Bureau  publishes  sample  data  (a  block\n",
      "group typically has a population of 600 to 3,000 people). We will just call them “dis‐\n",
      "tricts” for short.\n",
      "\n",
      "Your  model  should  learn  from  this  data  and  be  able  to  predict  the  median  housing\n",
      "price in any district, given all the other metrics.\n",
      "\n",
      "Since you are a well-organized data scientist, the first thing you do\n",
      "is  to  pull  out  your  Machine  Learning  project  checklist.  You  can\n",
      "start  with  the  one  in  ???;  it  should  work  reasonably  well  for  most\n",
      "Machine Learning projects but make sure to adapt it to your needs.\n",
      "In this chapter we will go through many checklist items, but we will\n",
      "also skip a few, either because they are self-explanatory or because\n",
      "they will be discussed in later chapters.\n",
      "\n",
      "Frame the Problem\n",
      "The first question to ask your boss is what exactly is the business objective; building a\n",
      "model is probably not the end goal. How does the company expect to use and benefit\n",
      "\n",
      "Look at the Big Picture \n",
      "\n",
      "| \n",
      "\n",
      "39\n",
      "\n",
      "\ffrom  this  model?  This  is  important  because  it  will  determine  how  you  frame  the\n",
      "problem, what algorithms you will select, what performance measure you will use to\n",
      "evaluate your model, and how much effort you should spend tweaking it.\n",
      "\n",
      "Your boss answers that your model’s output (a prediction of a district’s median hous‐\n",
      "ing  price)  will  be  fed  to  another  Machine  Learning  system  (see  Figure  2-2),  along\n",
      "with many other signals.3 This downstream system will determine whether it is worth\n",
      "investing in a given area or not. Getting this right is critical, as it directly affects reve‐\n",
      "nue.\n",
      "\n",
      "Figure 2-2. A Machine Learning pipeline for real estate investments\n",
      "\n",
      "Pipelines\n",
      "A sequence of data processing components is called a data pipeline. Pipelines are very\n",
      "common in Machine Learning systems, since there is a lot of data to manipulate and\n",
      "many data transformations to apply.\n",
      "\n",
      "Components typically run asynchronously. Each component pulls in a large amount\n",
      "of data, processes it, and spits out the result in another data store, and then some time\n",
      "later the next component in the pipeline pulls this data and spits out its own output,\n",
      "and so on. Each component is fairly self-contained: the interface between components\n",
      "is simply the data store. This makes the system quite simple to grasp (with the help of\n",
      "a data flow graph), and different teams can focus on different components. Moreover,\n",
      "if a component breaks down, the downstream components can often continue to run\n",
      "normally (at least for a while) by just using the last output from the broken compo‐\n",
      "nent. This makes the architecture quite robust.\n",
      "\n",
      "3 A piece of information fed to a Machine Learning system is often called a signal in reference to Shannon’s\n",
      "\n",
      "information theory: you want a high signal/noise ratio.\n",
      "\n",
      "40 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "\fOn  the  other  hand,  a  broken  component  can  go  unnoticed  for  some  time  if  proper\n",
      "monitoring  is  not  implemented.  The  data  gets  stale  and  the  overall  system’s  perfor‐\n",
      "mance drops.\n",
      "\n",
      "The next question to ask is what the current solution looks like (if any). It will often\n",
      "give  you  a  reference  performance,  as  well  as  insights  on  how  to  solve  the  problem.\n",
      "Your  boss  answers  that  the  district  housing  prices  are  currently  estimated  manually\n",
      "by  experts:  a  team  gathers  up-to-date  information  about  a  district,  and  when  they\n",
      "cannot get the median housing price, they estimate it using complex rules.\n",
      "\n",
      "This is costly and time-consuming, and their estimates are not great; in cases where\n",
      "they manage to find out the actual median housing price, they often realize that their\n",
      "estimates were off by more than 20%. This is why the company thinks that it would\n",
      "be useful to train a model to predict a district’s median housing price given other data\n",
      "about that district. The census data looks like a great dataset to exploit for this pur‐\n",
      "pose, since it includes the median housing prices of thousands of districts, as well as\n",
      "other data.\n",
      "\n",
      "Okay,  with  all  this  information  you  are  now  ready  to  start  designing  your  system.\n",
      "First,  you  need  to  frame  the  problem:  is  it  supervised,  unsupervised,  or  Reinforce‐\n",
      "ment Learning? Is it a classification task, a regression task, or something else? Should\n",
      "you use batch learning or online learning techniques? Before you read on, pause and\n",
      "try to answer these questions for yourself.\n",
      "\n",
      "Have you found the answers? Let’s see: it is clearly a typical supervised learning task\n",
      "since you are given labeled training examples (each instance comes with the expected\n",
      "output, i.e., the district’s median housing price). Moreover, it is also a typical regres‐\n",
      "sion task, since you are asked to predict a value. More specifically, this is a multiple\n",
      "regression problem since the system will use multiple features to make a prediction (it\n",
      "will  use  the  district’s  population,  the  median  income,  etc.).  It  is  also  a  univariate\n",
      "regression problem since we are only trying to predict a single value for each district.\n",
      "If  we  were  trying  to  predict  multiple  values  per  district,  it  would  be  a  multivariate\n",
      "regression problem. Finally, there is no continuous flow of data coming in the system,\n",
      "there  is  no  particular  need  to  adjust  to  changing  data  rapidly,  and  the  data  is  small\n",
      "enough to fit in memory, so plain batch learning should do just fine.\n",
      "\n",
      "If  the  data  was  huge,  you  could  either  split  your  batch  learning\n",
      "work across multiple servers (using the MapReduce technique), or\n",
      "you could use an online learning technique instead.\n",
      "\n",
      "Look at the Big Picture \n",
      "\n",
      "| \n",
      "\n",
      "41\n",
      "\n",
      "\fSelect a Performance Measure\n",
      "Your next step is to select a performance measure. A typical performance measure for\n",
      "regression problems is the Root Mean Square Error (RMSE). It gives an idea of how\n",
      "much  error  the  system  typically  makes  in  its  predictions,  with  a  higher  weight  for\n",
      "large errors. Equation 2-1 shows the mathematical formula to compute the RMSE.\n",
      "\n",
      "Equation 2-1. Root Mean Square Error (RMSE)\n",
      "\n",
      "RMSE X, h =\n",
      "\n",
      "m\n",
      "\n",
      "1\n",
      "m ∑\n",
      "\n",
      "i = 1\n",
      "\n",
      "h x i − y i 2\n",
      "\n",
      "42 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "\fNotations\n",
      "This equation introduces several very common Machine Learning notations that we\n",
      "will use throughout this book:\n",
      "\n",
      "• m is the number of instances in the dataset you are measuring the RMSE on.\n",
      "\n",
      "— For example, if you are evaluating the RMSE on a validation set of 2,000 dis‐\n",
      "\n",
      "tricts, then m = 2,000.\n",
      "\n",
      "• x(i) is a vector of all the feature values (excluding the label) of the ith instance in\n",
      "\n",
      "the dataset, and y(i) is its label (the desired output value for that instance).\n",
      "\n",
      "— For example, if the first district in the dataset is located at longitude –118.29°,\n",
      "latitude 33.91°, and it has 1,416 inhabitants with a median income of $38,372,\n",
      "and the median house value is $156,400 (ignoring the other features for now),\n",
      "then:\n",
      "\n",
      "x 1 =\n",
      "\n",
      "−118 . 29\n",
      "33 . 91\n",
      "1, 416\n",
      "38, 372\n",
      "\n",
      "and:\n",
      "\n",
      "y 1 = 156, 400\n",
      "\n",
      "• X is a matrix containing all the feature values (excluding labels) of all instances in\n",
      "the dataset. There is one row per instance and the ith row is equal to the transpose\n",
      "of x(i), noted (x(i))T.4\n",
      "\n",
      "— For example, if the first district is as just described, then the matrix X looks\n",
      "\n",
      "like this:\n",
      "\n",
      "X =\n",
      "\n",
      "x 1 T\n",
      "x 2 T\n",
      "⋮\n",
      "x 1999 T\n",
      "x 2000 T\n",
      "\n",
      "=\n",
      "\n",
      "−118 . 29 33 . 91 1, 416 38, 372\n",
      "\n",
      "⋮\n",
      "\n",
      "⋮\n",
      "\n",
      "⋮\n",
      "\n",
      "⋮\n",
      "\n",
      "4 Recall that the transpose operator flips a column vector into a row vector (and vice versa).\n",
      "\n",
      "Look at the Big Picture \n",
      "\n",
      "| \n",
      "\n",
      "43\n",
      "\n",
      "\f• h is your system’s prediction function, also called a hypothesis. When your system\n",
      "is given an instance’s feature vector x(i), it outputs a predicted value ŷ(i) = h(x(i))\n",
      "for that instance (ŷ is pronounced “y-hat”).\n",
      "\n",
      "— For example, if your system predicts that the median housing price in the first\n",
      "district is $158,400, then ŷ(1) = h(x(1)) = 158,400. The prediction error for this\n",
      "district is ŷ(1) – y(1) = 2,000.\n",
      "\n",
      "• RMSE(X,h)  is  the  cost  function  measured  on  the  set  of  examples  using  your\n",
      "\n",
      "hypothesis h.\n",
      "\n",
      "We use lowercase italic font for scalar values (such as m or y(i)) and function names\n",
      "(such as h), lowercase bold font for vectors (such as x(i)), and uppercase bold font for\n",
      "matrices (such as X).\n",
      "\n",
      "Even though the RMSE is generally the preferred performance measure for regression\n",
      "tasks, in some contexts you may prefer to use another function. For example, suppose\n",
      "that there are many outlier districts. In that case, you may consider using the Mean\n",
      "Absolute Error (also called the Average Absolute Deviation; see Equation 2-2):\n",
      "\n",
      "Equation 2-2. Mean Absolute Error\n",
      "\n",
      "MAE X, h =\n",
      "\n",
      "m\n",
      "\n",
      "1\n",
      "m ∑\n",
      "\n",
      "i = 1\n",
      "\n",
      "h x i − y i\n",
      "\n",
      "Both the RMSE and the MAE are ways to measure the distance between two vectors:\n",
      "the vector of predictions and the vector of target values. Various distance measures,\n",
      "or norms, are possible:\n",
      "\n",
      "• Computing  the  root  of  a  sum  of  squares  (RMSE)  corresponds  to  the  Euclidean\n",
      "norm:  it  is  the  notion  of  distance  you  are  familiar  with.  It  is  also  called  the  ℓ2\n",
      "norm, noted ∥ · ∥2 (or just ∥ · ∥).\n",
      "\n",
      "• Computing the sum of absolutes (MAE) corresponds to the ℓ1 norm, noted ∥ · ∥1.\n",
      "It  is  sometimes  called  the  Manhattan  norm  because  it  measures  the  distance\n",
      "between two points in a city if you can only travel along orthogonal city blocks.\n",
      "\n",
      "• More  generally,  the  ℓk  norm  of  a  vector  v  containing  n  elements  is  defined  as\n",
      "\n",
      "∥  ∥k = v0\n",
      "ments in the vector, and ℓ∞ gives the maximum absolute value in the vector.\n",
      "\n",
      "k + ⋯ + vn\n",
      "\n",
      "1\n",
      "k.  ℓ0  just  gives  the  number  of  non-zero  ele‐\n",
      "\n",
      "k + v1\n",
      "\n",
      "k\n",
      "\n",
      "• The higher the norm index, the more it focuses on large values and neglects small\n",
      "ones. This is why the RMSE is more sensitive to outliers than the MAE. But when\n",
      "\n",
      "44 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "\foutliers are exponentially rare (like in a bell-shaped curve), the RMSE performs\n",
      "very well and is generally preferred.\n",
      "\n",
      "Check the Assumptions\n",
      "Lastly, it is good practice to list and verify the assumptions that were made so far (by\n",
      "you or others); this can catch serious issues early on. For example, the district prices\n",
      "that  your  system  outputs  are  going  to  be  fed  into  a  downstream  Machine  Learning\n",
      "system, and we assume that these prices are going to be used as such. But what if the\n",
      "downstream  system  actually  converts  the  prices  into  categories  (e.g.,  “cheap,”\n",
      "“medium,” or “expensive”) and then uses those categories instead of the prices them‐\n",
      "selves? In this case, getting the price perfectly right is not important at all; your sys‐\n",
      "tem  just  needs  to  get  the  category  right.  If  that’s  so,  then  the  problem  should  have\n",
      "been framed as a classification task, not a regression task. You don’t want to find this\n",
      "out after working on a regression system for months.\n",
      "\n",
      "Fortunately, after talking with the team in charge of the downstream system, you are\n",
      "confident that they do indeed need the actual prices, not just categories. Great! You’re\n",
      "all set, the lights are green, and you can start coding now!\n",
      "\n",
      "Get the Data\n",
      "It’s  time  to  get  your  hands  dirty.  Don’t  hesitate  to  pick  up  your  laptop  and  walk\n",
      "through  the  following  code  examples  in  a  Jupyter  notebook.  The  full  Jupyter  note‐\n",
      "book is available at https://github.com/ageron/handson-ml2.\n",
      "\n",
      "Create the Workspace\n",
      "First you will need to have Python installed. It is probably already installed on your\n",
      "system. If not, you can get it at https://www.python.org/.5\n",
      "\n",
      "Next you need to create a workspace directory for your Machine Learning code and\n",
      "datasets. Open a terminal and type the following commands (after the $ prompts):\n",
      "\n",
      "$ export ML_PATH=\"$HOME/ml\"      # You can change the path if you prefer\n",
      "$ mkdir -p $ML_PATH\n",
      "\n",
      "You will need a number of Python modules: Jupyter, NumPy, Pandas, Matplotlib, and\n",
      "Scikit-Learn.  If  you  already  have  Jupyter  running  with  all  these  modules  installed,\n",
      "you can safely skip to “Download the Data” on page 49. If you don’t have them yet,\n",
      "there are many ways to install them (and their dependencies). You can use your sys‐\n",
      "\n",
      "5 The latest version of Python 3 is recommended. Python 2.7+ may work too, but it is now deprecated, all major\n",
      "\n",
      "scientific libraries are dropping support for it, so you should migrate to Python 3 as soon as possible.\n",
      "\n",
      "Get the Data \n",
      "\n",
      "| \n",
      "\n",
      "45\n",
      "\n",
      "\ftem’s  packaging  system  (e.g.,  apt-get  on  Ubuntu,  or  MacPorts  or  HomeBrew  on\n",
      "MacOS), install a Scientific Python distribution such as Anaconda and use its packag‐\n",
      "ing  system,  or  just  use  Python’s  own  packaging  system,  pip,  which  is  included  by\n",
      "default with the Python binary installers (since Python 2.7.9).6 You can check to see if\n",
      "pip is installed by typing the following command:\n",
      "\n",
      "$ python3 -m pip --version\n",
      "pip 19.0.2 from [...]/lib/python3.6/site-packages (python 3.6)\n",
      "\n",
      "You should make sure you have a recent version of pip installed. To upgrade the pip\n",
      "module, type:7\n",
      "\n",
      "$ python3 -m pip install --user -U pip\n",
      "Collecting pip\n",
      "[...]\n",
      "Successfully installed pip-19.0.2\n",
      "\n",
      "Creating an Isolated Environment\n",
      "If  you  would  like  to  work  in  an  isolated  environment  (which  is  strongly  recom‐\n",
      "mended so you can work on different projects without having conflicting library ver‐\n",
      "sions), install virtualenv8 by running the following pip command (again, if you want\n",
      "virtualenv to be installed for all users on your machine, remove --user and run this\n",
      "command with administrator rights):\n",
      "\n",
      "$ python3 -m pip install --user -U virtualenv\n",
      "Collecting virtualenv\n",
      "[...]\n",
      "Successfully installed virtualenv\n",
      "\n",
      "Now you can create an isolated Python environment by typing:\n",
      "\n",
      "$ cd $ML_PATH\n",
      "$ virtualenv env\n",
      "Using base prefix '[...]'\n",
      "New python executable in [...]/ml/env/bin/python3.6\n",
      "Also creating executable in [...]/ml/env/bin/python\n",
      "Installing setuptools, pip, wheel...done.\n",
      "\n",
      "6 We will show the installation steps using pip in a bash shell on a Linux or MacOS system. You may need to\n",
      "adapt these commands to your own system. On Windows, we recommend installing Anaconda instead.\n",
      "\n",
      "7 If you want to upgrade pip for all users on your machine rather than just your own user, you should remove\n",
      "the --user option and make sure you have administrator rights (e.g., by adding sudo before the whole com‐\n",
      "mand on Linux or MacOSX).\n",
      "\n",
      "8 Alternative tools include venv (very similar to virtualenv and included in the standard library), virtualenv‐\n",
      "wrapper (provides extra functionalities on top of virtualenv), pyenv (allows easy switching between Python\n",
      "versions), and pipenv (a great packaging tool by the same author as the popular requests library, built on top\n",
      "of pip, virtualenv and more).\n",
      "\n",
      "46 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "\fNow every time you want to activate this environment, just open a terminal and type:\n",
      "\n",
      "$ cd $ML_PATH\n",
      "$ source env/bin/activate # on Linux or MacOSX\n",
      "$ .\\env\\Scripts\\activate  # on Windows\n",
      "\n",
      "To  deactivate  this  environment,  just  type  deactivate.  While  the  environment  is\n",
      "active, any package you install using pip will be installed in this isolated environment,\n",
      "and Python will only have access to these packages (if you also want access to the sys‐\n",
      "tem’s packages, you should create the environment using virtualenv’s --system-site-\n",
      "packages option). Check out virtualenv’s documentation for more information.\n",
      "\n",
      "Now you can install all the required modules and their dependencies using this sim‐\n",
      "ple pip command (if you are not using a virtualenv, you will need the --user option\n",
      "or administrator rights):\n",
      "\n",
      "$ python3 -m pip install -U jupyter matplotlib numpy pandas scipy scikit-learn\n",
      "Collecting jupyter\n",
      "  Downloading jupyter-1.0.0-py2.py3-none-any.whl\n",
      "Collecting matplotlib\n",
      "  [...]\n",
      "\n",
      "To check your installation, try to import every module like this:\n",
      "\n",
      "$ python3 -c \"import jupyter, matplotlib, numpy, pandas, scipy, sklearn\"\n",
      "\n",
      "There should be no output and no error. Now you can fire up Jupyter by typing:\n",
      "\n",
      "$ jupyter notebook\n",
      "[I 15:24 NotebookApp] Serving notebooks from local directory: [...]/ml\n",
      "[I 15:24 NotebookApp] 0 active kernels\n",
      "[I 15:24 NotebookApp] The Jupyter Notebook is running at: http://localhost:8888/\n",
      "[I 15:24 NotebookApp] Use Control-C to stop this server and shut down all\n",
      "kernels (twice to skip confirmation).\n",
      "\n",
      "A Jupyter server is now running in your terminal, listening to port 8888. You can visit\n",
      "this  server  by  opening  your  web  browser  to  http://localhost:8888/  (this  usually  hap‐\n",
      "pens  automatically  when  the  server  starts).  You  should  see  your  empty  workspace\n",
      "directory (containing only the env directory if you followed the preceding virtualenv\n",
      "instructions).\n",
      "\n",
      "Now create a new Python notebook by clicking on the New button and selecting the\n",
      "appropriate Python version9 (see Figure 2-3).\n",
      "\n",
      "This  does  three  things:  first,  it  creates  a  new  notebook  file  called  Untitled.ipynb  in\n",
      "your workspace; second, it starts a Jupyter Python kernel to run this notebook; and\n",
      "\n",
      "9 Note that Jupyter can handle multiple versions of Python, and even many other languages such as R or\n",
      "\n",
      "Octave.\n",
      "\n",
      "Get the Data \n",
      "\n",
      "| \n",
      "\n",
      "47\n",
      "\n",
      "\fthird,  it  opens  this  notebook  in  a  new  tab.  You  should  start  by  renaming  this  note‐\n",
      "book to “Housing” (this will automatically rename the file to Housing.ipynb) by click‐\n",
      "ing Untitled and typing the new name.\n",
      "\n",
      "Figure 2-3. Your workspace in Jupyter\n",
      "\n",
      "A notebook contains a list of cells. Each cell can contain executable code or formatted\n",
      "text. Right now the notebook contains only one empty code cell, labeled “In [1]:”. Try\n",
      "typing  print(\"Hello  world!\")  in  the  cell,  and  click  on  the  play  button  (see\n",
      "Figure 2-4) or press Shift-Enter. This sends the current cell to this notebook’s Python\n",
      "kernel,  which  runs  it  and  returns  the  output.  The  result  is  displayed  below  the  cell,\n",
      "and since we reached the end of the notebook, a new cell is automatically created. Go\n",
      "through the User Interface Tour from Jupyter’s Help menu to learn the basics.\n",
      "\n",
      "Figure 2-4. Hello world Python notebook\n",
      "\n",
      "48 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "\fDownload the Data\n",
      "In  typical  environments  your  data  would  be  available  in  a  relational  database  (or\n",
      "some other common datastore) and spread across multiple tables/documents/files. To\n",
      "access it, you would first need to get your credentials and access authorizations,10 and\n",
      "familiarize yourself with the data schema. In this project, however, things are much\n",
      "simpler: you will just download a single compressed file, housing.tgz, which contains a\n",
      "comma-separated value (CSV) file called housing.csv with all the data.\n",
      "\n",
      "You could use your web browser to download it, and run tar xzf housing.tgz to\n",
      "decompress the file and extract the CSV file, but it is preferable to create a small func‐\n",
      "tion to do that. It is useful in particular if data changes regularly, as it allows you to\n",
      "write a small script that you can run whenever you need to fetch the latest data (or\n",
      "you  can  set  up  a  scheduled  job  to  do  that  automatically  at  regular  intervals).  Auto‐\n",
      "mating the process of fetching the data is also useful if you need to install the dataset\n",
      "on multiple machines.\n",
      "\n",
      "Here is the function to fetch the data:11\n",
      "\n",
      "import os\n",
      "import tarfile\n",
      "from six.moves import urllib\n",
      "\n",
      "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
      "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
      "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
      "\n",
      "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
      "    if not os.path.isdir(housing_path):\n",
      "        os.makedirs(housing_path)\n",
      "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
      "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
      "    housing_tgz = tarfile.open(tgz_path)\n",
      "    housing_tgz.extractall(path=housing_path)\n",
      "    housing_tgz.close()\n",
      "\n",
      "Now when you call fetch_housing_data(), it creates a datasets/housing directory in\n",
      "your workspace, downloads the housing.tgz file, and extracts the housing.csv from it in\n",
      "this directory.\n",
      "\n",
      "Now let’s load the data using Pandas. Once again you should write a small function to\n",
      "load the data:\n",
      "\n",
      "10 You might also need to check legal constraints, such as private fields that should never be copied to unsafe\n",
      "\n",
      "datastores.\n",
      "\n",
      "11 In a real project you would save this code in a Python file, but for now you can just write it in your Jupyter\n",
      "\n",
      "notebook.\n",
      "\n",
      "Get the Data \n",
      "\n",
      "| \n",
      "\n",
      "49\n",
      "\n",
      "\fimport pandas as pd\n",
      "\n",
      "def load_housing_data(housing_path=HOUSING_PATH):\n",
      "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
      "    return pd.read_csv(csv_path)\n",
      "\n",
      "This function returns a Pandas DataFrame object containing all the data.\n",
      "\n",
      "Take a Quick Look at the Data Structure\n",
      "Let’s  take  a  look  at  the  top  five  rows  using  the  DataFrame’s  head()  method  (see\n",
      "Figure 2-5).\n",
      "\n",
      "Figure 2-5. Top five rows in the dataset\n",
      "\n",
      "Each row represents one district. There are 10 attributes (you can see the first 6 in the\n",
      "screenshot):  longitude,  latitude,  housing_median_age,  total_rooms,  total_bed\n",
      "rooms,  population,  households,  median_income,  median_house_value,  and\n",
      "ocean_proximity.\n",
      "\n",
      "The info() method is useful to get a quick description of the data, in particular the\n",
      "total  number  of  rows,  and  each  attribute’s  type  and  number  of  non-null  values  (see\n",
      "Figure 2-6).\n",
      "\n",
      "50 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "\fFigure 2-6. Housing info\n",
      "\n",
      "There  are  20,640  instances  in  the  dataset,  which  means  that  it  is  fairly  small  by\n",
      "Machine Learning standards, but it’s perfect to get started. Notice that the total_bed\n",
      "rooms attribute has only 20,433 non-null values, meaning that 207 districts are miss‐\n",
      "ing this feature. We will need to take care of this later.\n",
      "\n",
      "All attributes are numerical, except the ocean_proximity field. Its type is object, so it\n",
      "could hold any kind of Python object, but since you loaded this data from a CSV file\n",
      "you know that it must be a text attribute. When you looked at the top five rows, you\n",
      "probably  noticed  that  the  values  in  the  ocean_proximity  column  were  repetitive,\n",
      "which  means  that  it  is  probably  a  categorical  attribute.  You  can  find  out  what  cate‐\n",
      "gories  exist  and  how  many  districts  belong  to  each  category  by  using  the\n",
      "value_counts() method:\n",
      "\n",
      ">>> housing[\"ocean_proximity\"].value_counts()\n",
      "<1H OCEAN     9136\n",
      "INLAND        6551\n",
      "NEAR OCEAN    2658\n",
      "NEAR BAY      2290\n",
      "ISLAND           5\n",
      "Name: ocean_proximity, dtype: int64\n",
      "\n",
      "Let’s  look  at  the  other  fields.  The  describe()  method  shows  a  summary  of  the\n",
      "numerical attributes (Figure 2-7).\n",
      "\n",
      "Get the Data \n",
      "\n",
      "| \n",
      "\n",
      "51\n",
      "\n",
      "\fFigure 2-7. Summary of each numerical attribute\n",
      "\n",
      "The count, mean, min, and max rows are self-explanatory. Note that the null values are\n",
      "ignored  (so,  for  example,  count  of  total_bedrooms  is  20,433,  not  20,640).  The  std\n",
      "row  shows  the  standard  deviation,  which  measures  how  dispersed  the  values  are.12\n",
      "The 25%, 50%, and 75% rows show the corresponding percentiles: a percentile indi‐\n",
      "cates the value below which a given percentage of observations in a group of observa‐\n",
      "tions falls. For example, 25% of the districts have a housing_median_age lower than\n",
      "18, while 50% are lower than 29 and 75% are lower than 37. These are often called the\n",
      "25th percentile (or 1st quartile), the median, and the 75th percentile (or 3rd quartile).\n",
      "\n",
      "Another quick way to get a feel of the type of data you are dealing with is to plot a \n",
      "histogram for each numerical attribute. A histogram shows the number of instances\n",
      "(on the vertical axis) that have a given value range (on the horizontal axis). You can\n",
      "either  plot  this  one  attribute  at  a  time,  or  you  can  call  the  hist()  method  on  the\n",
      "whole  dataset,  and  it  will  plot  a  histogram  for  each  numerical  attribute  (see\n",
      "Figure  2-8).  For  example,  you  can  see  that  slightly  over  800  districts  have  a\n",
      "median_house_value equal to about $100,000.\n",
      "\n",
      "%matplotlib inline   # only in a Jupyter notebook\n",
      "import matplotlib.pyplot as plt\n",
      "housing.hist(bins=50, figsize=(20,15))\n",
      "plt.show()\n",
      "\n",
      "12 The standard deviation is generally denoted σ (the Greek letter sigma), and it is the square root of the var‐\n",
      "\n",
      "iance, which is the average of the squared deviation from the mean. When a feature has a bell-shaped normal\n",
      "distribution (also called a Gaussian distribution), which is very common, the “68-95-99.7” rule applies: about\n",
      "68% of the values fall within 1σ of the mean, 95% within 2σ, and 99.7% within 3σ.\n",
      "\n",
      "52 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "\fThe hist() method relies on Matplotlib, which in turn relies on a\n",
      "user-specified graphical backend to draw on your screen. So before\n",
      "you can plot anything, you need to specify which backend Matplot‐\n",
      "lib should use. The simplest option is to use Jupyter’s magic com‐\n",
      "mand %matplotlib inline. This tells Jupyter to set up Matplotlib\n",
      "so it uses Jupyter’s own backend. Plots are then rendered within the\n",
      "notebook  itself.  Note  that  calling  show()  is  optional  in  a  Jupyter\n",
      "notebook, as Jupyter will automatically display plots when a cell is\n",
      "executed.\n",
      "\n",
      "Figure 2-8. A histogram for each numerical attribute\n",
      "\n",
      "Notice a few things in these histograms:\n",
      "\n",
      "1. First, the median income attribute does not look like it is expressed in US dollars\n",
      "(USD). After checking with the team that collected the data, you are told that the\n",
      "data  has  been  scaled  and  capped  at  15  (actually  15.0001)  for  higher  median\n",
      "incomes,  and  at  0.5  (actually  0.4999)  for  lower  median  incomes.  The  numbers\n",
      "represent  roughly  tens  of  thousands  of  dollars  (e.g.,  3  actually  means  about\n",
      "$30,000). Working with preprocessed attributes is common in Machine Learning,\n",
      "\n",
      "Get the Data \n",
      "\n",
      "| \n",
      "\n",
      "53\n",
      "\n",
      "\fand  it  is  not  necessarily  a  problem,  but  you  should  try  to  understand  how  the\n",
      "data was computed.\n",
      "\n",
      "2. The housing median age and the median house value were also capped. The lat‐\n",
      "ter may be a serious problem since it is your target attribute (your labels). Your\n",
      "Machine Learning algorithms may learn that prices never go beyond that limit.\n",
      "You need to check with your client team (the team that will use your system’s out‐\n",
      "put) to see if this is a problem or not. If they tell you that they need precise pre‐\n",
      "dictions even beyond $500,000, then you have mainly two options:\n",
      "\n",
      "a. Collect proper labels for the districts whose labels were capped.\n",
      "\n",
      "b. Remove those districts from the training set (and also from the test set, since\n",
      "your  system  should  not  be  evaluated  poorly  if  it  predicts  values  beyond\n",
      "$500,000).\n",
      "\n",
      "3. These attributes have very different scales. We will discuss this later in this chap‐\n",
      "\n",
      "ter when we explore feature scaling.\n",
      "\n",
      "4. Finally, many histograms are tail heavy: they extend much farther to the right of\n",
      "the  median  than  to  the  left.  This  may  make  it  a  bit  harder  for  some  Machine\n",
      "Learning algorithms to detect patterns. We will try transforming these attributes\n",
      "later on to have more bell-shaped distributions.\n",
      "\n",
      "Hopefully you now have a better understanding of the kind of data you are dealing\n",
      "with.\n",
      "\n",
      "Wait! Before you look at the data any further, you need to create a\n",
      "test set, put it aside, and never look at it.\n",
      "\n",
      "Create a Test Set\n",
      "It may sound strange to voluntarily set aside part of the data at this stage. After all,\n",
      "you have only taken a quick glance at the data, and surely you should learn a whole\n",
      "lot  more  about  it  before  you  decide  what  algorithms  to  use,  right?  This  is  true,  but\n",
      "your  brain  is  an  amazing  pattern  detection  system,  which  means  that  it  is  highly\n",
      "prone to overfitting: if you look at the test set, you may stumble upon some seemingly\n",
      "interesting  pattern  in  the  test  data  that  leads  you  to  select  a  particular  kind  of\n",
      "Machine Learning model. When you estimate the generalization error using the test\n",
      "set,  your  estimate  will  be  too  optimistic  and  you  will  launch  a  system  that  will  not\n",
      "perform as well as expected. This is called data snooping bias.\n",
      "\n",
      "Creating  a  test  set  is  theoretically  quite  simple:  just  pick  some  instances  randomly,\n",
      "typically 20% of the dataset (or less if your dataset is very large), and set them aside:\n",
      "\n",
      "54 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "\fimport numpy as np\n",
      "\n",
      "def split_train_test(data, test_ratio):\n",
      "    shuffled_indices = np.random.permutation(len(data))\n",
      "    test_set_size = int(len(data) * test_ratio)\n",
      "    test_indices = shuffled_indices[:test_set_size]\n",
      "    train_indices = shuffled_indices[test_set_size:]\n",
      "    return data.iloc[train_indices], data.iloc[test_indices]\n",
      "\n",
      "You can then use this function like this:13\n",
      "\n",
      ">>> train_set, test_set = split_train_test(housing, 0.2)\n",
      ">>> len(train_set)\n",
      "16512\n",
      ">>> len(test_set)\n",
      "4128\n",
      "\n",
      "Well, this works, but it is not perfect: if you run the program again, it will generate a\n",
      "different test set! Over time, you (or your Machine Learning algorithms) will get to\n",
      "see the whole dataset, which is what you want to avoid.\n",
      "\n",
      "One  solution  is  to  save  the  test  set  on  the  first  run  and  then  load  it  in  subsequent\n",
      "runs.  Another  option  is  to  set  the  random  number  generator’s  seed  (e.g.,  np.ran\n",
      "dom.seed(42))14 before calling np.random.permutation(), so that it always generates\n",
      "the same shuffled indices.\n",
      "\n",
      "But both these solutions will break next time you fetch an updated dataset. A com‐\n",
      "mon solution is to use each instance’s identifier to decide whether or not it should go\n",
      "in  the  test  set  (assuming  instances  have  a  unique  and  immutable  identifier).  For\n",
      "example, you could compute a hash of each instance’s identifier and put that instance\n",
      "in the test set if the hash is lower or equal to 20% of the maximum hash value. This\n",
      "ensures  that  the  test  set  will  remain  consistent  across  multiple  runs,  even  if  you\n",
      "refresh the dataset. The new test set will contain 20% of the new instances, but it will\n",
      "not  contain  any  instance  that  was  previously  in  the  training  set.  Here  is  a  possible\n",
      "implementation:\n",
      "\n",
      "from zlib import crc32\n",
      "\n",
      "def test_set_check(identifier, test_ratio):\n",
      "    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n",
      "\n",
      "def split_train_test_by_id(data, test_ratio, id_column):\n",
      "    ids = data[id_column]\n",
      "\n",
      "13 In this book, when a code example contains a mix of code and outputs, as is the case here, it is formatted like\n",
      "in the Python interpreter, for better readability: the code lines are prefixed with >>> (or ... for indented\n",
      "blocks), and the outputs have no prefix.\n",
      "\n",
      "14 You will often see people set the random seed to 42. This number has no special property, other than to be\n",
      "\n",
      "The Answer to the Ultimate Question of Life, the Universe, and Everything.\n",
      "\n",
      "Get the Data \n",
      "\n",
      "| \n",
      "\n",
      "55\n",
      "\n",
      "\f    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n",
      "    return data.loc[~in_test_set], data.loc[in_test_set]\n",
      "\n",
      "Unfortunately, the housing dataset does not have an identifier column. The simplest\n",
      "solution is to use the row index as the ID:\n",
      "\n",
      "housing_with_id = housing.reset_index()   # adds an `index` column\n",
      "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")\n",
      "\n",
      "If you use the row index as a unique identifier, you need to make sure that new data\n",
      "gets appended to the end of the dataset, and no row ever gets deleted. If this is not\n",
      "possible, then you can try to use the most stable features to build a unique identifier.\n",
      "For example, a district’s latitude and longitude are guaranteed to be stable for a few\n",
      "million years, so you could combine them into an ID like so:15\n",
      "\n",
      "housing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]\n",
      "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"id\")\n",
      "\n",
      "Scikit-Learn provides a few functions to split datasets into multiple subsets in various\n",
      "ways. The simplest function is train_test_split, which does pretty much the same\n",
      "thing as the function  split_train_test defined earlier, with a couple of additional\n",
      "features. First there is a random_state parameter that allows you to set the random\n",
      "generator seed as explained previously, and second you can pass it multiple datasets\n",
      "with an identical number of rows, and it will split them on the same indices (this is\n",
      "very useful, for example, if you have a separate DataFrame for labels):\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
      "\n",
      "So far we have considered purely random sampling methods. This is generally fine if\n",
      "your dataset is large enough (especially relative to the number of attributes), but if it\n",
      "is  not,  you  run  the  risk  of  introducing  a  significant  sampling  bias.  When  a  survey\n",
      "company decides to call 1,000 people to ask them a few questions, they don’t just pick\n",
      "1,000 people randomly in a phone book. They try to ensure that these 1,000 people\n",
      "are representative of the whole population. For example, the US population is com‐\n",
      "posed of 51.3% female and 48.7% male, so a well-conducted survey in the US would\n",
      "try to maintain this ratio in the sample: 513 female and 487 male. This is called strati‐\n",
      "fied  sampling:  the  population  is  divided  into  homogeneous  subgroups  called  strata,\n",
      "and the right number of instances is sampled from each stratum to guarantee that the\n",
      "test set is representative of the overall population. If they used purely random sam‐\n",
      "pling, there would be about 12% chance of sampling a skewed test set with either less\n",
      "than 49% female or more than 54% female. Either way, the survey results would be\n",
      "significantly biased.\n",
      "\n",
      "15 The location information is actually quite coarse, and as a result many districts will have the exact same ID, so\n",
      "\n",
      "they will end up in the same set (test or train). This introduces some unfortunate sampling bias.\n",
      "\n",
      "56 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "\fSuppose  you  chatted  with  experts  who  told  you  that  the  median  income  is  a  very\n",
      "important  attribute  to  predict  median  housing  prices.  You  may  want  to  ensure  that\n",
      "the test set is representative of the various categories of incomes in the whole dataset.\n",
      "Since the median income is a continuous numerical attribute, you first need to create\n",
      "an income category attribute. Let’s look at the median income histogram more closely\n",
      "(back in Figure 2-8): most median income values are clustered around 1.5 to 6 (i.e.,\n",
      "$15,000–$60,000), but some median incomes go far beyond 6. It is important to have\n",
      "a sufficient number of instances in your dataset for each stratum, or else the estimate\n",
      "of the stratum’s importance may be biased. This means that you should not have too\n",
      "many strata, and each stratum should be large enough. The following code uses the\n",
      "pd.cut()  function  to  create  an  income  category  attribute  with  5  categories  (labeled\n",
      "from 1 to 5): category 1 ranges from 0 to 1.5 (i.e., less than $15,000), category 2 from\n",
      "1.5 to 3, and so on:\n",
      "\n",
      "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
      "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
      "                               labels=[1, 2, 3, 4, 5])\n",
      "\n",
      "These income categories are represented in Figure 2-9:\n",
      "\n",
      "housing[\"income_cat\"].hist()\n",
      "\n",
      "Figure 2-9. Histogram of income categories\n",
      "\n",
      "Now you are ready to do stratified sampling based on the income category. For this\n",
      "you can use Scikit-Learn’s StratifiedShuffleSplit class:\n",
      "\n",
      "from sklearn.model_selection import StratifiedShuffleSplit\n",
      "\n",
      "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
      "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
      "    strat_train_set = housing.loc[train_index]\n",
      "    strat_test_set = housing.loc[test_index]\n",
      "\n",
      "Get the Data \n",
      "\n",
      "| \n",
      "\n",
      "57\n",
      "\n",
      "\fLet’s see if this worked as expected. You can start by looking at the income category\n",
      "proportions in the test set:\n",
      "\n",
      ">>> strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)\n",
      "3    0.350533\n",
      "2    0.318798\n",
      "4    0.176357\n",
      "5    0.114583\n",
      "1    0.039729\n",
      "Name: income_cat, dtype: float64\n",
      "\n",
      "With similar code you can measure the income category proportions in the full data‐\n",
      "set. Figure 2-10 compares the income category proportions in the overall dataset, in\n",
      "the test set generated with stratified sampling, and in a test set generated using purely\n",
      "random sampling. As you can see, the test set generated using stratified sampling has\n",
      "income category proportions almost identical to those in the full dataset, whereas the\n",
      "test set generated using purely random sampling is quite skewed.\n",
      "\n",
      "Figure 2-10. Sampling bias comparison of stratified versus purely random sampling\n",
      "\n",
      "Now you should remove the income_cat attribute so the data is back to its original\n",
      "state:\n",
      "\n",
      "for set_ in (strat_train_set, strat_test_set):\n",
      "    set_.drop(\"income_cat\", axis=1, inplace=True)\n",
      "\n",
      "We spent quite a bit of time on test set generation for a good reason: this is an often\n",
      "neglected  but  critical  part  of  a  Machine  Learning  project.  Moreover,  many  of  these\n",
      "ideas will be useful later when we discuss cross-validation. Now it’s time to move on\n",
      "to the next stage: exploring the data.\n",
      "\n",
      "Discover and Visualize the Data to Gain Insights\n",
      "So far you have only taken a quick glance at the data to get a general understanding of\n",
      "the kind of data you are manipulating. Now the goal is to go a little bit more in depth.\n",
      "\n",
      "First, make sure you have put the test set aside and you are only exploring the train‐\n",
      "ing set. Also, if the training set is very large, you may want to sample an exploration\n",
      "\n",
      "58 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "\fset, to make manipulations easy and fast. In our case, the set is quite small so you can\n",
      "just work directly on the full set. Let’s create a copy so you can play with it without\n",
      "harming the training set:\n",
      "\n",
      "housing = strat_train_set.copy()\n",
      "\n",
      "Visualizing Geographical Data\n",
      "Since there is geographical information (latitude and longitude), it is a good idea to\n",
      "create a scatterplot of all districts to visualize the data (Figure 2-11):\n",
      "\n",
      "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")\n",
      "\n",
      "Figure 2-11. A geographical scatterplot of the data\n",
      "\n",
      "This looks like California all right, but other than that it is hard to see any particular\n",
      "pattern. Setting the alpha option to 0.1 makes it much easier to visualize the places\n",
      "where there is a high density of data points (Figure 2-12):\n",
      "\n",
      "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)\n",
      "\n",
      "Discover and Visualize the Data to Gain Insights \n",
      "\n",
      "| \n",
      "\n",
      "59\n",
      "\n",
      "\fFigure 2-12. A better visualization highlighting high-density areas\n",
      "\n",
      "Now  that’s  much  better:  you  can  clearly  see  the  high-density  areas,  namely  the  Bay\n",
      "Area and around Los Angeles and San Diego, plus a long line of fairly high density in\n",
      "the Central Valley, in particular around Sacramento and Fresno.\n",
      "\n",
      "More  generally,  our  brains  are  very  good  at  spotting  patterns  on  pictures,  but  you\n",
      "may  need  to  play  around  with  visualization  parameters  to  make  the  patterns  stand\n",
      "out.\n",
      "\n",
      "Now let’s look at the housing prices (Figure 2-13). The radius of each circle represents\n",
      "the district’s population (option s), and the color represents the price (option c). We\n",
      "will  use  a  predefined  color  map  (option  cmap)  called  jet,  which  ranges  from  blue\n",
      "(low values) to red (high prices):16\n",
      "\n",
      "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n",
      "    s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n",
      "    c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
      ")\n",
      "plt.legend()\n",
      "\n",
      "16 If you are reading this in grayscale, grab a red pen and scribble over most of the coastline from the Bay Area\n",
      "\n",
      "down to San Diego (as you might expect). You can add a patch of yellow around Sacramento as well.\n",
      "\n",
      "60 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "\fFigure 2-13. California housing prices\n",
      "\n",
      "Discover and Visualize the Data to Gain Insights \n",
      "\n",
      "| \n",
      "\n",
      "61\n",
      "\n",
      "\fThis  image  tells  you  that  the  housing  prices  are  very  much  related  to  the  location\n",
      "(e.g., close to the ocean) and to the population density, as you probably knew already.\n",
      "It will probably be useful to use a clustering algorithm to detect the main clusters, and\n",
      "add new features that measure the proximity to the cluster centers. The ocean prox‐\n",
      "imity  attribute  may  be  useful  as  well,  although  in  Northern  California  the  housing\n",
      "prices in coastal districts are not too high, so it is not a simple rule.\n",
      "\n",
      "Looking for Correlations\n",
      "Since  the  dataset  is  not  too  large,  you  can  easily  compute  the  standard  correlation\n",
      "coefficient  (also  called  Pearson’s  r)  between  every  pair  of  attributes  using  the  corr()\n",
      "method:\n",
      "\n",
      "corr_matrix = housing.corr()\n",
      "\n",
      "Now let’s look at how much each attribute correlates with the median house value:\n",
      "\n",
      ">>> corr_matrix[\"median_house_value\"].sort_values(ascending=False)\n",
      "median_house_value    1.000000\n",
      "median_income         0.687170\n",
      "total_rooms           0.135231\n",
      "housing_median_age    0.114220\n",
      "households            0.064702\n",
      "total_bedrooms        0.047865\n",
      "population           -0.026699\n",
      "longitude            -0.047279\n",
      "latitude             -0.142826\n",
      "Name: median_house_value, dtype: float64\n",
      "\n",
      "The correlation coefficient ranges from –1 to 1. When it is close to 1, it means that\n",
      "there is a strong positive correlation; for example, the median house value tends to go\n",
      "up  when  the  median  income  goes  up.  When  the  coefficient  is  close  to  –1,  it  means\n",
      "that  there  is  a  strong  negative  correlation;  you  can  see  a  small  negative  correlation\n",
      "between the latitude and the median house value (i.e., prices have a slight tendency to\n",
      "go down when you go north). Finally, coefficients close to zero mean that there is no\n",
      "linear correlation. Figure 2-14 shows various plots along with the correlation coeffi‐\n",
      "cient between their horizontal and vertical axes.\n",
      "\n",
      "62 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "\fFigure 2-14. Standard correlation coefficient of various datasets (source: Wikipedia;\n",
      "public domain image)\n",
      "\n",
      "The correlation coefficient only measures linear correlations (“if x\n",
      "goes up, then y generally goes up/down”). It may completely miss\n",
      "out on nonlinear relationships (e.g., “if x is close to zero then y gen‐\n",
      "erally  goes  up”).  Note  how  all  the  plots  of  the  bottom  row  have  a\n",
      "correlation coefficient equal to zero despite the fact that their axes\n",
      "are  clearly  not  independent:  these  are  examples  of  nonlinear  rela‐\n",
      "tionships. Also, the second row shows examples where the correla‐\n",
      "tion coefficient is equal to 1 or –1; notice that this has nothing to\n",
      "do with the slope. For example, your height in inches has a correla‐\n",
      "tion coefficient of 1 with your height in feet or in nanometers.\n",
      "\n",
      "Another  way  to  check  for  correlation  between  attributes  is  to  use  Pandas’\n",
      "scatter_matrix function, which plots every numerical attribute against every other\n",
      "numerical attribute. Since there are now 11 numerical attributes, you would get 112 =\n",
      "121  plots,  which  would  not  fit  on  a  page,  so  let’s  just  focus  on  a  few  promising\n",
      "attributes that seem most correlated with the median housing value (Figure 2-15):\n",
      "\n",
      "from pandas.plotting import scatter_matrix\n",
      "\n",
      "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
      "              \"housing_median_age\"]\n",
      "scatter_matrix(housing[attributes], figsize=(12, 8))\n",
      "\n",
      "Discover and Visualize the Data to Gain Insights \n",
      "\n",
      "| \n",
      "\n",
      "63\n",
      "\n",
      "\fFigure 2-15. Scatter matrix\n",
      "\n",
      "The main diagonal (top left to bottom right) would be full of straight lines if Pandas\n",
      "plotted each variable against itself, which would not be very useful. So instead Pandas\n",
      "displays a histogram of each attribute (other options are available; see Pandas’ docu‐\n",
      "mentation for more details).\n",
      "\n",
      "The  most  promising  attribute  to  predict  the  median  house  value  is  the  median\n",
      "income, so let’s zoom in on their correlation scatterplot (Figure 2-16):\n",
      "\n",
      "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n",
      "             alpha=0.1)\n",
      "\n",
      "This  plot  reveals  a  few  things.  First,  the  correlation  is  indeed  very  strong;  you  can\n",
      "clearly see the upward trend and the points are not too dispersed. Second, the price\n",
      "cap that we noticed earlier is clearly visible as a horizontal line at $500,000. But this\n",
      "plot  reveals  other  less  obvious  straight  lines:  a  horizontal  line  around  $450,000,\n",
      "another around $350,000, perhaps one around $280,000, and a few more below that.\n",
      "You may want to try removing the corresponding districts to prevent your algorithms\n",
      "from learning to reproduce these data quirks.\n",
      "\n",
      "64 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "\fFigure 2-16. Median income versus median house value\n",
      "\n",
      "Experimenting with Attribute Combinations\n",
      "Hopefully the previous sections gave you an idea of a few ways you can explore the\n",
      "data and gain insights. You identified a few data quirks that you may want to clean up\n",
      "before feeding the data to a Machine Learning algorithm, and you found interesting\n",
      "correlations  between  attributes,  in  particular  with  the  target  attribute.  You  also\n",
      "noticed that some attributes have a tail-heavy distribution, so you may want to trans‐\n",
      "form  them  (e.g.,  by  computing  their  logarithm).  Of  course,  your  mileage  will  vary\n",
      "considerably with each project, but the general ideas are similar.\n",
      "\n",
      "One  last  thing  you  may  want  to  do  before  actually  preparing  the  data  for  Machine\n",
      "Learning  algorithms  is  to  try  out  various  attribute  combinations.  For  example,  the\n",
      "total  number  of  rooms  in  a  district  is  not  very  useful  if  you  don’t  know  how  many\n",
      "households there are. What you really want is the number of rooms per household.\n",
      "Similarly,  the  total  number  of  bedrooms  by  itself  is  not  very  useful:  you  probably\n",
      "want to compare it to the number of rooms. And the population per household also\n",
      "seems  like  an  interesting  attribute  combination  to  look  at.  Let’s  create  these  new\n",
      "attributes:\n",
      "\n",
      "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
      "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
      "housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]\n",
      "\n",
      "And now let’s look at the correlation matrix again:\n",
      "\n",
      ">>> corr_matrix = housing.corr()\n",
      ">>> corr_matrix[\"median_house_value\"].sort_values(ascending=False)\n",
      "median_house_value          1.000000\n",
      "\n",
      "Discover and Visualize the Data to Gain Insights \n",
      "\n",
      "| \n",
      "\n",
      "65\n",
      "\n",
      "\fmedian_income               0.687160\n",
      "rooms_per_household         0.146285\n",
      "total_rooms                 0.135097\n",
      "housing_median_age          0.114110\n",
      "households                  0.064506\n",
      "total_bedrooms              0.047689\n",
      "population_per_household   -0.021985\n",
      "population                 -0.026920\n",
      "longitude                  -0.047432\n",
      "latitude                   -0.142724\n",
      "bedrooms_per_room          -0.259984\n",
      "Name: median_house_value, dtype: float64\n",
      "\n",
      "Hey,  not  bad!  The  new  bedrooms_per_room  attribute  is  much  more  correlated  with\n",
      "the  median  house  value  than  the  total  number  of  rooms  or  bedrooms.  Apparently\n",
      "houses with a lower bedroom/room ratio tend to be more expensive. The number of\n",
      "rooms per household is also more informative than the total number of rooms in a\n",
      "district—obviously the larger the houses, the more expensive they are.\n",
      "\n",
      "This  round  of  exploration  does  not  have  to  be  absolutely  thorough;  the  point  is  to\n",
      "start off on the right foot and quickly gain insights that will help you get a first rea‐\n",
      "sonably good prototype. But this is an iterative process: once you get a prototype up\n",
      "and running, you can analyze its output to gain more insights and come back to this\n",
      "exploration step.\n",
      "\n",
      "Prepare the Data for Machine Learning Algorithms\n",
      "It’s  time  to  prepare  the  data  for  your  Machine  Learning  algorithms.  Instead  of  just\n",
      "doing this manually, you should write functions to do that, for several good reasons:\n",
      "\n",
      "• This will allow you to reproduce these transformations easily on any dataset (e.g.,\n",
      "\n",
      "the next time you get a fresh dataset).\n",
      "\n",
      "• You will gradually build a library of transformation functions that you can reuse\n",
      "\n",
      "in future projects.\n",
      "\n",
      "• You can use these functions in your live system to transform the new data before\n",
      "\n",
      "feeding it to your algorithms.\n",
      "\n",
      "• This  will  make  it  possible  for  you  to  easily  try  various  transformations  and  see\n",
      "\n",
      "which combination of transformations works best.\n",
      "\n",
      "But first let’s revert to a clean training set (by copying strat_train_set once again),\n",
      "and let’s separate the predictors and the labels since we don’t necessarily want to apply\n",
      "the same transformations to the predictors and the target values (note that  drop() \n",
      "creates a copy of the data and does not affect strat_train_set):\n",
      "\n",
      "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
      "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
      "\n",
      "66 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "\fData Cleaning\n",
      "Most Machine Learning algorithms cannot work with missing features, so let’s create\n",
      "a  few  functions  to  take  care  of  them.  You  noticed  earlier  that  the  total_bedrooms\n",
      "attribute has some missing values, so let’s fix this. You have three options:\n",
      "\n",
      "• Get rid of the corresponding districts.\n",
      "\n",
      "• Get rid of the whole attribute.\n",
      "\n",
      "• Set the values to some value (zero, the mean, the median, etc.).\n",
      "\n",
      "You can accomplish these easily using DataFrame’s dropna(), drop(), and fillna()\n",
      "methods:\n",
      "\n",
      "housing.dropna(subset=[\"total_bedrooms\"])    # option 1\n",
      "housing.drop(\"total_bedrooms\", axis=1)       # option 2\n",
      "median = housing[\"total_bedrooms\"].median()  # option 3\n",
      "housing[\"total_bedrooms\"].fillna(median, inplace=True)\n",
      "\n",
      "If you choose option 3, you should compute the median value on the training set, and\n",
      "use  it  to  fill  the  missing  values  in  the  training  set,  but  also  don’t  forget  to  save  the\n",
      "median value that you have computed. You will need it later to replace missing values\n",
      "in the test set when you want to evaluate your system, and also once the system goes\n",
      "live to replace missing values in new data.\n",
      "\n",
      "Scikit-Learn  provides  a  handy  class  to  take  care  of  missing  values:  SimpleImputer.\n",
      "Here is how to use it. First, you need to create a SimpleImputer instance, specifying\n",
      "that  you  want  to  replace  each  attribute’s  missing  values  with  the  median  of  that\n",
      "attribute:\n",
      "\n",
      "from sklearn.impute import SimpleImputer\n",
      "\n",
      "imputer = SimpleImputer(strategy=\"median\")\n",
      "\n",
      "Since the median can only be computed on numerical attributes, we need to create a\n",
      "copy of the data without the text attribute ocean_proximity:\n",
      "\n",
      "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
      "\n",
      "Now you can fit the imputer instance to the training data using the fit() method:\n",
      "\n",
      "imputer.fit(housing_num)\n",
      "\n",
      "The imputer has simply computed the median of each attribute and stored the result\n",
      "in its statistics_ instance variable. Only the total_bedrooms attribute had missing\n",
      "values, but we cannot be sure that there won’t be any missing values in new data after\n",
      "the system goes live, so it is safer to apply the imputer to all the numerical attributes:\n",
      "\n",
      ">>> imputer.statistics_\n",
      "array([ -118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409])\n",
      "\n",
      "Prepare the Data for Machine Learning Algorithms \n",
      "\n",
      "| \n",
      "\n",
      "67\n",
      "\n",
      "\f>>> housing_num.median().values\n",
      "array([ -118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409])\n",
      "\n",
      "Now  you  can  use  this  “trained”  imputer  to  transform  the  training  set  by  replacing\n",
      "missing values by the learned medians:\n",
      "\n",
      "X = imputer.transform(housing_num)\n",
      "\n",
      "The result is a plain NumPy array containing the transformed features. If you want to\n",
      "put it back into a Pandas DataFrame, it’s simple:\n",
      "\n",
      "housing_tr = pd.DataFrame(X, columns=housing_num.columns)\n",
      "\n",
      "Scikit-Learn Design\n",
      "\n",
      "Scikit-Learn’s API is remarkably well designed. The main design principles are:17\n",
      "\n",
      "• Consistency. All objects share a consistent and simple interface:\n",
      "\n",
      "— Estimators. Any object that can estimate some parameters based on a dataset\n",
      "is called an estimator (e.g., an imputer is an estimator). The estimation itself is\n",
      "performed by the fit() method, and it takes only a dataset as a parameter (or\n",
      "two  for  supervised  learning  algorithms;  the  second  dataset  contains  the\n",
      "labels).  Any  other  parameter  needed  to  guide  the  estimation  process  is  con‐\n",
      "sidered a hyperparameter (such as an imputer’s strategy), and it must be set\n",
      "as an instance variable (generally via a constructor parameter).\n",
      "\n",
      "— Transformers.  Some  estimators  (such  as  an  imputer)  can  also  transform  a\n",
      "dataset; these are called transformers. Once again, the API is quite simple: the\n",
      "transformation is performed by the transform() method with the dataset to\n",
      "transform as a parameter. It returns the transformed dataset. This transforma‐\n",
      "tion generally relies on the learned parameters, as is the case for an imputer.\n",
      "All  transformers  also  have  a  convenience  method  called  fit_transform() \n",
      "that  is  equivalent  to  calling  fit()  and  then  transform()  (but  sometimes\n",
      "fit_transform() is optimized and runs much faster).\n",
      "\n",
      "— Predictors. Finally, some estimators are capable of making predictions given a\n",
      "dataset; they are called predictors. For example, the LinearRegression model \n",
      "in  the  previous  chapter  was  a  predictor:  it  predicted  life  satisfaction  given  a\n",
      "country’s  GDP  per  capita.  A  predictor  has  a  predict()  method  that  takes  a\n",
      "dataset of new instances and returns a dataset of corresponding predictions. It\n",
      "also has a score() method that measures the quality of the predictions given\n",
      "\n",
      "17 For more details on the design principles, see “API design for machine learning software: experiences from\n",
      "\n",
      "the scikit-learn project,” L. Buitinck, G. Louppe, M. Blondel, F. Pedregosa, A. Müller, et al. (2013).\n",
      "\n",
      "68 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "\fa  test  set  (and  the  corresponding  labels  in  the  case  of  supervised  learning\n",
      "algorithms).18\n",
      "\n",
      "• Inspection. All the estimator’s hyperparameters are accessible directly via public\n",
      "instance  variables  (e.g.,  imputer.strategy),  and  all  the  estimator’s  learned\n",
      "parameters  are  also  accessible  via  public  instance  variables  with  an  underscore\n",
      "suffix (e.g., imputer.statistics_).\n",
      "\n",
      "• Nonproliferation of classes. Datasets are represented as NumPy arrays or SciPy\n",
      "sparse matrices, instead of homemade classes. Hyperparameters are just regular\n",
      "Python strings or numbers.\n",
      "\n",
      "• Composition.  Existing  building  blocks  are  reused  as  much  as  possible.  For\n",
      "example, it is easy to create a Pipeline estimator from an arbitrary sequence of\n",
      "transformers followed by a final estimator, as we will see.\n",
      "\n",
      "• Sensible  defaults.  Scikit-Learn  provides  reasonable  default  values  for  most\n",
      "\n",
      "parameters, making it easy to create a baseline working system quickly.\n",
      "\n",
      "Handling Text and Categorical Attributes\n",
      "Earlier  we  left  out  the  categorical  attribute  ocean_proximity  because  it  is  a  text\n",
      "attribute so we cannot compute its median:\n",
      "\n",
      ">>> housing_cat = housing[[\"ocean_proximity\"]]\n",
      ">>> housing_cat.head(10)\n",
      "      ocean_proximity\n",
      "17606       <1H OCEAN\n",
      "18632       <1H OCEAN\n",
      "14650      NEAR OCEAN\n",
      "3230           INLAND\n",
      "3555        <1H OCEAN\n",
      "19480          INLAND\n",
      "8879        <1H OCEAN\n",
      "13685          INLAND\n",
      "4937        <1H OCEAN\n",
      "4861        <1H OCEAN\n",
      "\n",
      "Most Machine Learning algorithms prefer to work with numbers anyway, so let’s con‐\n",
      "vert these categories from text to numbers. For this, we can use Scikit-Learn’s Ordina\n",
      "lEncoder class19:\n",
      "\n",
      ">>> from sklearn.preprocessing import OrdinalEncoder\n",
      ">>> ordinal_encoder = OrdinalEncoder()\n",
      "\n",
      "18 Some predictors also provide methods to measure the confidence of their predictions.\n",
      "\n",
      "19 This class is available since Scikit-Learn 0.20. If you use an earlier version, please consider upgrading, or use\n",
      "\n",
      "Pandas’ Series.factorize() method.\n",
      "\n",
      "Prepare the Data for Machine Learning Algorithms \n",
      "\n",
      "| \n",
      "\n",
      "69\n",
      "\n",
      "\f>>> housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
      ">>> housing_cat_encoded[:10]\n",
      "array([[0.],\n",
      "       [0.],\n",
      "       [4.],\n",
      "       [1.],\n",
      "       [0.],\n",
      "       [1.],\n",
      "       [0.],\n",
      "       [1.],\n",
      "       [0.],\n",
      "       [0.]])\n",
      "\n",
      "You can get the list of categories using the categories_ instance variable. It is a list\n",
      "containing  a  1D  array  of  categories  for  each  categorical  attribute  (in  this  case,  a  list\n",
      "containing a single array since there is just one categorical attribute):\n",
      "\n",
      ">>> ordinal_encoder.categories_\n",
      "[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\n",
      "       dtype=object)]\n",
      "\n",
      "One issue with this representation is that ML algorithms will assume that two nearby\n",
      "values are more similar than two distant values. This may be fine in some cases (e.g.,\n",
      "for ordered categories such as “bad”, “average”, “good”, “excellent”), but it is obviously\n",
      "not  the  case  for  the  ocean_proximity  column  (for  example,  categories  0  and  4  are\n",
      "clearly more similar than categories 0 and 1). To fix this issue, a common solution is\n",
      "to create one binary attribute per category: one attribute equal to 1 when the category\n",
      "is “<1H OCEAN” (and 0 otherwise), another attribute equal to 1 when the category is\n",
      "“INLAND”  (and  0  otherwise),  and  so  on.  This  is  called  one-hot  encoding,  because\n",
      "only one attribute will be equal to 1 (hot), while the others will be 0 (cold). The new\n",
      "attributes are sometimes called dummy attributes. Scikit-Learn provides a  OneHotEn\n",
      "coder class to convert categorical values into one-hot vectors20:\n",
      "\n",
      ">>> from sklearn.preprocessing import OneHotEncoder\n",
      ">>> cat_encoder = OneHotEncoder()\n",
      ">>> housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
      ">>> housing_cat_1hot\n",
      "<16512x5 sparse matrix of type '<class 'numpy.float64'>'\n",
      "  with 16512 stored elements in Compressed Sparse Row format>\n",
      "\n",
      "Notice that the output is a SciPy sparse matrix, instead of a NumPy array. This is very\n",
      "useful when you have categorical attributes with thousands of categories. After one-\n",
      "hot  encoding  we  get  a  matrix  with  thousands  of  columns,  and  the  matrix  is  full  of\n",
      "zeros except for a single 1 per row. Using up tons of memory mostly to store zeros\n",
      "would be very wasteful, so instead a sparse matrix only stores the location of the non‐\n",
      "\n",
      "20 Before Scikit-Learn 0.20, it could only encode integer categorical values, but since 0.20 it can also handle\n",
      "\n",
      "other types of inputs, including text categorical inputs.\n",
      "\n",
      "70 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "\fzero elements. You can use it mostly like a normal 2D array,21 but if you really want to\n",
      "convert it to a (dense) NumPy array, just call the toarray() method:\n",
      "\n",
      ">>> housing_cat_1hot.toarray()\n",
      "array([[1., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       ...,\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.]])\n",
      "\n",
      "Once  again,  you  can  get  the  list  of  categories  using  the  encoder’s  categories_\n",
      "instance variable:\n",
      "\n",
      ">>> cat_encoder.categories_\n",
      "[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\n",
      "       dtype=object)]\n",
      "\n",
      "If a categorical attribute has a large number of possible categories\n",
      "(e.g.,  country  code,  profession,  species,  etc.),  then  one-hot  encod‐\n",
      "ing  will  result  in  a  large  number  of  input  features.  This  may  slow\n",
      "down training and degrade performance. If this happens, you may\n",
      "want to replace the categorical input with useful numerical features\n",
      "related  to  the  categories:  for  example,  you  could  replace  the\n",
      "ocean_proximity feature with the distance to the ocean (similarly,\n",
      "a country code could be replaced with the country’s population and\n",
      "GDP  per  capita).  Alternatively,  you  could  replace  each  category\n",
      "with a learnable low dimensional vector called an embedding. Each\n",
      "category’s representation would be learned during training: this is\n",
      "an  example  of  representation  learning  (see  Chapter  13  and  ???  for\n",
      "more details).\n",
      "\n",
      "Custom Transformers\n",
      "Although  Scikit-Learn  provides  many  useful  transformers,  you  will  need  to  write\n",
      "your  own  for  tasks  such  as  custom  cleanup  operations  or  combining  specific\n",
      "attributes. You will want your transformer to work seamlessly with Scikit-Learn func‐\n",
      "tionalities (such as pipelines), and since Scikit-Learn relies on duck typing (not inher‐\n",
      "itance),  all  you  need  is  to  create  a  class  and  implement  three  methods:  fit()\n",
      "(returning  self),  transform(),  and  fit_transform().  You  can  get  the  last  one  for\n",
      "free by simply adding TransformerMixin as a base class. Also, if you add BaseEstima\n",
      "tor  as  a  base  class  (and  avoid  *args  and  **kargs  in  your  constructor)  you  will  get\n",
      "two  extra  methods  (get_params()  and  set_params())  that  will  be  useful  for  auto‐\n",
      "\n",
      "21 See SciPy’s documentation for more details.\n",
      "\n",
      "Prepare the Data for Machine Learning Algorithms \n",
      "\n",
      "| \n",
      "\n",
      "71\n",
      "\n",
      "\fmatic hyperparameter tuning. For example, here is a small transformer class that adds\n",
      "the combined attributes we discussed earlier:\n",
      "\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "\n",
      "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
      "\n",
      "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
      "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
      "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
      "    def fit(self, X, y=None):\n",
      "        return self  # nothing else to do\n",
      "    def transform(self, X, y=None):\n",
      "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
      "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
      "        if self.add_bedrooms_per_room:\n",
      "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
      "            return np.c_[X, rooms_per_household, population_per_household,\n",
      "                         bedrooms_per_room]\n",
      "        else:\n",
      "            return np.c_[X, rooms_per_household, population_per_household]\n",
      "\n",
      "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
      "housing_extra_attribs = attr_adder.transform(housing.values)\n",
      "\n",
      "In  this  example  the  transformer  has  one  hyperparameter,  add_bedrooms_per_room,\n",
      "set to True by default (it is often helpful to provide sensible defaults). This hyperpara‐\n",
      "meter  will  allow  you  to  easily  find  out  whether  adding  this  attribute  helps  the\n",
      "Machine Learning algorithms or not. More generally, you can add a hyperparameter\n",
      "to  gate  any  data  preparation  step  that  you  are  not  100%  sure  about.  The  more  you\n",
      "automate these data preparation steps, the more combinations you can automatically\n",
      "try out, making it much more likely that you will find a great combination (and sav‐\n",
      "ing you a lot of time).\n",
      "\n",
      "Feature Scaling\n",
      "One of the most important transformations you need to apply to your data is feature\n",
      "scaling. With few exceptions, Machine Learning algorithms don’t perform well when\n",
      "the input numerical attributes have very different scales. This is the case for the hous‐\n",
      "ing data: the total number of rooms ranges from about 6 to 39,320, while the median\n",
      "incomes only range from 0 to 15. Note that scaling the target values is generally not\n",
      "required.\n",
      "\n",
      "There  are  two  common  ways  to  get  all  attributes  to  have  the  same  scale:  min-max\n",
      "scaling and standardization.\n",
      "\n",
      "Min-max  scaling  (many  people  call  this  normalization)  is  quite  simple:  values  are\n",
      "shifted and rescaled so that they end up ranging from 0 to 1. We do this by subtract‐\n",
      "ing  the  min  value  and  dividing  by  the  max  minus  the  min.  Scikit-Learn  provides  a\n",
      "\n",
      "72 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "\ftransformer  called  MinMaxScaler  for  this.  It  has  a  feature_range  hyperparameter\n",
      "that lets you change the range if you don’t want 0–1 for some reason.\n",
      "\n",
      "Standardization  is  quite  different:  first  it  subtracts  the  mean  value  (so  standardized\n",
      "values always have a zero mean), and then it divides by the standard deviation so that\n",
      "the resulting distribution has unit variance. Unlike min-max scaling, standardization\n",
      "does  not  bound  values  to  a  specific  range,  which  may  be  a  problem  for  some  algo‐\n",
      "rithms (e.g., neural networks often expect an input value ranging from 0 to 1). How‐\n",
      "ever, standardization is much less affected by outliers. For example, suppose a district\n",
      "had a median income equal to 100 (by mistake). Min-max scaling would then crush\n",
      "all the other values from 0–15 down to 0–0.15, whereas standardization would not be\n",
      "much affected. Scikit-Learn provides a transformer called StandardScaler for stand‐\n",
      "ardization.\n",
      "\n",
      "As with all the transformations, it is important to fit the scalers to\n",
      "the training data only, not to the full dataset (including the test set).\n",
      "Only then can you use them to transform the training set and the\n",
      "test set (and new data).\n",
      "\n",
      "Transformation Pipelines\n",
      "As you can see, there are many data transformation steps that need to be executed in\n",
      "the  right  order.  Fortunately,  Scikit-Learn  provides  the  Pipeline  class  to  help  with\n",
      "such  sequences  of  transformations.  Here  is  a  small  pipeline  for  the  numerical\n",
      "attributes:\n",
      "\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "num_pipeline = Pipeline([\n",
      "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
      "        ('attribs_adder', CombinedAttributesAdder()),\n",
      "        ('std_scaler', StandardScaler()),\n",
      "    ])\n",
      "\n",
      "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
      "\n",
      "The Pipeline constructor takes a list of name/estimator pairs defining a sequence of\n",
      "steps.  All  but  the  last  estimator  must  be  transformers  (i.e.,  they  must  have  a\n",
      "fit_transform() method). The names can be anything you like (as long as they are\n",
      "unique and don’t contain double underscores “__”): they will come in handy later for\n",
      "hyperparameter tuning.\n",
      "\n",
      "When you call the pipeline’s fit() method, it calls fit_transform() sequentially on\n",
      "all transformers, passing the output of each call as the parameter to the next call, until\n",
      "it reaches the final estimator, for which it just calls the fit() method.\n",
      "\n",
      "Prepare the Data for Machine Learning Algorithms \n",
      "\n",
      "| \n",
      "\n",
      "73\n",
      "\n",
      "\fThe pipeline exposes the same methods as the final estimator. In this example, the last\n",
      "estimator  is  a  StandardScaler,  which  is  a  transformer,  so  the  pipeline  has  a  trans\n",
      "form() method that applies all the transforms to the data in sequence (and of course\n",
      "also a fit_transform() method, which is the one we used).\n",
      "\n",
      "So  far,  we  have  handled  the  categorical  columns  and  the  numerical  columns  sepa‐\n",
      "rately. It would be more convenient to have a single transformer able to handle all col‐\n",
      "umns,  applying  the  appropriate  transformations  to  each  column.  In  version  0.20,\n",
      "Scikit-Learn introduced the ColumnTransformer for this purpose, and the good news\n",
      "is that it works great with Pandas DataFrames. Let’s use it to apply all the transforma‐\n",
      "tions to the housing data:\n",
      "\n",
      "from sklearn.compose import ColumnTransformer\n",
      "\n",
      "num_attribs = list(housing_num)\n",
      "cat_attribs = [\"ocean_proximity\"]\n",
      "\n",
      "full_pipeline = ColumnTransformer([\n",
      "        (\"num\", num_pipeline, num_attribs),\n",
      "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
      "    ])\n",
      "\n",
      "housing_prepared = full_pipeline.fit_transform(housing)\n",
      "\n",
      "Here is how this works: first we import the ColumnTransformer class, next we get the\n",
      "list  of  numerical  column  names  and  the  list  of  categorical  column  names,  and  we\n",
      "construct a ColumnTransformer. The constructor requires a list of tuples, where each\n",
      "tuple  contains  a  name22,  a  transformer  and  a  list  of  names  (or  indices)  of  columns\n",
      "that the transformer should be applied to. In this example, we specify that the numer‐\n",
      "ical columns should be transformed using the num_pipeline that we defined earlier,\n",
      "and the categorical columns should be transformed using a OneHotEncoder. Finally,\n",
      "we apply this ColumnTransformer to the housing data: it applies each transformer to\n",
      "the  appropriate  columns  and  concatenates  the  outputs  along  the  second  axis  (the\n",
      "transformers must return the same number of rows).\n",
      "\n",
      "Note that the OneHotEncoder returns a sparse matrix, while the num_pipeline returns\n",
      "a  dense  matrix.  When  there  is  such  a  mix  of  sparse  and  dense  matrices,  the  Colum\n",
      "nTransformer  estimates  the  density  of  the  final  matrix  (i.e.,  the  ratio  of  non-zero\n",
      "cells), and it returns a sparse matrix if the density is lower than a given threshold (by\n",
      "default,  sparse_threshold=0.3).  In  this  example,  it  returns  a  dense  matrix.  And\n",
      "that’s it! We have a preprocessing pipeline that takes the full housing data and applies\n",
      "the appropriate transformations to each column.\n",
      "\n",
      "22 Just like for pipelines, the name can be anything as long as it does not contain double underscores.\n",
      "\n",
      "74 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "\fInstead  of  a  transformer,  you  can  specify  the  string  \"drop\"  if  you\n",
      "want  the  columns  to  be  dropped.  Or  you  can  specify  \"pass\n",
      "through\" if you want the columns to be left untouched. By default,\n",
      "the remaining columns (i.e., the ones that were not listed) will be\n",
      "dropped,  but  you  can  set  the  remainder  hyperparameter  to  any\n",
      "transformer (or to \"passthrough\") if you want these columns to be\n",
      "handled differently.\n",
      "\n",
      "If you are using Scikit-Learn 0.19 or earlier, you can use a third-party library such as\n",
      "sklearn-pandas, or roll out your own custom transformer to get the same function‐\n",
      "ality  as  the  ColumnTransformer.  Alternatively,  you  can  use  the  FeatureUnion  class\n",
      "which  can  also  apply  different  transformers  and  concatenate  their  outputs,  but  you\n",
      "cannot  specify  different  columns  for  each  transformer,  they  all  apply  to  the  whole\n",
      "data. It is possible to work around this limitation using a custom transformer for col‐\n",
      "umn selection (see the Jupyter notebook for an example).\n",
      "\n",
      "Select and Train a Model\n",
      "At  last!  You  framed  the  problem,  you  got  the  data  and  explored  it,  you  sampled  a\n",
      "training  set  and  a  test  set,  and  you  wrote  transformation  pipelines  to  clean  up  and\n",
      "prepare your data for Machine Learning algorithms automatically. You are now ready\n",
      "to select and train a Machine Learning model.\n",
      "\n",
      "Training and Evaluating on the Training Set\n",
      "The good news is that thanks to all these previous steps, things are now going to be\n",
      "much simpler than you might think. Let’s first train a Linear Regression model, like\n",
      "we did in the previous chapter:\n",
      "\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "lin_reg = LinearRegression()\n",
      "lin_reg.fit(housing_prepared, housing_labels)\n",
      "\n",
      "Done!  You  now  have  a  working  Linear  Regression  model.  Let’s  try  it  out  on  a  few\n",
      "instances from the training set:\n",
      "\n",
      ">>> some_data = housing.iloc[:5]\n",
      ">>> some_labels = housing_labels.iloc[:5]\n",
      ">>> some_data_prepared = full_pipeline.transform(some_data)\n",
      ">>> print(\"Predictions:\", lin_reg.predict(some_data_prepared))\n",
      "Predictions: [ 210644.6045  317768.8069  210956.4333  59218.9888  189747.5584]\n",
      ">>> print(\"Labels:\", list(some_labels))\n",
      "Labels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]\n",
      "\n",
      "Select and Train a Model \n",
      "\n",
      "| \n",
      "\n",
      "75\n",
      "\n",
      "\fIt works, although the predictions are not exactly accurate (e.g., the first prediction is\n",
      "off by close to 40%!). Let’s measure this regression model’s RMSE on the whole train‐\n",
      "ing set using Scikit-Learn’s mean_squared_error function:\n",
      "\n",
      ">>> from sklearn.metrics import mean_squared_error\n",
      ">>> housing_predictions = lin_reg.predict(housing_prepared)\n",
      ">>> lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
      ">>> lin_rmse = np.sqrt(lin_mse)\n",
      ">>> lin_rmse\n",
      "68628.19819848922\n",
      "\n",
      "Okay,  this  is  better  than  nothing  but  clearly  not  a  great  score:  most  districts’\n",
      "median_housing_values range between $120,000 and $265,000, so a typical predic‐\n",
      "tion error of $68,628 is not very satisfying. This is an example of a model underfitting\n",
      "the  training  data.  When  this  happens  it  can  mean  that  the  features  do  not  provide\n",
      "enough  information  to  make  good  predictions,  or  that  the  model  is  not  powerful\n",
      "enough. As we saw in the previous chapter, the main ways to fix underfitting are to\n",
      "select a more powerful model, to feed the training algorithm with better features, or\n",
      "to  reduce  the  constraints  on  the  model.  This  model  is  not  regularized,  so  this  rules\n",
      "out  the  last  option.  You  could  try  to  add  more  features  (e.g.,  the  log  of  the  popula‐\n",
      "tion), but first let’s try a more complex model to see how it does.\n",
      "\n",
      "Let’s  train  a  DecisionTreeRegressor.  This  is  a  powerful  model,  capable  of  finding\n",
      "complex  nonlinear  relationships  in  the  data  (Decision  Trees  are  presented  in  more\n",
      "detail in Chapter 6). The code should look familiar by now:\n",
      "\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "\n",
      "tree_reg = DecisionTreeRegressor()\n",
      "tree_reg.fit(housing_prepared, housing_labels)\n",
      "\n",
      "Now that the model is trained, let’s evaluate it on the training set:\n",
      "\n",
      ">>> housing_predictions = tree_reg.predict(housing_prepared)\n",
      ">>> tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
      ">>> tree_rmse = np.sqrt(tree_mse)\n",
      ">>> tree_rmse\n",
      "0.0\n",
      "\n",
      "Wait, what!? No error at all? Could this model really be absolutely perfect? Of course,\n",
      "it is much more likely that the model has badly overfit the data. How can you be sure?\n",
      "As we saw earlier, you don’t want to touch the test set until you are ready to launch a\n",
      "model you are confident about, so you need to use part of the training set for train‐\n",
      "ing, and part for model validation.\n",
      "\n",
      "Better Evaluation Using Cross-Validation\n",
      "One way to evaluate the Decision Tree model would be to use the train_test_split\n",
      "function to split the training set into a smaller training set and a validation set, then\n",
      "\n",
      "76 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "\ftrain your models against the smaller training set and evaluate them against the vali‐\n",
      "dation set. It’s a bit of work, but nothing too difficult and it would work fairly well.\n",
      "\n",
      "A great alternative is to use Scikit-Learn’s K-fold cross-validation feature. The follow‐\n",
      "ing code randomly splits the training set into 10 distinct subsets called folds, then it\n",
      "trains  and  evaluates  the  Decision  Tree  model  10  times,  picking  a  different  fold  for\n",
      "evaluation  every  time  and  training  on  the  other  9  folds.  The  result  is  an  array  con‐\n",
      "taining the 10 evaluation scores:\n",
      "\n",
      "from sklearn.model_selection import cross_val_score\n",
      "scores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n",
      "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
      "tree_rmse_scores = np.sqrt(-scores)\n",
      "\n",
      "Scikit-Learn’s  cross-validation  features  expect  a  utility  function\n",
      "(greater  is  better)  rather  than  a  cost  function  (lower  is  better),  so\n",
      "the scoring function is actually the opposite of the MSE (i.e., a neg‐\n",
      "ative  value),  which  is  why  the  preceding  code  computes  -scores\n",
      "before calculating the square root.\n",
      "\n",
      "Let’s look at the results:\n",
      "\n",
      ">>> def display_scores(scores):\n",
      "...     print(\"Scores:\", scores)\n",
      "...     print(\"Mean:\", scores.mean())\n",
      "...     print(\"Standard deviation:\", scores.std())\n",
      "...\n",
      ">>> display_scores(tree_rmse_scores)\n",
      "Scores: [70194.33680785 66855.16363941 72432.58244769 70758.73896782\n",
      " 71115.88230639 75585.14172901 70262.86139133 70273.6325285\n",
      " 75366.87952553 71231.65726027]\n",
      "Mean: 71407.68766037929\n",
      "Standard deviation: 2439.4345041191004\n",
      "\n",
      "Now the Decision Tree doesn’t look as good as it did earlier. In fact, it seems to per‐\n",
      "form  worse  than  the  Linear  Regression  model!  Notice  that  cross-validation  allows\n",
      "you to get not only an estimate of the performance of your model, but also a measure\n",
      "of how precise this estimate is (i.e., its standard deviation). The Decision Tree has a\n",
      "score of approximately 71,407, generally ±2,439. You would not have this information\n",
      "if you just used one validation set. But cross-validation comes at the cost of training\n",
      "the model several times, so it is not always possible.\n",
      "\n",
      "Let’s compute the same scores for the Linear Regression model just to be sure:\n",
      "\n",
      ">>> lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n",
      "...                              scoring=\"neg_mean_squared_error\", cv=10)\n",
      "...\n",
      ">>> lin_rmse_scores = np.sqrt(-lin_scores)\n",
      ">>> display_scores(lin_rmse_scores)\n",
      "\n",
      "Select and Train a Model \n",
      "\n",
      "| \n",
      "\n",
      "77\n",
      "\n",
      "\fScores: [66782.73843989 66960.118071   70347.95244419 74739.57052552\n",
      " 68031.13388938 71193.84183426 64969.63056405 68281.61137997\n",
      " 71552.91566558 67665.10082067]\n",
      "Mean: 69052.46136345083\n",
      "Standard deviation: 2731.674001798348\n",
      "\n",
      "That’s  right:  the  Decision  Tree  model  is  overfitting  so  badly  that  it  performs  worse\n",
      "than the Linear Regression model.\n",
      "\n",
      "Let’s  try  one  last  model  now:  the  RandomForestRegressor.  As  we  will  see  in  Chap‐\n",
      "ter 7, Random Forests work by training many Decision Trees on random subsets of\n",
      "the features, then averaging out their predictions. Building a model on top of many\n",
      "other models is called Ensemble Learning, and it is often a great way to push ML algo‐\n",
      "rithms even further. We will skip most of the code since it is essentially the same as\n",
      "for the other models:\n",
      "\n",
      ">>> from sklearn.ensemble import RandomForestRegressor\n",
      ">>> forest_reg = RandomForestRegressor()\n",
      ">>> forest_reg.fit(housing_prepared, housing_labels)\n",
      ">>> [...]\n",
      ">>> forest_rmse\n",
      "18603.515021376355\n",
      ">>> display_scores(forest_rmse_scores)\n",
      "Scores: [49519.80364233 47461.9115823  50029.02762854 52325.28068953\n",
      " 49308.39426421 53446.37892622 48634.8036574  47585.73832311\n",
      " 53490.10699751 50021.5852922 ]\n",
      "Mean: 50182.303100336096\n",
      "Standard deviation: 2097.0810550985693\n",
      "\n",
      "Wow, this is much better: Random Forests look very promising. However, note that\n",
      "the score on the training set is still much lower than on the validation sets, meaning\n",
      "that the model is still overfitting the training set. Possible solutions for overfitting are\n",
      "to simplify the model, constrain it (i.e., regularize it), or get a lot more training data.\n",
      "However, before you dive much deeper in Random Forests, you should try out many\n",
      "other models from various categories of Machine Learning algorithms (several Sup‐\n",
      "port Vector Machines with different kernels, possibly a neural network, etc.), without\n",
      "spending too much time tweaking the hyperparameters. The goal is to shortlist a few\n",
      "(two to five) promising models.\n",
      "\n",
      "78 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "\fYou  should  save  every  model  you  experiment  with,  so  you  can\n",
      "come back easily to any model you want. Make sure you save both\n",
      "the  hyperparameters  and  the  trained  parameters,  as  well  as  the\n",
      "cross-validation  scores  and  perhaps  the  actual  predictions  as  well.\n",
      "This  will  allow  you  to  easily  compare  scores  across  model  types,\n",
      "and  compare  the  types  of  errors  they  make.  You  can  easily  save\n",
      "Scikit-Learn  models  by  using  Python’s  pickle  module,  or  using\n",
      "sklearn.externals.joblib, which is more efficient at serializing \n",
      "large NumPy arrays:\n",
      "\n",
      "from sklearn.externals import joblib\n",
      "\n",
      "joblib.dump(my_model, \"my_model.pkl\")\n",
      "# and later...\n",
      "my_model_loaded = joblib.load(\"my_model.pkl\")\n",
      "\n",
      "Fine-Tune Your Model\n",
      "Let’s  assume  that  you  now  have  a  shortlist  of  promising  models.  You  now  need  to\n",
      "fine-tune them. Let’s look at a few ways you can do that.\n",
      "\n",
      "Grid Search\n",
      "One way to do that would be to fiddle with the hyperparameters manually, until you\n",
      "find a great combination of hyperparameter values. This would be very tedious work,\n",
      "and you may not have time to explore many combinations.\n",
      "\n",
      "Instead you should get Scikit-Learn’s GridSearchCV to search for you. All you need to\n",
      "do is tell it which hyperparameters you want it to experiment with, and what values to\n",
      "try out, and it will evaluate all the possible combinations of hyperparameter values,\n",
      "using cross-validation. For example, the following code searches for the best combi‐\n",
      "nation of hyperparameter values for the RandomForestRegressor:\n",
      "\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "param_grid = [\n",
      "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
      "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
      "  ]\n",
      "\n",
      "forest_reg = RandomForestRegressor()\n",
      "\n",
      "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
      "                           scoring='neg_mean_squared_error',\n",
      "                           return_train_score=True)\n",
      "\n",
      "grid_search.fit(housing_prepared, housing_labels)\n",
      "\n",
      "Fine-Tune Your Model \n",
      "\n",
      "| \n",
      "\n",
      "79\n",
      "\n",
      "\fWhen you have no idea what value a hyperparameter should have,\n",
      "a  simple  approach  is  to  try  out  consecutive  powers  of  10  (or  a\n",
      "smaller number if you want a more fine-grained search, as shown\n",
      "in this example with the n_estimators hyperparameter).\n",
      "\n",
      "This  param_grid  tells  Scikit-Learn  to  first  evaluate  all  3  ×  4  =  12  combinations  of\n",
      "n_estimators  and  max_features  hyperparameter  values  specified  in  the  first  dict\n",
      "(don’t worry about what these hyperparameters mean for now; they will be explained\n",
      "in  Chapter  7),  then  try  all  2  ×  3  =  6  combinations  of  hyperparameter  values  in  the\n",
      "second dict, but this time with the bootstrap hyperparameter set to False instead of\n",
      "True (which is the default value for this hyperparameter).\n",
      "\n",
      "All in all, the grid search will explore 12 + 6 = 18 combinations of RandomForestRe\n",
      "gressor hyperparameter values, and it will train each model five times (since we are\n",
      "using five-fold cross validation). In other words, all in all, there will be 18 × 5 = 90\n",
      "rounds of training! It may take quite a long time, but when it is done you can get the\n",
      "best combination of parameters like this:\n",
      "\n",
      ">>> grid_search.best_params_\n",
      "{'max_features': 8, 'n_estimators': 30}\n",
      "\n",
      "Since  8  and  30  are  the  maximum  values  that  were  evaluated,  you\n",
      "should  probably  try  searching  again  with  higher  values,  since  the\n",
      "score may continue to improve.\n",
      "\n",
      "You can also get the best estimator directly:\n",
      "\n",
      ">>> grid_search.best_estimator_\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features=8, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=30, n_jobs=None, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False)\n",
      "\n",
      "If  GridSearchCV  is  initialized  with  refit=True  (which  is  the\n",
      "default),  then  once  it  finds  the  best  estimator  using  cross-\n",
      "validation, it retrains it on the whole training set. This is usually a\n",
      "good idea since feeding it more data will likely improve its perfor‐\n",
      "mance.\n",
      "\n",
      "And of course the evaluation scores are also available:\n",
      "\n",
      ">>> cvres = grid_search.cv_results_\n",
      ">>> for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
      "\n",
      "80 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "\f...     print(np.sqrt(-mean_score), params)\n",
      "...\n",
      "63669.05791727153 {'max_features': 2, 'n_estimators': 3}\n",
      "55627.16171305252 {'max_features': 2, 'n_estimators': 10}\n",
      "53384.57867637289 {'max_features': 2, 'n_estimators': 30}\n",
      "60965.99185930139 {'max_features': 4, 'n_estimators': 3}\n",
      "52740.98248528835 {'max_features': 4, 'n_estimators': 10}\n",
      "50377.344409590376 {'max_features': 4, 'n_estimators': 30}\n",
      "58663.84733372485 {'max_features': 6, 'n_estimators': 3}\n",
      "52006.15355973719 {'max_features': 6, 'n_estimators': 10}\n",
      "50146.465964159885 {'max_features': 6, 'n_estimators': 30}\n",
      "57869.25504027614 {'max_features': 8, 'n_estimators': 3}\n",
      "51711.09443660957 {'max_features': 8, 'n_estimators': 10}\n",
      "49682.25345942335 {'max_features': 8, 'n_estimators': 30}\n",
      "62895.088889905004 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}\n",
      "54658.14484390074 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}\n",
      "59470.399594730654 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}\n",
      "52725.01091081235 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}\n",
      "57490.612956065226 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}\n",
      "51009.51445842374 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\n",
      "\n",
      "In this example, we obtain the best solution by setting the max_features hyperpara‐\n",
      "meter  to  8,  and  the  n_estimators  hyperparameter  to  30.  The  RMSE  score  for  this\n",
      "combination is 49,682, which is slightly better than the score you got earlier using the\n",
      "default  hyperparameter  values  (which  was  50,182).  Congratulations,  you  have  suc‐\n",
      "cessfully fine-tuned your best model!\n",
      "\n",
      "Don’t forget that you can treat some of the data preparation steps as\n",
      "hyperparameters.  For  example,  the  grid  search  will  automatically\n",
      "find out whether or not to add a feature you were not sure about\n",
      "(e.g.,  using  the  add_bedrooms_per_room  hyperparameter  of  your\n",
      "CombinedAttributesAdder  transformer).  It  may  similarly  be  used\n",
      "to  automatically  find  the  best  way  to  handle  outliers,  missing  fea‐\n",
      "tures, feature selection, and more.\n",
      "\n",
      "Randomized Search\n",
      "The grid search approach is fine when you are exploring relatively few combinations,\n",
      "like in the previous example, but when the hyperparameter search space is large, it is\n",
      "often preferable to use RandomizedSearchCV instead. This class can be used in much\n",
      "the same way as the GridSearchCV class, but instead of trying out all possible combi‐\n",
      "nations, it evaluates a given number of random combinations by selecting a random\n",
      "value for each hyperparameter at every iteration. This approach has two main bene‐\n",
      "fits:\n",
      "\n",
      "Fine-Tune Your Model \n",
      "\n",
      "| \n",
      "\n",
      "81\n",
      "\n",
      "\f• If you let the randomized search run for, say, 1,000 iterations, this approach will\n",
      "explore 1,000 different values for each hyperparameter (instead of just a few val‐\n",
      "ues per hyperparameter with the grid search approach).\n",
      "\n",
      "• You have more control over the computing budget you want to allocate to hyper‐\n",
      "\n",
      "parameter search, simply by setting the number of iterations.\n",
      "\n",
      "Ensemble Methods\n",
      "Another way to fine-tune your system is to try to combine the models that perform\n",
      "best.  The  group  (or  “ensemble”)  will  often  perform  better  than  the  best  individual\n",
      "model  (just  like  Random  Forests  perform  better  than  the  individual  Decision  Trees\n",
      "they rely on), especially if the individual models make very different types of errors.\n",
      "We will cover this topic in more detail in Chapter 7.\n",
      "\n",
      "Analyze the Best Models and Their Errors\n",
      "You will often gain good insights on the problem by inspecting the best models. For\n",
      "example,  the  RandomForestRegressor  can  indicate  the  relative  importance  of  each\n",
      "attribute for making accurate predictions:\n",
      "\n",
      ">>> feature_importances = grid_search.best_estimator_.feature_importances_\n",
      ">>> feature_importances\n",
      "array([7.33442355e-02, 6.29090705e-02, 4.11437985e-02, 1.46726854e-02,\n",
      "       1.41064835e-02, 1.48742809e-02, 1.42575993e-02, 3.66158981e-01,\n",
      "       5.64191792e-02, 1.08792957e-01, 5.33510773e-02, 1.03114883e-02,\n",
      "       1.64780994e-01, 6.02803867e-05, 1.96041560e-03, 2.85647464e-03])\n",
      "\n",
      "Let’s display these importance scores next to their corresponding attribute names:\n",
      "\n",
      ">>> extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
      ">>> cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n",
      ">>> cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
      ">>> attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
      ">>> sorted(zip(feature_importances, attributes), reverse=True)\n",
      "[(0.3661589806181342, 'median_income'),\n",
      " (0.1647809935615905, 'INLAND'),\n",
      " (0.10879295677551573, 'pop_per_hhold'),\n",
      " (0.07334423551601242, 'longitude'),\n",
      " (0.0629090704826203, 'latitude'),\n",
      " (0.05641917918195401, 'rooms_per_hhold'),\n",
      " (0.05335107734767581, 'bedrooms_per_room'),\n",
      " (0.041143798478729635, 'housing_median_age'),\n",
      " (0.014874280890402767, 'population'),\n",
      " (0.014672685420543237, 'total_rooms'),\n",
      " (0.014257599323407807, 'households'),\n",
      " (0.014106483453584102, 'total_bedrooms'),\n",
      " (0.010311488326303787, '<1H OCEAN'),\n",
      " (0.002856474637320158, 'NEAR OCEAN'),\n",
      "\n",
      "82 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "\f (0.00196041559947807, 'NEAR BAY'),\n",
      " (6.028038672736599e-05, 'ISLAND')]\n",
      "\n",
      "With this information, you may want to try dropping some of the less useful features\n",
      "(e.g., apparently only one ocean_proximity category is really useful, so you could try\n",
      "dropping the others).\n",
      "\n",
      "You should also look at the specific errors that your system makes, then try to under‐\n",
      "stand why it makes them and what could fix the problem (adding extra features or, on\n",
      "the contrary, getting rid of uninformative ones, cleaning up outliers, etc.).\n",
      "\n",
      "Evaluate Your System on the Test Set\n",
      "After tweaking your models for a while, you eventually have a system that performs\n",
      "sufficiently well. Now is the time to evaluate the final model on the test set. There is\n",
      "nothing  special  about  this  process;  just  get  the  predictors  and  the  labels  from  your\n",
      "test  set,  run  your  full_pipeline  to  transform  the  data  (call  transform(),  not\n",
      "fit_transform(), you do not want to fit the test set!), and evaluate the final model\n",
      "on the test set:\n",
      "\n",
      "final_model = grid_search.best_estimator_\n",
      "\n",
      "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
      "y_test = strat_test_set[\"median_house_value\"].copy()\n",
      "\n",
      "X_test_prepared = full_pipeline.transform(X_test)\n",
      "\n",
      "final_predictions = final_model.predict(X_test_prepared)\n",
      "\n",
      "final_mse = mean_squared_error(y_test, final_predictions)\n",
      "final_rmse = np.sqrt(final_mse)   # => evaluates to 47,730.2\n",
      "\n",
      "In  some  cases,  such  a  point  estimate  of  the  generalization  error  will  not  be  quite\n",
      "enough to convince you to launch: what if it is just 0.1% better than the model cur‐\n",
      "rently in production? You might want to have an idea of how precise this estimate is.\n",
      "For this, you can compute a 95% confidence interval for the generalization error using\n",
      "scipy.stats.t.interval():\n",
      "\n",
      ">>> from scipy import stats\n",
      ">>> confidence = 0.95\n",
      ">>> squared_errors = (final_predictions - y_test) ** 2\n",
      ">>> np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n",
      "...                          loc=squared_errors.mean(),\n",
      "...                          scale=stats.sem(squared_errors)))\n",
      "...\n",
      "array([45685.10470776, 49691.25001878])\n",
      "\n",
      "The performance will usually be slightly worse than what you measured using cross-\n",
      "validation  if  you  did  a  lot  of  hyperparameter  tuning  (because  your  system  ends  up\n",
      "fine-tuned to perform well on the validation data, and will likely not perform as well\n",
      "\n",
      "Fine-Tune Your Model \n",
      "\n",
      "| \n",
      "\n",
      "83\n",
      "\n",
      "\fon unknown datasets). It is not the case in this example, but when this happens you\n",
      "must resist the temptation to tweak the hyperparameters to make the numbers look\n",
      "good on the test set; the improvements would be unlikely to generalize to new data.\n",
      "\n",
      "Now  comes  the  project  prelaunch  phase:  you  need  to  present  your  solution  (high‐\n",
      "lighting  what  you  have  learned,  what  worked  and  what  did  not,  what  assumptions\n",
      "were made, and what your system’s limitations are), document everything, and create\n",
      "nice  presentations  with  clear  visualizations  and  easy-to-remember  statements  (e.g.,\n",
      "“the median income is the number one predictor of housing prices”). In this Califor‐\n",
      "nia  housing  example,  the  final  performance  of  the  system  is  not  better  than  the\n",
      "experts’,  but  it  may  still  be  a  good  idea  to  launch  it,  especially  if  this  frees  up  some\n",
      "time for the experts so they can work on more interesting and productive tasks.\n",
      "\n",
      "Launch, Monitor, and Maintain Your System\n",
      "Perfect, you got approval to launch! You need to get your solution ready for produc‐\n",
      "tion,  in  particular  by  plugging  the  production  input  data  sources  into  your  system\n",
      "and writing tests.\n",
      "\n",
      "You  also  need  to  write  monitoring  code  to  check  your  system’s  live  performance  at\n",
      "regular intervals and trigger alerts when it drops. This is important to catch not only\n",
      "sudden  breakage,  but  also  performance  degradation.  This  is  quite  common  because\n",
      "models tend to “rot” as data evolves over time, unless the models are regularly trained\n",
      "on fresh data.\n",
      "\n",
      "Evaluating your system’s performance will require sampling the system’s predictions\n",
      "and  evaluating  them.  This  will  generally  require  a  human  analysis.  These  analysts\n",
      "may  be  field  experts,  or  workers  on  a  crowdsourcing  platform  (such  as  Amazon\n",
      "Mechanical Turk or CrowdFlower). Either way, you need to plug the human evalua‐\n",
      "tion pipeline into your system.\n",
      "\n",
      "You  should  also  make  sure  you  evaluate  the  system’s  input  data  quality.  Sometimes\n",
      "performance  will  degrade  slightly  because  of  a  poor  quality  signal  (e.g.,  a  malfunc‐\n",
      "tioning sensor sending random values, or another team’s output becoming stale), but\n",
      "it may take a while before your system’s performance degrades enough to trigger an\n",
      "alert. If you monitor your system’s inputs, you may catch this earlier. Monitoring the\n",
      "inputs is particularly important for online learning systems.\n",
      "\n",
      "Finally,  you  will  generally  want  to  train  your  models  on  a  regular  basis  using  fresh\n",
      "data. You should automate this process as much as possible. If you don’t, you are very\n",
      "likely to refresh your model only every six months (at best), and your system’s perfor‐\n",
      "mance may fluctuate severely over time. If your system is an online learning system,\n",
      "you should make sure you save snapshots of its state at regular intervals so you can\n",
      "easily roll back to a previously working state.\n",
      "\n",
      "84 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 2: End-to-End Machine Learning Project\n",
      "\n",
      "\fTry It Out!\n",
      "Hopefully  this  chapter  gave  you  a  good  idea  of  what  a  Machine  Learning  project\n",
      "looks like, and showed you some of the tools you can use to train a great system. As\n",
      "you  can  see,  much  of  the  work  is  in  the  data  preparation  step,  building  monitoring\n",
      "tools, setting up human evaluation pipelines, and automating regular model training.\n",
      "The  Machine  Learning  algorithms  are  also  important,  of  course,  but  it  is  probably\n",
      "preferable  to  be  comfortable  with  the  overall  process  and  know  three  or  four  algo‐\n",
      "rithms well rather than to spend all your time exploring advanced algorithms and not\n",
      "enough time on the overall process.\n",
      "\n",
      "So, if you have not already done so, now is a good time to pick up a laptop, select a\n",
      "dataset that you are interested in, and try to go through the whole process from A to\n",
      "Z.  A  good  place  to  start  is  on  a  competition  website  such  as  http://kaggle.com/:  you\n",
      "will have a dataset to play with, a clear goal, and people to share the experience with.\n",
      "\n",
      "Exercises\n",
      "Using this chapter’s housing dataset:\n",
      "\n",
      "1. Try a Support Vector Machine regressor (sklearn.svm.SVR), with various hyper‐\n",
      "parameters  such  as  kernel=\"linear\"  (with  various  values  for  the  C  hyperpara‐\n",
      "meter)  or  kernel=\"rbf\"  (with  various  values \n",
      "the  C  and  gamma\n",
      "hyperparameters). Don’t worry about what these hyperparameters mean for now.\n",
      "How does the best SVR predictor perform?\n",
      "\n",
      "for \n",
      "\n",
      "2. Try replacing GridSearchCV with RandomizedSearchCV.\n",
      "\n",
      "3. Try  adding  a  transformer  in  the  preparation  pipeline  to  select  only  the  most\n",
      "\n",
      "important attributes.\n",
      "\n",
      "4. Try  creating  a  single  pipeline  that  does  the  full  data  preparation  plus  the  final\n",
      "\n",
      "prediction.\n",
      "\n",
      "5. Automatically explore some preparation options using GridSearchCV.\n",
      "\n",
      "Solutions  to  these  exercises  are  available  in  the  online  Jupyter  notebooks  at  https://\n",
      "github.com/ageron/handson-ml2.\n",
      "\n",
      "Try It Out! \n",
      "\n",
      "| \n",
      "\n",
      "85\n",
      "\n",
      "\f\fCHAPTER 3\n",
      "Classification\n",
      "\n",
      "With  Early  Release  ebooks,  you  get  books  in  their  earliest  form—\n",
      "the author’s raw and unedited content as he or she writes—so you\n",
      "can  take  advantage  of  these  technologies  long  before  the  official\n",
      "release of these titles. The following will be Chapter 3 in the final\n",
      "release of the book.\n",
      "\n",
      "In  Chapter  1  we  mentioned  that  the  most  common  supervised  learning  tasks  are\n",
      "regression (predicting values) and classification (predicting classes). In Chapter 2 we\n",
      "explored a regression task, predicting housing values, using various algorithms such\n",
      "as Linear Regression, Decision Trees, and Random Forests (which will be explained\n",
      "in  further  detail  in  later  chapters).  Now  we  will  turn  our  attention  to  classification\n",
      "systems.\n",
      "\n",
      "MNIST\n",
      "In  this  chapter,  we  will  be  using  the  MNIST  dataset,  which  is  a  set  of  70,000  small\n",
      "images of digits handwritten by high school students and employees of the US Cen‐\n",
      "sus Bureau. Each image is labeled with the digit it represents. This set has been stud‐\n",
      "ied so much that it is often called the “Hello World” of Machine Learning: whenever\n",
      "people  come  up  with  a  new  classification  algorithm,  they  are  curious  to  see  how  it\n",
      "will  perform  on  MNIST.  Whenever  someone  learns  Machine  Learning,  sooner  or\n",
      "later they tackle MNIST.\n",
      "\n",
      "Scikit-Learn provides many helper functions to download popular datasets. MNIST is\n",
      "one of them. The following code fetches the MNIST dataset:1\n",
      "\n",
      "1 By default Scikit-Learn caches downloaded datasets in a directory called $HOME/scikit_learn_data.\n",
      "\n",
      "87\n",
      "\n",
      "\f>>> from sklearn.datasets import fetch_openml\n",
      ">>> mnist = fetch_openml('mnist_784', version=1)\n",
      ">>> mnist.keys()\n",
      "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'details',\n",
      "           'categories', 'url'])\n",
      "\n",
      "Datasets loaded by Scikit-Learn generally have a similar dictionary structure includ‐\n",
      "ing:\n",
      "\n",
      "• A DESCR key describing the dataset\n",
      "• A data key containing an array with one row per instance and one column per\n",
      "\n",
      "feature\n",
      "\n",
      "• A target key containing an array with the labels\n",
      "\n",
      "Let’s look at these arrays:\n",
      "\n",
      ">>> X, y = mnist[\"data\"], mnist[\"target\"]\n",
      ">>> X.shape\n",
      "(70000, 784)\n",
      ">>> y.shape\n",
      "(70000,)\n",
      "\n",
      "There are 70,000 images, and each image has 784 features. This is because each image\n",
      "is  28×28  pixels,  and  each  feature  simply  represents  one  pixel’s  intensity,  from  0\n",
      "(white) to 255 (black). Let’s take a peek at one digit from the dataset. All you need to\n",
      "do is grab an instance’s feature vector, reshape it to a 28×28 array, and display it using\n",
      "Matplotlib’s imshow() function:\n",
      "\n",
      "import matplotlib as mpl\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "some_digit = X[0]\n",
      "some_digit_image = some_digit.reshape(28, 28)\n",
      "\n",
      "plt.imshow(some_digit_image, cmap = mpl.cm.binary, interpolation=\"nearest\")\n",
      "plt.axis(\"off\")\n",
      "plt.show()\n",
      "\n",
      "This looks like a 5, and indeed that’s what the label tells us:\n",
      "\n",
      "88 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 3: Classification\n",
      "\n",
      "\f>>> y[0]\n",
      "'5'\n",
      "\n",
      "Note that the label is a string. We prefer numbers, so let’s cast y to integers:\n",
      "\n",
      ">>> y = y.astype(np.uint8)\n",
      "\n",
      "Figure 3-1 shows a few more images from the MNIST dataset to give you a feel for\n",
      "the complexity of the classification task.\n",
      "\n",
      "Figure 3-1. A few digits from the MNIST dataset\n",
      "\n",
      "But wait! You should always create a test set and set it aside before inspecting the data\n",
      "closely. The MNIST dataset is actually already split into a training set (the first 60,000\n",
      "images) and a test set (the last 10,000 images):\n",
      "\n",
      "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
      "\n",
      "The  training  set  is  already  shuffled  for  us,  which  is  good  as  this  guarantees  that  all\n",
      "cross-validation folds will be similar (you don’t want one fold to be missing some dig‐\n",
      "its).  Moreover,  some  learning  algorithms  are  sensitive  to  the  order  of  the  training\n",
      "\n",
      "MNIST \n",
      "\n",
      "| \n",
      "\n",
      "89\n",
      "\n",
      "\finstances, and they perform poorly if they get many similar instances in a row. Shuf‐\n",
      "fling the dataset ensures that this won’t happen.2\n",
      "\n",
      "Training a Binary Classifier\n",
      "Let’s  simplify  the  problem  for  now  and  only  try  to  identify  one  digit—for  example,\n",
      "the number 5. This “5-detector” will be an example of a binary classifier, capable of\n",
      "distinguishing between just two classes, 5 and not-5. Let’s create the target vectors for\n",
      "this classification task:\n",
      "\n",
      "y_train_5 = (y_train == 5)  # True for all 5s, False for all other digits.\n",
      "y_test_5 = (y_test == 5)\n",
      "\n",
      "Okay, now let’s pick a classifier and train it. A good place to start is with a Stochastic\n",
      "Gradient Descent (SGD) classifier, using Scikit-Learn’s SGDClassifier class. This clas‐\n",
      "sifier  has  the  advantage  of  being  capable  of  handling  very  large  datasets  efficiently.\n",
      "This is in part because SGD deals with training instances independently, one at a time\n",
      "(which also makes SGD well suited for online learning), as we will see later. Let’s create\n",
      "an SGDClassifier and train it on the whole training set:\n",
      "\n",
      "from sklearn.linear_model import SGDClassifier\n",
      "\n",
      "sgd_clf = SGDClassifier(random_state=42)\n",
      "sgd_clf.fit(X_train, y_train_5)\n",
      "\n",
      "The  SGDClassifier  relies  on  randomness  during  training  (hence\n",
      "the  name  “stochastic”).  If  you  want  reproducible  results,  you\n",
      "should set the random_state parameter.\n",
      "\n",
      "Now you can use it to detect images of the number 5:\n",
      "\n",
      ">>> sgd_clf.predict([some_digit])\n",
      "array([ True])\n",
      "\n",
      "The classifier guesses that this image represents a 5 (True). Looks like it guessed right\n",
      "in this particular case! Now, let’s evaluate this model’s performance.\n",
      "\n",
      "Performance Measures\n",
      "Evaluating a classifier is often significantly trickier than evaluating a regressor, so we\n",
      "will  spend  a  large  part  of  this  chapter  on  this  topic.  There  are  many  performance\n",
      "\n",
      "2 Shuffling may be a bad idea in some contexts—for example, if you are working on time series data (such as\n",
      "\n",
      "stock market prices or weather conditions). We will explore this in the next chapters.\n",
      "\n",
      "90 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 3: Classification\n",
      "\n",
      "\fmeasures available, so grab another coffee and get ready to learn many new concepts\n",
      "and acronyms!\n",
      "\n",
      "Measuring Accuracy Using Cross-Validation\n",
      "A good way to evaluate a model is to use cross-validation, just as you did in Chap‐\n",
      "ter 2.\n",
      "\n",
      "Implementing Cross-Validation\n",
      "Occasionally you will need more control over the cross-validation process than what\n",
      "Scikit-Learn  provides  off-the-shelf.  In  these  cases,  you  can  implement  cross-\n",
      "validation  yourself;  it  is  actually  fairly  straightforward.  The  following  code  does\n",
      "roughly the same thing as Scikit-Learn’s cross_val_score() function, and prints the \n",
      "same result:\n",
      "\n",
      "from sklearn.model_selection import StratifiedKFold\n",
      "from sklearn.base import clone\n",
      "\n",
      "skfolds = StratifiedKFold(n_splits=3, random_state=42)\n",
      "\n",
      "for train_index, test_index in skfolds.split(X_train, y_train_5):\n",
      "    clone_clf = clone(sgd_clf)\n",
      "    X_train_folds = X_train[train_index]\n",
      "    y_train_folds = y_train_5[train_index]\n",
      "    X_test_fold = X_train[test_index]\n",
      "    y_test_fold = y_train_5[test_index]\n",
      "\n",
      "    clone_clf.fit(X_train_folds, y_train_folds)\n",
      "    y_pred = clone_clf.predict(X_test_fold)\n",
      "    n_correct = sum(y_pred == y_test_fold)\n",
      "    print(n_correct / len(y_pred))  # prints 0.9502, 0.96565 and 0.96495\n",
      "\n",
      "The StratifiedKFold class performs stratified sampling (as explained in Chapter 2)\n",
      "to produce folds that contain a representative ratio of each class. At each iteration the\n",
      "code creates a clone of the classifier, trains that clone on the training folds, and makes\n",
      "predictions  on  the  test  fold.  Then  it  counts  the  number  of  correct  predictions  and\n",
      "outputs the ratio of correct predictions.\n",
      "\n",
      "Let’s  use  the  cross_val_score()  function  to  evaluate  your  SGDClassifier  model\n",
      "using  K-fold  cross-validation,  with  three  folds.  Remember  that  K-fold  cross-\n",
      "validation means splitting the training set into K-folds (in this case, three), then mak‐\n",
      "ing  predictions  and  evaluating  them  on  each  fold  using  a  model  trained  on  the\n",
      "remaining folds (see Chapter 2):\n",
      "\n",
      "Performance Measures \n",
      "\n",
      "| \n",
      "\n",
      "91\n",
      "\n",
      "\f>>> from sklearn.model_selection import cross_val_score\n",
      ">>> cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n",
      "array([0.96355, 0.93795, 0.95615])\n",
      "\n",
      "Wow! Above 93% accuracy (ratio of correct predictions) on all cross-validation folds? \n",
      "This  looks  amazing,  doesn’t  it?  Well,  before  you  get  too  excited,  let’s  look  at  a  very\n",
      "dumb classifier that just classifies every single image in the “not-5” class:\n",
      "\n",
      "from sklearn.base import BaseEstimator\n",
      "\n",
      "class Never5Classifier(BaseEstimator):\n",
      "    def fit(self, X, y=None):\n",
      "        pass\n",
      "    def predict(self, X):\n",
      "        return np.zeros((len(X), 1), dtype=bool)\n",
      "\n",
      "Can you guess this model’s accuracy? Let’s find out:\n",
      "\n",
      ">>> never_5_clf = Never5Classifier()\n",
      ">>> cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n",
      "array([0.91125, 0.90855, 0.90915])\n",
      "\n",
      "That’s right, it has over 90% accuracy! This is simply because only about 10% of the\n",
      "images are 5s, so if you always guess that an image is not a 5, you will be right about\n",
      "90% of the time. Beats Nostradamus.\n",
      "\n",
      "This demonstrates why accuracy is generally not the preferred performance measure\n",
      "for classifiers, especially when you are dealing with skewed datasets (i.e., when some\n",
      "classes are much more frequent than others).\n",
      "\n",
      "Confusion Matrix\n",
      "A much better way to evaluate the performance of a classifier is to look at the confu‐\n",
      "sion matrix. The general idea is to count the number of times instances of class A are\n",
      "classified as class B. For example, to know the number of times the classifier confused\n",
      "images of 5s with 3s, you would look in the 5th row and 3rd column of the confusion\n",
      "matrix.\n",
      "\n",
      "To compute the confusion matrix, you first need to have a set of predictions, so they\n",
      "can be compared to the actual targets. You could make predictions on the test set, but\n",
      "let’s keep it untouched for now (remember that you want to use the test set only at the\n",
      "very  end  of  your  project,  once  you  have  a  classifier  that  you  are  ready  to  launch).\n",
      "Instead, you can use the cross_val_predict() function:\n",
      "\n",
      "from sklearn.model_selection import cross_val_predict\n",
      "\n",
      "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\n",
      "\n",
      "Just  like  the  cross_val_score()  function,  cross_val_predict()  performs  K-fold\n",
      "cross-validation, but instead of returning the evaluation scores, it returns the predic‐\n",
      "\n",
      "92 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 3: Classification\n",
      "\n",
      "\ftions  made  on  each  test  fold.  This  means  that  you  get  a  clean  prediction  for  each\n",
      "instance in the training set (“clean” meaning that the prediction is made by a model\n",
      "that never saw the data during training).\n",
      "\n",
      "Now you are ready to get the confusion matrix using the confusion_matrix() func‐\n",
      "tion.  Just  pass  it  the  target  classes  (y_train_5)  and  the  predicted  classes\n",
      "(y_train_pred):\n",
      "\n",
      ">>> from sklearn.metrics import confusion_matrix\n",
      ">>> confusion_matrix(y_train_5, y_train_pred)\n",
      "array([[53057,  1522],\n",
      "       [ 1325,  4096]])\n",
      "\n",
      "Each row in a confusion matrix represents an actual class, while each column repre‐\n",
      "sents a predicted class. The first row of this matrix considers non-5 images (the nega‐\n",
      "tive  class):  53,057  of  them  were  correctly  classified  as  non-5s  (they  are  called  true\n",
      "negatives),  while  the  remaining  1,522  were  wrongly  classified  as  5s  (false  positives).\n",
      "The  second  row  considers  the  images  of  5s  (the  positive  class):  1,325  were  wrongly\n",
      "classified as non-5s (false negatives), while the remaining 4,096 were correctly classi‐\n",
      "fied as 5s (true positives). A perfect classifier would have only true positives and true\n",
      "negatives, so its confusion matrix would have nonzero values only on its main diago‐\n",
      "nal (top left to bottom right):\n",
      "\n",
      ">>> y_train_perfect_predictions = y_train_5  # pretend we reached perfection\n",
      ">>> confusion_matrix(y_train_5, y_train_perfect_predictions)\n",
      "array([[54579,     0],\n",
      "       [    0,  5421]])\n",
      "\n",
      "The confusion matrix gives you a lot of information, but sometimes you may prefer a\n",
      "more concise metric. An interesting one to look at is the accuracy of the positive pre‐\n",
      "dictions; this is called the precision of the classifier (Equation 3-1).\n",
      "\n",
      "Equation 3-1. Precision\n",
      "\n",
      "precision =\n",
      "\n",
      "TP\n",
      "TP + FP\n",
      "\n",
      "TP is the number of true positives, and FP is the number of false positives.\n",
      "\n",
      "A trivial way to have perfect precision is to make one single positive prediction and\n",
      "ensure it is correct (precision = 1/1 = 100%). This would not be very useful since the\n",
      "classifier  would  ignore  all  but  one  positive  instance.  So  precision  is  typically  used\n",
      "along  with  another  metric  named  recall,  also  called  sensitivity  or  true  positive  rate\n",
      "\n",
      "Performance Measures \n",
      "\n",
      "| \n",
      "\n",
      "93\n",
      "\n",
      "\f(TPR): this is the ratio of positive instances that are correctly detected by the classifier\n",
      "(Equation 3-2).\n",
      "\n",
      "Equation 3-2. Recall\n",
      "\n",
      "recall =\n",
      "\n",
      "TP\n",
      "TP + FN\n",
      "\n",
      "FN is of course the number of false negatives.\n",
      "\n",
      "If you are confused about the confusion matrix, Figure 3-2 may help.\n",
      "\n",
      "Figure 3-2. An illustrated confusion matrix\n",
      "\n",
      "Precision and Recall\n",
      "Scikit-Learn provides several functions to compute classifier metrics, including preci‐\n",
      "sion and recall:\n",
      "\n",
      ">>> from sklearn.metrics import precision_score, recall_score\n",
      ">>> precision_score(y_train_5, y_train_pred) # == 4096 / (4096 + 1522)\n",
      "0.7290850836596654\n",
      ">>> recall_score(y_train_5, y_train_pred) # == 4096 / (4096 + 1325)\n",
      "0.7555801512636044\n",
      "\n",
      "Now your 5-detector does not look as shiny as it did when you looked at its accuracy.\n",
      "When it claims an image represents a 5, it is correct only 72.9% of the time. More‐\n",
      "over, it only detects 75.6% of the 5s.\n",
      "\n",
      "It is often convenient to combine precision and recall into a single metric called the F1\n",
      "score, in particular if you need a simple way to compare two classifiers. The F1 score is \n",
      "the harmonic mean of precision and recall (Equation 3-3). Whereas the regular mean\n",
      "\n",
      "94 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 3: Classification\n",
      "\n",
      "\ftreats all values equally, the harmonic mean gives much more weight to low values.\n",
      "As a result, the classifier will only get a high F1 score if both recall and precision are\n",
      "high.\n",
      "\n",
      "Equation 3-3. F1\n",
      "\n",
      "F1 =\n",
      "\n",
      "2\n",
      "1\n",
      "precision +\n",
      "\n",
      "1\n",
      "recall\n",
      "\n",
      "= 2 ×\n",
      "\n",
      "precision × recall\n",
      "precision + recall\n",
      "\n",
      "=\n",
      "\n",
      "TP\n",
      "FN + FP\n",
      "2\n",
      "\n",
      "TP +\n",
      "\n",
      "To compute the F1 score, simply call the f1_score() function:\n",
      "\n",
      ">>> from sklearn.metrics import f1_score\n",
      ">>> f1_score(y_train_5, y_train_pred)\n",
      "0.7420962043663375\n",
      "\n",
      "The F1 score favors classifiers that have similar precision and recall. This is not always\n",
      "what you want: in some contexts you mostly care about precision, and in other con‐\n",
      "texts you really care about recall. For example, if you trained a classifier to detect vid‐\n",
      "eos  that  are  safe  for  kids,  you  would  probably  prefer  a  classifier  that  rejects  many\n",
      "good videos (low recall) but keeps only safe ones (high precision), rather than a clas‐\n",
      "sifier that has a much higher recall but lets a few really bad videos show up in your\n",
      "product (in such cases, you may even want to add a human pipeline to check the clas‐\n",
      "sifier’s  video  selection).  On  the  other  hand,  suppose  you  train  a  classifier  to  detect\n",
      "shoplifters on surveillance images: it is probably fine if your classifier has only 30%\n",
      "precision  as  long  as  it  has  99%  recall  (sure,  the  security  guards  will  get  a  few  false\n",
      "alerts, but almost all shoplifters will get caught).\n",
      "\n",
      "Unfortunately,  you  can’t  have  it  both  ways:  increasing  precision  reduces  recall,  and\n",
      "vice versa. This is called the precision/recall tradeoff.\n",
      "\n",
      "Precision/Recall Tradeoff\n",
      "To understand this tradeoff, let’s look at how the SGDClassifier makes its classifica‐\n",
      "tion decisions. For each instance, it computes a score based on a decision function, \n",
      "and  if  that  score  is  greater  than  a  threshold,  it  assigns  the  instance  to  the  positive\n",
      "class, or else it assigns it to the negative class. Figure 3-3 shows a few digits positioned\n",
      "from the lowest score on the left to the highest score on the right. Suppose the deci‐\n",
      "sion threshold is positioned at the central arrow (between the two 5s): you will find 4\n",
      "true positives (actual 5s) on the right of that threshold, and one false positive (actually\n",
      "a  6).  Therefore,  with  that  threshold,  the  precision  is  80%  (4  out  of  5).  But  out  of  6\n",
      "actual  5s,  the  classifier  only  detects  4,  so  the  recall  is  67%  (4  out  of  6).  Now  if  you\n",
      "raise  the  threshold  (move  it  to  the  arrow  on  the  right),  the  false  positive  (the  6)\n",
      "becomes a true negative, thereby increasing precision (up to 100% in this case), but\n",
      "one true positive becomes a false negative, decreasing recall down to 50%. Conversely,\n",
      "lowering the threshold increases recall and reduces precision.\n",
      "\n",
      "Performance Measures \n",
      "\n",
      "| \n",
      "\n",
      "95\n",
      "\n",
      "\fFigure 3-3. Decision threshold and precision/recall tradeoff\n",
      "\n",
      "Scikit-Learn does not let you set the threshold directly, but it does give you access to\n",
      "the decision scores that it uses to make predictions. Instead of calling the classifier’s\n",
      "predict()  method,  you  can  call  its  decision_function()  method,  which  returns  a\n",
      "score for each instance, and then make predictions based on those scores using any\n",
      "threshold you want:\n",
      "\n",
      ">>> y_scores = sgd_clf.decision_function([some_digit])\n",
      ">>> y_scores\n",
      "array([2412.53175101])\n",
      ">>> threshold = 0\n",
      ">>> y_some_digit_pred = (y_scores > threshold)\n",
      "array([ True])\n",
      "\n",
      "The SGDClassifier uses a threshold equal to 0, so the previous code returns the same\n",
      "result as the predict() method (i.e., True). Let’s raise the threshold:\n",
      "\n",
      ">>> threshold = 8000\n",
      ">>> y_some_digit_pred = (y_scores > threshold)\n",
      ">>> y_some_digit_pred\n",
      "array([False])\n",
      "\n",
      "This  confirms  that  raising  the  threshold  decreases  recall.  The  image  actually  repre‐\n",
      "sents a 5, and the classifier detects it when the threshold is 0, but it misses it when the\n",
      "threshold is increased to 8,000.\n",
      "\n",
      "Now how do you decide which threshold to use? For this you will first need to get the\n",
      "scores  of  all  instances  in  the  training  set  using  the  cross_val_predict()  function\n",
      "again,  but  this  time  specifying  that  you  want  it  to  return  decision  scores  instead  of\n",
      "predictions:\n",
      "\n",
      "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,\n",
      "                             method=\"decision_function\")\n",
      "\n",
      "Now with these scores you can compute precision and recall for all possible thresh‐\n",
      "olds using the precision_recall_curve() function:\n",
      "\n",
      "96 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 3: Classification\n",
      "\n",
      "\ffrom sklearn.metrics import precision_recall_curve\n",
      "\n",
      "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n",
      "\n",
      "Finally,  you  can  plot  precision  and  recall  as  functions  of  the  threshold  value  using\n",
      "Matplotlib (Figure 3-4):\n",
      "\n",
      "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
      "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
      "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
      "    [...] # highlight the threshold, add the legend, axis label and grid\n",
      "\n",
      "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
      "plt.show()\n",
      "\n",
      "Figure 3-4. Precision and recall versus the decision threshold\n",
      "\n",
      "You may wonder why the precision curve is bumpier than the recall\n",
      "curve in Figure 3-4. The reason is that precision may sometimes go\n",
      "down when you raise the threshold (although in general it will go\n",
      "up).  To  understand  why,  look  back  at  Figure  3-3  and  notice  what\n",
      "happens when you start from the central threshold and move it just\n",
      "one digit to the right: precision goes from 4/5 (80%) down to 3/4\n",
      "(75%). On the other hand, recall can only go down when the thres‐\n",
      "hold is increased, which explains why its curve looks smooth.\n",
      "\n",
      "Another  way  to  select  a  good  precision/recall  tradeoff  is  to  plot  precision  directly\n",
      "against recall, as shown in Figure 3-5 (the same threshold as earlier is highlighed).\n",
      "\n",
      "Performance Measures \n",
      "\n",
      "| \n",
      "\n",
      "97\n",
      "\n",
      "\fFigure 3-5. Precision versus recall\n",
      "\n",
      "You  can  see  that  precision  really  starts  to  fall  sharply  around  80%  recall.  You  will\n",
      "probably want to select a precision/recall tradeoff just before that drop—for example,\n",
      "at around 60% recall. But of course the choice depends on your project.\n",
      "\n",
      "So let’s suppose you decide to aim for 90% precision. You look up the first plot and\n",
      "find  that  you  need  to  use  a  threshold  of  about  8,000.  To  be  more  precise  you  can\n",
      "search  for  the  lowest  threshold  that  gives  you  at  least  90%  precision  (np.argmax()\n",
      "will give us the first index of the maximum value, which in this case means the first\n",
      "True value):\n",
      "\n",
      "threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)] # ~7816\n",
      "\n",
      "To  make  predictions  (on  the  training  set  for  now),  instead  of  calling  the  classifier’s\n",
      "predict() method, you can just run this code:\n",
      "\n",
      "y_train_pred_90 = (y_scores >= threshold_90_precision)\n",
      "\n",
      "Let’s check these predictions’ precision and recall:\n",
      "\n",
      ">>> precision_score(y_train_5, y_train_pred_90)\n",
      "0.9000380083618396\n",
      ">>> recall_score(y_train_5, y_train_pred_90)\n",
      "0.4368197749492714\n",
      "\n",
      "Great, you have a 90% precision classifier ! As you can see, it is fairly easy to create a\n",
      "classifier with virtually any precision you want: just set a high enough threshold, and\n",
      "you’re  done.  Hmm,  not  so  fast.  A  high-precision  classifier  is  not  very  useful  if  its \n",
      "recall is too low!\n",
      "\n",
      "98 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 3: Classification\n",
      "\n",
      "\fIf  someone  says  “let’s  reach  99%  precision,”  you  should  ask,  “at\n",
      "what recall?”\n",
      "\n",
      "The ROC Curve\n",
      "The receiver operating characteristic (ROC) curve is another common tool used with\n",
      "binary classifiers. It is very similar to the precision/recall curve, but instead of plot‐\n",
      "ting precision versus recall, the ROC curve plots the true positive rate (another name\n",
      "for recall) against the false positive rate. The FPR is the ratio of negative instances that\n",
      "are incorrectly classified as positive. It is equal to one minus the true negative rate, \n",
      "which  is  the  ratio  of  negative  instances  that  are  correctly  classified  as  negative.  The\n",
      "TNR  is  also  called  specificity.  Hence  the  ROC  curve  plots  sensitivity  (recall)  versus\n",
      "1 – specificity.\n",
      "\n",
      "To plot the ROC curve, you first need to compute the TPR and FPR for various thres‐\n",
      "hold values, using the roc_curve() function:\n",
      "\n",
      "from sklearn.metrics import roc_curve\n",
      "\n",
      "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\n",
      "\n",
      "Then you can plot the FPR against the TPR using Matplotlib. This code produces the\n",
      "plot in Figure 3-6:\n",
      "\n",
      "def plot_roc_curve(fpr, tpr, label=None):\n",
      "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
      "    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal\n",
      "    [...] # Add axis labels and grid\n",
      "\n",
      "plot_roc_curve(fpr, tpr)\n",
      "plt.show()\n",
      "\n",
      "Performance Measures \n",
      "\n",
      "| \n",
      "\n",
      "99\n",
      "\n",
      "\fFigure 3-6. ROC curve\n",
      "\n",
      "Once  again  there  is  a  tradeoff:  the  higher  the  recall  (TPR),  the  more  false  positives\n",
      "(FPR) the classifier produces. The dotted line represents the ROC curve of a purely\n",
      "random classifier; a good classifier stays as far away from that line as possible (toward\n",
      "the top-left corner).\n",
      "\n",
      "One way to compare classifiers is to measure the area under the curve (AUC). A per‐\n",
      "fect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will\n",
      "have a ROC AUC equal to 0.5. Scikit-Learn provides a function to compute the ROC\n",
      "AUC:\n",
      "\n",
      ">>> from sklearn.metrics import roc_auc_score\n",
      ">>> roc_auc_score(y_train_5, y_scores)\n",
      "0.9611778893101814\n",
      "\n",
      "Since  the  ROC  curve  is  so  similar  to  the  precision/recall  (or  PR)\n",
      "curve, you may wonder how to decide which one to use. As a rule\n",
      "of  thumb,  you  should  prefer  the  PR  curve  whenever  the  positive\n",
      "class is rare or when you care more about the false positives than\n",
      "the  false  negatives,  and  the  ROC  curve  otherwise.  For  example,\n",
      "looking at the previous ROC curve (and the ROC AUC score), you\n",
      "may  think  that  the  classifier  is  really  good.  But  this  is  mostly\n",
      "because  there  are  few  positives  (5s)  compared  to  the  negatives\n",
      "(non-5s). In contrast, the PR curve makes it clear that the classifier\n",
      "has  room  for  improvement  (the  curve  could  be  closer  to  the  top-\n",
      "right corner).\n",
      "\n",
      "100 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 3: Classification\n",
      "\n",
      "\fLet’s  train  a  RandomForestClassifier  and  compare  its  ROC  curve  and  ROC  AUC\n",
      "score  to  the  SGDClassifier.  First,  you  need  to  get  scores  for  each  instance  in  the\n",
      "training  set.  But  due  to  the  way  it  works  (see  Chapter  7),  the  RandomForestClassi\n",
      "fier  class  does  not  have  a  decision_function()  method.  Instead  it  has  a  pre\n",
      "dict_proba()  method.  Scikit-Learn  classifiers  generally  have  one  or  the  other.  The\n",
      "predict_proba() method returns an array containing a row per instance and a col‐\n",
      "umn per class, each containing the probability that the given instance belongs to the\n",
      "given class (e.g., 70% chance that the image represents a 5):\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "forest_clf = RandomForestClassifier(random_state=42)\n",
      "y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3,\n",
      "                                    method=\"predict_proba\")\n",
      "\n",
      "But to plot a ROC curve, you need scores, not probabilities. A simple solution is to\n",
      "use the positive class’s probability as the score:\n",
      "\n",
      "y_scores_forest = y_probas_forest[:, 1]   # score = proba of positive class\n",
      "fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5,y_scores_forest)\n",
      "\n",
      "Now you are ready to plot the ROC curve. It is useful to plot the first ROC curve as\n",
      "well to see how they compare (Figure 3-7):\n",
      "\n",
      "plt.plot(fpr, tpr, \"b:\", label=\"SGD\")\n",
      "plot_roc_curve(fpr_forest, tpr_forest, \"Random Forest\")\n",
      "plt.legend(loc=\"lower right\")\n",
      "plt.show()\n",
      "\n",
      "Figure 3-7. Comparing ROC curves\n",
      "\n",
      "Performance Measures \n",
      "\n",
      "| \n",
      "\n",
      "101\n",
      "\n",
      "\fAs you can see in Figure 3-7, the RandomForestClassifier’s ROC curve looks much\n",
      "better  than  the  SGDClassifier’s:  it  comes  much  closer  to  the  top-left  corner.  As  a\n",
      "result, its ROC AUC score is also significantly better:\n",
      "\n",
      ">>> roc_auc_score(y_train_5, y_scores_forest)\n",
      "0.9983436731328145\n",
      "\n",
      "Try measuring the precision and recall scores: you should find 99.0% precision and\n",
      "86.6% recall. Not too bad!\n",
      "\n",
      "Hopefully you now know how to train binary classifiers, choose the appropriate met‐\n",
      "ric for your task, evaluate your classifiers using cross-validation, select the precision/\n",
      "recall  tradeoff  that  fits  your  needs,  and  compare  various  models  using  ROC  curves\n",
      "and ROC AUC scores. Now let’s try to detect more than just the 5s.\n",
      "\n",
      "Multiclass Classification\n",
      "Whereas binary classifiers distinguish between two classes, multiclass classifiers (also\n",
      "called multinomial classifiers) can distinguish between more than two classes.\n",
      "\n",
      "Some  algorithms  (such  as  Random  Forest  classifiers  or  naive  Bayes  classifiers)  are\n",
      "capable of handling multiple classes directly. Others (such as Support Vector Machine\n",
      "classifiers or Linear classifiers) are strictly binary classifiers. However, there are vari‐\n",
      "ous  strategies  that  you  can  use  to  perform  multiclass  classification  using  multiple\n",
      "binary classifiers.\n",
      "\n",
      "For  example,  one  way  to  create  a  system  that  can  classify  the  digit  images  into  10\n",
      "classes (from 0 to 9) is to train 10 binary classifiers, one for each digit (a 0-detector, a\n",
      "1-detector, a 2-detector, and so on). Then when you want to classify an image, you get\n",
      "the decision score from each classifier for that image and you select the class whose\n",
      "classifier  outputs  the  highest  score.  This  is  called  the  one-versus-all  (OvA)  strategy \n",
      "(also called one-versus-the-rest).\n",
      "\n",
      "Another strategy is to train a binary classifier for every pair of digits: one to distin‐\n",
      "guish 0s and 1s, another to distinguish 0s and 2s, another for 1s and 2s, and so on.\n",
      "This  is  called  the  one-versus-one  (OvO)  strategy.  If  there  are  N  classes,  you  need  to\n",
      "train  N  ×  (N  –  1)  /  2  classifiers.  For  the  MNIST  problem,  this  means  training  45\n",
      "binary  classifiers!  When  you  want  to  classify  an  image,  you  have  to  run  the  image\n",
      "through all 45 classifiers and see which class wins the most duels. The main advan‐\n",
      "tage of OvO is that each classifier only needs to be trained on the part of the training\n",
      "set for the two classes that it must distinguish.\n",
      "\n",
      "Some algorithms (such as Support Vector Machine classifiers) scale poorly with the\n",
      "size of the training set, so for these algorithms OvO is preferred since it is faster to\n",
      "train  many  classifiers  on  small  training  sets  than  training  few  classifiers  on  large\n",
      "training sets. For most binary classification algorithms, however, OvA is preferred.\n",
      "\n",
      "102 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 3: Classification\n",
      "\n",
      "\fScikit-Learn detects when you try to use a binary classification algorithm for a multi‐\n",
      "class classification task, and it automatically runs OvA (except for SVM classifiers for\n",
      "which it uses OvO). Let’s try this with the SGDClassifier:\n",
      "\n",
      ">>> sgd_clf.fit(X_train, y_train)  # y_train, not y_train_5\n",
      ">>> sgd_clf.predict([some_digit])\n",
      "array([5], dtype=uint8)\n",
      "\n",
      "That was easy! This code trains the SGDClassifier on the training set using the origi‐\n",
      "nal  target  classes  from  0  to  9  (y_train),  instead  of  the  5-versus-all  target  classes\n",
      "(y_train_5). Then it makes a prediction (a correct one in this case). Under the hood,\n",
      "Scikit-Learn  actually  trained  10  binary  classifiers,  got  their  decision  scores  for  the\n",
      "image, and selected the class with the highest score.\n",
      "\n",
      "To  see  that  this  is  indeed  the  case,  you  can  call  the  decision_function()  method.\n",
      "Instead  of  returning  just  one  score  per  instance,  it  now  returns  10  scores,  one  per\n",
      "class:\n",
      "\n",
      ">>> some_digit_scores = sgd_clf.decision_function([some_digit])\n",
      ">>> some_digit_scores\n",
      "array([[-15955.22627845, -38080.96296175, -13326.66694897,\n",
      "           573.52692379, -17680.6846644 ,   2412.53175101,\n",
      "        -25526.86498156, -12290.15704709,  -7946.05205023,\n",
      "        -10631.35888549]])\n",
      "\n",
      "The highest score is indeed the one corresponding to class 5:\n",
      "\n",
      ">>> np.argmax(some_digit_scores)\n",
      "5\n",
      ">>> sgd_clf.classes_\n",
      "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)\n",
      ">>> sgd_clf.classes_[5]\n",
      "5\n",
      "\n",
      "When a classifier is trained, it stores the list of target classes in its\n",
      "classes_ attribute, ordered by value. In this case, the index of each\n",
      "class  in  the  classes_  array  conveniently  matches  the  class  itself\n",
      "(e.g., the class at index 5 happens to be class 5), but in general you\n",
      "won’t be so lucky.\n",
      "\n",
      "If you want to force ScikitLearn to use one-versus-one or one-versus-all, you can use\n",
      "the OneVsOneClassifier or OneVsRestClassifier classes. Simply create an instance\n",
      "and pass a binary classifier to its constructor. For example, this code creates a multi‐\n",
      "class classifier using the OvO strategy, based on a SGDClassifier:\n",
      "\n",
      ">>> from sklearn.multiclass import OneVsOneClassifier\n",
      ">>> ovo_clf = OneVsOneClassifier(SGDClassifier(random_state=42))\n",
      ">>> ovo_clf.fit(X_train, y_train)\n",
      ">>> ovo_clf.predict([some_digit])\n",
      "\n",
      "Multiclass Classification \n",
      "\n",
      "| \n",
      "\n",
      "103\n",
      "\n",
      "\farray([5], dtype=uint8)\n",
      ">>> len(ovo_clf.estimators_)\n",
      "45\n",
      "\n",
      "Training a RandomForestClassifier is just as easy:\n",
      "\n",
      ">>> forest_clf.fit(X_train, y_train)\n",
      ">>> forest_clf.predict([some_digit])\n",
      "array([5], dtype=uint8)\n",
      "\n",
      "This  time  Scikit-Learn  did  not  have  to  run  OvA  or  OvO  because  Random  Forest\n",
      "classifiers  can  directly  classify  instances  into  multiple  classes.  You  can  call\n",
      "predict_proba()  to  get  the  list  of  probabilities  that  the  classifier  assigned  to  each\n",
      "instance for each class:\n",
      "\n",
      ">>> forest_clf.predict_proba([some_digit])\n",
      "array([[0.  , 0.  , 0.01, 0.08, 0.  , 0.9 , 0.  , 0.  , 0.  , 0.01]])\n",
      "\n",
      "You can see that the classifier is fairly confident about its prediction: the 0.9 at the 5th\n",
      "index in the array means that the model estimates a 90% probability that the image\n",
      "represents a 5. It also thinks that the image could instead be a 2, a 3 or a 9, respec‐\n",
      "tively with 1%, 8% and 1% probability.\n",
      "\n",
      "Now of course you want to evaluate these classifiers. As usual, you want to use cross-\n",
      "validation. Let’s evaluate the SGDClassifier’s accuracy using the cross_val_score()\n",
      "function:\n",
      "\n",
      ">>> cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n",
      "array([0.8489802 , 0.87129356, 0.86988048])\n",
      "\n",
      "It gets over 84% on all test folds. If you used a random classifier, you would get 10%\n",
      "accuracy, so this is not such a bad score, but you can still do much better. For exam‐\n",
      "ple,  simply  scaling  the  inputs  (as  discussed  in  Chapter  2)  increases  accuracy  above\n",
      "89%:\n",
      "\n",
      ">>> from sklearn.preprocessing import StandardScaler\n",
      ">>> scaler = StandardScaler()\n",
      ">>> X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\n",
      ">>> cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring=\"accuracy\")\n",
      "array([0.89707059, 0.8960948 , 0.90693604])\n",
      "\n",
      "Error Analysis\n",
      "Of  course,  if  this  were  a  real  project,  you  would  follow  the  steps  in  your  Machine\n",
      "Learning  project  checklist  (see  ???):  exploring  data  preparation  options,  trying  out\n",
      "multiple  models,  shortlisting  the  best  ones  and  fine-tuning  their  hyperparameters\n",
      "using GridSearchCV, and automating as much as possible, as you did in the previous\n",
      "chapter. Here, we will assume that you have found a promising model and you want\n",
      "to  find  ways  to  improve  it.  One  way  to  do  this  is  to  analyze  the  types  of  errors  it\n",
      "makes.\n",
      "\n",
      "104 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 3: Classification\n",
      "\n",
      "\fFirst, you can look at the confusion matrix. You need to make predictions using the\n",
      "cross_val_predict() function, then call the confusion_matrix() function, just like\n",
      "you did earlier:\n",
      "\n",
      ">>> y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\n",
      ">>> conf_mx = confusion_matrix(y_train, y_train_pred)\n",
      ">>> conf_mx\n",
      "array([[5578,    0,   22,    7,    8,   45,   35,    5,  222,    1],\n",
      "       [   0, 6410,   35,   26,    4,   44,    4,    8,  198,   13],\n",
      "       [  28,   27, 5232,  100,   74,   27,   68,   37,  354,   11],\n",
      "       [  23,   18,  115, 5254,    2,  209,   26,   38,  373,   73],\n",
      "       [  11,   14,   45,   12, 5219,   11,   33,   26,  299,  172],\n",
      "       [  26,   16,   31,  173,   54, 4484,   76,   14,  482,   65],\n",
      "       [  31,   17,   45,    2,   42,   98, 5556,    3,  123,    1],\n",
      "       [  20,   10,   53,   27,   50,   13,    3, 5696,  173,  220],\n",
      "       [  17,   64,   47,   91,    3,  125,   24,   11, 5421,   48],\n",
      "       [  24,   18,   29,   67,  116,   39,    1,  174,  329, 5152]])\n",
      "\n",
      "That’s a lot of numbers. It’s often more convenient to look at an image representation\n",
      "of the confusion matrix, using Matplotlib’s matshow() function:\n",
      "\n",
      "plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
      "plt.show()\n",
      "\n",
      "This confusion matrix looks fairly good, since most images are on the main diagonal,\n",
      "which means that they were classified correctly. The 5s look slightly darker than the\n",
      "other digits, which could mean that there are fewer images of 5s in the dataset or that\n",
      "the classifier does not perform as well on 5s as on other digits. In fact, you can verify\n",
      "that both are the case.\n",
      "\n",
      "Let’s focus the plot on the errors. First, you need to divide each value in the confusion\n",
      "matrix by the number of images in the corresponding class, so you can compare error\n",
      "\n",
      "Error Analysis \n",
      "\n",
      "| \n",
      "\n",
      "105\n",
      "\n",
      "\frates instead of absolute number of errors (which would make abundant classes look\n",
      "unfairly bad):\n",
      "\n",
      "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
      "norm_conf_mx = conf_mx / row_sums\n",
      "\n",
      "Now let’s fill the diagonal with zeros to keep only the errors, and let’s plot the result:\n",
      "\n",
      "np.fill_diagonal(norm_conf_mx, 0)\n",
      "plt.matshow(norm_conf_mx, cmap=plt.cm.gray)\n",
      "plt.show()\n",
      "\n",
      "Now you can clearly see the kinds of errors the classifier makes. Remember that rows\n",
      "represent  actual  classes,  while  columns  represent  predicted  classes.  The  column  for\n",
      "class 8 is quite bright, which tells you that many images get misclassified as 8s. How‐\n",
      "ever, the row for class 8 is not that bad, telling you that actual 8s in general get prop‐\n",
      "erly  classified  as  8s.  As  you  can  see,  the  confusion  matrix  is  not  necessarily\n",
      "symmetrical. You can also see that 3s and 5s often get confused (in both directions).\n",
      "\n",
      "Analyzing the confusion matrix can often give you insights on ways to improve your\n",
      "classifier. Looking at this plot, it seems that your efforts should be spent on reducing\n",
      "the  false  8s.  For  example,  you  could  try  to  gather  more  training  data  for  digits  that\n",
      "look like 8s (but are not) so the classifier can learn to distinguish them from real 8s.\n",
      "Or you could engineer new features that would help the classifier—for example, writ‐\n",
      "ing an algorithm to count the number of closed loops (e.g., 8 has two, 6 has one, 5 has\n",
      "none).  Or  you  could  preprocess  the  images  (e.g.,  using  Scikit-Image,  Pillow,  or\n",
      "OpenCV) to make some patterns stand out more, such as closed loops.\n",
      "\n",
      "Analyzing  individual  errors  can  also  be  a  good  way  to  gain  insights  on  what  your\n",
      "classifier is doing and why it is failing, but it is more difficult and time-consuming.\n",
      "\n",
      "106 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 3: Classification\n",
      "\n",
      "\fFor example, let’s plot examples of 3s and 5s (the  plot_digits() function just uses\n",
      "Matplotlib’s imshow() function; see this chapter’s Jupyter notebook for details):\n",
      "\n",
      "cl_a, cl_b = 3, 5\n",
      "X_aa = X_train[(y_train == cl_a) & (y_train_pred == cl_a)]\n",
      "X_ab = X_train[(y_train == cl_a) & (y_train_pred == cl_b)]\n",
      "X_ba = X_train[(y_train == cl_b) & (y_train_pred == cl_a)]\n",
      "X_bb = X_train[(y_train == cl_b) & (y_train_pred == cl_b)]\n",
      "\n",
      "plt.figure(figsize=(8,8))\n",
      "plt.subplot(221); plot_digits(X_aa[:25], images_per_row=5)\n",
      "plt.subplot(222); plot_digits(X_ab[:25], images_per_row=5)\n",
      "plt.subplot(223); plot_digits(X_ba[:25], images_per_row=5)\n",
      "plt.subplot(224); plot_digits(X_bb[:25], images_per_row=5)\n",
      "plt.show()\n",
      "\n",
      "The two 5×5 blocks on the left show digits classified as 3s, and the two 5×5 blocks on\n",
      "the right show images classified as 5s. Some of the digits that the classifier gets wrong\n",
      "(i.e., in the bottom-left and top-right blocks) are so badly written that even a human\n",
      "would have trouble classifying them (e.g., the 5 on the 1st row and 2nd column truly\n",
      "looks like a badly written 3). However, most misclassified images seem like obvious\n",
      "errors to us, and it’s hard to understand why the classifier made the mistakes it did.3\n",
      "The  reason  is  that  we  used  a  simple  SGDClassifier,  which  is  a  linear  model.  All  it\n",
      "does is assign a weight per class to each pixel, and when it sees a new image it just\n",
      "sums up the weighted pixel intensities to get a score for each class. So since 3s and 5s\n",
      "differ only by a few pixels, this model will easily confuse them.\n",
      "\n",
      "3 But remember that our brain is a fantastic pattern recognition system, and our visual system does a lot of\n",
      "\n",
      "complex preprocessing before any information reaches our consciousness, so the fact that it feels simple does\n",
      "not mean that it is.\n",
      "\n",
      "Error Analysis \n",
      "\n",
      "| \n",
      "\n",
      "107\n",
      "\n",
      "\fThe main difference between 3s and 5s is the position of the small line that joins the\n",
      "top line to the bottom arc. If you draw a 3 with the junction slightly shifted to the left,\n",
      "the classifier might classify it as a 5, and vice versa. In other words, this classifier is\n",
      "quite sensitive to image shifting and rotation. So one way to reduce the 3/5 confusion\n",
      "would be to preprocess the images to ensure that they are well centered and not too\n",
      "rotated. This will probably help reduce other errors as well.\n",
      "\n",
      "Multilabel Classification\n",
      "Until now each instance has always been assigned to just one class. In some cases you\n",
      "may  want  your  classifier  to  output  multiple  classes  for  each  instance.  For  example,\n",
      "consider a face-recognition classifier: what should it do if it recognizes several people\n",
      "on the same picture? Of course it should attach one tag per person it recognizes. Say\n",
      "the classifier has been trained to recognize three faces, Alice, Bob, and Charlie; then\n",
      "when it is shown a picture of Alice and Charlie, it should output [1, 0, 1] (meaning\n",
      "“Alice  yes,  Bob  no,  Charlie  yes”).  Such  a  classification  system  that  outputs  multiple\n",
      "binary tags is called a multilabel classification system.\n",
      "\n",
      "We won’t go into face recognition just yet, but let’s look at a simpler example, just for\n",
      "illustration purposes:\n",
      "\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "y_train_large = (y_train >= 7)\n",
      "y_train_odd = (y_train % 2 == 1)\n",
      "y_multilabel = np.c_[y_train_large, y_train_odd]\n",
      "\n",
      "knn_clf = KNeighborsClassifier()\n",
      "knn_clf.fit(X_train, y_multilabel)\n",
      "\n",
      "This  code  creates  a  y_multilabel  array  containing  two  target  labels  for  each  digit\n",
      "image: the first indicates whether or not the digit is large (7, 8, or 9) and the second\n",
      "indicates  whether  or  not  it  is  odd.  The  next  lines  create  a  KNeighborsClassifier \n",
      "instance  (which  supports  multilabel  classification,  but  not  all  classifiers  do)  and  we\n",
      "train it using the multiple targets array. Now you can make a prediction, and notice\n",
      "that it outputs two labels:\n",
      "\n",
      ">>> knn_clf.predict([some_digit])\n",
      "array([[False,  True]])\n",
      "\n",
      "And it gets it right! The digit 5 is indeed not large (False) and odd (True).\n",
      "\n",
      "There are many ways to evaluate a multilabel classifier, and selecting the right metric\n",
      "really depends on your project. For example, one approach is to measure the F1 score\n",
      "for each individual label (or any other binary classifier metric discussed earlier), then\n",
      "simply compute the average score. This code computes the average F1 score across all\n",
      "labels:\n",
      "\n",
      "108 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 3: Classification\n",
      "\n",
      "\f>>> y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)\n",
      ">>> f1_score(y_multilabel, y_train_knn_pred, average=\"macro\")\n",
      "0.976410265560605\n",
      "\n",
      "This assumes that all labels are equally important, which may not be the case. In par‐\n",
      "ticular, if you have many more pictures of Alice than of Bob or Charlie, you may want\n",
      "to give more weight to the classifier’s score on pictures of Alice. One simple option is\n",
      "to give each label a weight equal to its support (i.e., the number of instances with that\n",
      "target label). To do this, simply set average=\"weighted\" in the preceding code.4\n",
      "\n",
      "Multioutput Classification\n",
      "The last type of classification task we are going to discuss here is called multioutput-\n",
      "multiclass classification (or simply multioutput classification). It is simply a generaliza‐\n",
      "tion  of  multilabel  classification  where  each  label  can  be  multiclass  (i.e.,  it  can  have\n",
      "more than two possible values).\n",
      "\n",
      "To illustrate this, let’s build a system that removes noise from images. It will take as\n",
      "input  a  noisy  digit  image,  and  it  will  (hopefully)  output  a  clean  digit  image,  repre‐\n",
      "sented  as  an  array  of  pixel  intensities,  just  like  the  MNIST  images.  Notice  that  the\n",
      "classifier’s output is multilabel (one label per pixel) and each label can have multiple\n",
      "values (pixel intensity ranges from 0 to 255). It is thus an example of a multioutput\n",
      "classification system.\n",
      "\n",
      "The line between classification and regression is sometimes blurry,\n",
      "such as in this example. Arguably, predicting pixel intensity is more\n",
      "akin  to  regression  than  to  classification.  Moreover,  multioutput\n",
      "systems are not limited to classification tasks; you could even have\n",
      "a  system  that  outputs  multiple  labels  per  instance,  including  both\n",
      "class labels and value labels.\n",
      "\n",
      "Let’s  start  by  creating  the  training  and  test  sets  by  taking  the  MNIST  images  and\n",
      "adding noise to their pixel intensities using NumPy’s randint() function. The target\n",
      "images will be the original images:\n",
      "\n",
      "noise = np.random.randint(0, 100, (len(X_train), 784))\n",
      "X_train_mod = X_train + noise\n",
      "noise = np.random.randint(0, 100, (len(X_test), 784))\n",
      "X_test_mod = X_test + noise\n",
      "y_train_mod = X_train\n",
      "y_test_mod = X_test\n",
      "\n",
      "4 Scikit-Learn offers a few other averaging options and multilabel classifier metrics; see the documentation for\n",
      "\n",
      "more details.\n",
      "\n",
      "Multioutput Classification \n",
      "\n",
      "| \n",
      "\n",
      "109\n",
      "\n",
      "\fLet’s take a peek at an image from the test set (yes, we’re snooping on the test data, so\n",
      "you should be frowning right now):\n",
      "\n",
      "On the left is the noisy input image, and on the right is the clean target image. Now\n",
      "let’s train the classifier and make it clean this image:\n",
      "\n",
      "knn_clf.fit(X_train_mod, y_train_mod)\n",
      "clean_digit = knn_clf.predict([X_test_mod[some_index]])\n",
      "plot_digit(clean_digit)\n",
      "\n",
      "Looks close enough to the target! This concludes our tour of classification. Hopefully\n",
      "you  should  now  know  how  to  select  good  metrics  for  classification  tasks,  pick  the\n",
      "appropriate  precision/recall  tradeoff,  compare  classifiers,  and  more  generally  build\n",
      "good classification systems for a variety of tasks.\n",
      "\n",
      "Exercises\n",
      "\n",
      "1. Try to build a classifier for the MNIST dataset that achieves over 97% accuracy\n",
      "on  the  test  set.  Hint:  the  KNeighborsClassifier  works  quite  well  for  this  task;\n",
      "you  just  need  to  find  good  hyperparameter  values  (try  a  grid  search  on  the\n",
      "weights and n_neighbors hyperparameters).\n",
      "\n",
      "2. Write a function that can shift an MNIST image in any direction (left, right, up,\n",
      "or down) by one pixel.5 Then, for each image in the training set, create four shif‐\n",
      "\n",
      "5 You can use the shift() function from the scipy.ndimage.interpolation module. For example,\n",
      "\n",
      "shift(image, [2, 1], cval=0) shifts the image 2 pixels down and 1 pixel to the right.\n",
      "\n",
      "110 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 3: Classification\n",
      "\n",
      "\fted copies (one per direction) and add them to the training set. Finally, train your\n",
      "best model on this expanded training set and measure its accuracy on the test set.\n",
      "You should observe that your model performs even better now! This technique of\n",
      "artificially  growing  the  training  set  is  called  data  augmentation  or  training  set\n",
      "expansion.\n",
      "\n",
      "3. Tackle the Titanic dataset. A great place to start is on Kaggle.\n",
      "\n",
      "4. Build a spam classifier (a more challenging exercise):\n",
      "\n",
      "• Download  examples  of  spam  and  ham  from  Apache  SpamAssassin’s  public\n",
      "\n",
      "datasets.\n",
      "\n",
      "• Unzip the datasets and familiarize yourself with the data format.\n",
      "\n",
      "• Split the datasets into a training set and a test set.\n",
      "\n",
      "• Write  a  data  preparation  pipeline  to  convert  each  email  into  a  feature  vector.\n",
      "Your  preparation  pipeline  should  transform  an  email  into  a  (sparse)  vector\n",
      "indicating  the  presence  or  absence  of  each  possible  word.  For  example,  if  all\n",
      "emails only ever contain four words, “Hello,” “how,” “are,” “you,” then the email\n",
      "“Hello  you  Hello  Hello  you”  would  be  converted  into  a  vector  [1,  0,  0,  1]\n",
      "(meaning  [“Hello”  is  present,  “how”  is  absent,  “are”  is  absent,  “you”  is\n",
      "present]),  or  [3,  0,  0,  2]  if  you  prefer  to  count  the  number  of  occurrences  of\n",
      "each word.\n",
      "\n",
      "• You may want to add hyperparameters to your preparation pipeline to control\n",
      "whether  or  not  to  strip  off  email  headers,  convert  each  email  to  lowercase,\n",
      "remove  punctuation,  replace  all  URLs  with  “URL,”  replace  all  numbers  with\n",
      "“NUMBER,” or even perform stemming (i.e., trim off word endings; there are\n",
      "Python libraries available to do this).\n",
      "\n",
      "• Then try out several classifiers and see if you can build a great spam classifier,\n",
      "\n",
      "with both high recall and high precision.\n",
      "\n",
      "Solutions  to  these  exercises  are  available  in  the  online  Jupyter  notebooks  at  https://\n",
      "github.com/ageron/handson-ml2.\n",
      "\n",
      "Exercises \n",
      "\n",
      "| \n",
      "\n",
      "111\n",
      "\n",
      "\f\fCHAPTER 4\n",
      "Training Models\n",
      "\n",
      "With  Early  Release  ebooks,  you  get  books  in  their  earliest  form—\n",
      "the author’s raw and unedited content as he or she writes—so you\n",
      "can  take  advantage  of  these  technologies  long  before  the  official\n",
      "release of these titles. The following will be Chapter 4 in the final\n",
      "release of the book.\n",
      "\n",
      "So far we have treated Machine Learning models and their training algorithms mostly\n",
      "like black boxes. If you went through some of the exercises in the previous chapters,\n",
      "you may have been surprised by how much you can get done without knowing any‐\n",
      "thing about what’s under the hood: you optimized a regression system, you improved\n",
      "a  digit  image  classifier,  and  you  even  built  a  spam  classifier  from  scratch—all  this\n",
      "without knowing how they actually work. Indeed, in many situations you don’t really\n",
      "need to know the implementation details.\n",
      "\n",
      "However,  having  a  good  understanding  of  how  things  work  can  help  you  quickly\n",
      "home in on the appropriate model, the right training algorithm to use, and a good set\n",
      "of hyperparameters for your task. Understanding what’s under the hood will also help\n",
      "you debug issues and perform error analysis more efficiently. Lastly, most of the top‐\n",
      "ics discussed in this chapter will be essential in understanding, building, and training\n",
      "neural networks (discussed in Part II of this book).\n",
      "\n",
      "In  this  chapter,  we  will  start  by  looking  at  the  Linear  Regression  model,  one  of  the\n",
      "simplest models there is. We will discuss two very different ways to train it:\n",
      "\n",
      "• Using a direct “closed-form” equation that directly computes the model parame‐\n",
      "ters  that  best  fit  the  model  to  the  training  set  (i.e.,  the  model  parameters  that\n",
      "minimize the cost function over the training set).\n",
      "\n",
      "113\n",
      "\n",
      "\f• Using  an  iterative  optimization  approach,  called  Gradient  Descent  (GD),  that\n",
      "gradually  tweaks  the  model  parameters  to  minimize  the  cost  function  over  the\n",
      "training  set,  eventually  converging  to  the  same  set  of  parameters  as  the  first\n",
      "method. We will look at a few variants of Gradient Descent that we will use again\n",
      "and again when we study neural networks in Part II: Batch GD, Mini-batch GD,\n",
      "and Stochastic GD.\n",
      "\n",
      "Next we will look at Polynomial Regression, a more complex model that can fit non‐\n",
      "linear  datasets.  Since  this  model  has  more  parameters  than  Linear  Regression,  it  is\n",
      "more prone to overfitting the training data, so we will look at how to detect whether\n",
      "or not this is the case, using learning curves, and then we will look at several regulari‐\n",
      "zation techniques that can reduce the risk of overfitting the training set.\n",
      "\n",
      "Finally,  we  will  look  at  two  more  models  that  are  commonly  used  for  classification\n",
      "tasks: Logistic Regression and Softmax Regression.\n",
      "\n",
      "There will be quite a few math equations in this chapter, using basic\n",
      "notions  of  linear  algebra  and  calculus.  To  understand  these  equa‐\n",
      "tions, you will need to know what vectors and matrices are, how to\n",
      "transpose them, multiply them, and inverse them, and what partial\n",
      "derivatives are. If you are unfamiliar with these concepts, please go\n",
      "through the linear algebra and calculus introductory tutorials avail‐\n",
      "able as Jupyter notebooks in the online supplemental material. For\n",
      "those  who  are  truly  allergic  to  mathematics,  you  should  still  go\n",
      "through this chapter and simply skip the equations; hopefully, the\n",
      "text will be sufficient to help you understand most of the concepts.\n",
      "\n",
      "Linear Regression\n",
      "In Chapter 1, we looked at a simple regression model of life satisfaction: life_satisfac‐\n",
      "tion = θ0 + θ1 × GDP_per_capita.\n",
      "\n",
      "This model is just a linear function of the input feature GDP_per_capita. θ0 and θ1 are\n",
      "the model’s parameters.\n",
      "\n",
      "More generally, a linear model makes a prediction by simply computing a weighted\n",
      "sum of the input features, plus a constant called the bias term (also called the intercept\n",
      "term), as shown in Equation 4-1.\n",
      "\n",
      "Equation 4-1. Linear Regression model prediction\n",
      "y = θ0 + θ1x1 + θ2x2 + ⋯ + θnxn\n",
      "\n",
      "• ŷ is the predicted value.\n",
      "\n",
      "114 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Training Models\n",
      "\n",
      "\f• n is the number of features.\n",
      "• xi is the ith feature value.\n",
      "• θj is the jth model parameter (including the bias term θ0 and the feature weights\n",
      "\n",
      "θ1, θ2, ⋯, θn).\n",
      "\n",
      "This can be written much more concisely using a vectorized form, as shown in Equa‐\n",
      "tion 4-2.\n",
      "\n",
      "Equation 4-2. Linear Regression model prediction (vectorized form)\n",
      "\n",
      "y = hθ x = θ · x\n",
      "\n",
      "• θ  is  the  model’s  parameter  vector,  containing  the  bias  term  θ0  and  the  feature\n",
      "\n",
      "weights θ1 to θn.\n",
      "\n",
      "• x is the instance’s feature vector, containing x0 to xn, with x0 always equal to 1.\n",
      "• θ  ·  x  is  the  dot  product  of  the  vectors  θ  and  x,  which  is  of  course  equal  to\n",
      "\n",
      "θ0x0 + θ1x1 + θ2x2 + ⋯ + θnxn.\n",
      "\n",
      "• hθ is the hypothesis function, using the model parameters θ.\n",
      "\n",
      "In Machine Learning, vectors are often represented as column vec‐\n",
      "tors, which are 2D arrays with a single column. If θ and x are col‐\n",
      "umn  vectors,  then  the  prediction  is:  y = θTx,  where  θT  is  the\n",
      "transpose of θ (a row vector instead of a column vector) and θTx is\n",
      "the matrix multiplication of θT and x. It is of course the same pre‐\n",
      "diction,  except  it  is  now  represented  as  a  single  cell  matrix  rather\n",
      "than a scalar value. In this book we will use this notation to avoid\n",
      "switching between dot products and matrix multiplications.\n",
      "\n",
      "Okay, that’s the Linear Regression model, so now how do we train it? Well, recall that\n",
      "training a model means setting its parameters so that the model best fits the training\n",
      "set. For this purpose, we first need a measure of how well (or poorly) the model fits\n",
      "the training data. In Chapter 2 we saw that the most common performance measure\n",
      "of a regression model is the Root Mean Square Error (RMSE) (Equation 2-1). There‐\n",
      "fore, to train a Linear Regression model, you need to find the value of θ that minimi‐\n",
      "zes  the  RMSE.  In  practice,  it  is  simpler  to  minimize  the  Mean  Square  Error  (MSE)\n",
      "\n",
      "Linear Regression \n",
      "\n",
      "| \n",
      "\n",
      "115\n",
      "\n",
      "\fthan  the  RMSE,  and  it  leads  to  the  same  result  (because  the  value  that  minimizes  a\n",
      "function also minimizes its square root).1\n",
      "\n",
      "The MSE of a Linear Regression hypothesis hθ on a training set X is calculated using\n",
      "Equation 4-3.\n",
      "\n",
      "Equation 4-3. MSE cost function for a Linear Regression model\n",
      "\n",
      "MSE X, hθ =\n",
      "\n",
      "m\n",
      "\n",
      "1\n",
      "m ∑\n",
      "\n",
      "i = 1\n",
      "\n",
      "θTx i − y i 2\n",
      "\n",
      "Most  of  these  notations  were  presented  in  Chapter  2  (see  “Notations”  on  page  43).\n",
      "The only difference is that we write hθ instead of just h in order to make it clear that\n",
      "the  model  is  parametrized  by  the  vector  θ.  To  simplify  notations,  we  will  just  write\n",
      "MSE(θ) instead of MSE(X, hθ).\n",
      "\n",
      "The Normal Equation\n",
      "To find the value of θ that minimizes the cost function, there is a closed-form solution\n",
      "—in other words, a mathematical equation that gives the result directly. This is called\n",
      "the Normal Equation (Equation 4-4).2\n",
      "\n",
      "Equation 4-4. Normal Equation\n",
      "\n",
      "θ = XTX\n",
      "\n",
      "−1\n",
      "\n",
      "  XT   y\n",
      "\n",
      "• θ is the value of θ that minimizes the cost function.\n",
      "• y is the vector of target values containing y(1) to y(m).\n",
      "\n",
      "Let’s generate some linear-looking data to test this equation on (Figure 4-1):\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "X = 2 * np.random.rand(100, 1)\n",
      "y = 4 + 3 * X + np.random.randn(100, 1)\n",
      "\n",
      "1 It is often the case that a learning algorithm will try to optimize a different function than the performance\n",
      "\n",
      "measure used to evaluate the final model. This is generally because that function is easier to compute, because\n",
      "it has useful differentiation properties that the performance measure lacks, or because we want to constrain\n",
      "the model during training, as we will see when we discuss regularization.\n",
      "\n",
      "2 The demonstration that this returns the value of θ that minimizes the cost function is outside the scope of this\n",
      "\n",
      "book.\n",
      "\n",
      "116 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Training Models\n",
      "\n",
      "\fFigure 4-1. Randomly generated linear dataset\n",
      "\n",
      "Now let’s compute θ using the Normal Equation. We will use the inv() function from\n",
      "NumPy’s Linear Algebra module (np.linalg) to compute the inverse of a matrix, and\n",
      "the dot() method for matrix multiplication:\n",
      "\n",
      "X_b = np.c_[np.ones((100, 1)), X]  # add x0 = 1 to each instance\n",
      "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
      "\n",
      "The actual function that we used to generate the data is y = 4 + 3x1 + Gaussian noise.\n",
      "Let’s see what the equation found:\n",
      "\n",
      ">>> theta_best\n",
      "array([[4.21509616],\n",
      "       [2.77011339]])\n",
      "\n",
      "We would have hoped for θ0 = 4 and θ1 = 3 instead of θ0 = 4.215 and θ1 = 2.770. Close\n",
      "enough, but the noise made it impossible to recover the exact parameters of the origi‐\n",
      "nal function.\n",
      "\n",
      "Now you can make predictions using θ:\n",
      "\n",
      ">>> X_new = np.array([[0], [2]])\n",
      ">>> X_new_b = np.c_[np.ones((2, 1)), X_new] # add x0 = 1 to each instance\n",
      ">>> y_predict = X_new_b.dot(theta_best)\n",
      ">>> y_predict\n",
      "array([[4.21509616],\n",
      "       [9.75532293]])\n",
      "\n",
      "Let’s plot this model’s predictions (Figure 4-2):\n",
      "\n",
      "plt.plot(X_new, y_predict, \"r-\")\n",
      "plt.plot(X, y, \"b.\")\n",
      "\n",
      "Linear Regression \n",
      "\n",
      "| \n",
      "\n",
      "117\n",
      "\n",
      "\fplt.axis([0, 2, 0, 15])\n",
      "plt.show()\n",
      "\n",
      "Figure 4-2. Linear Regression model predictions\n",
      "\n",
      "Performing linear regression using Scikit-Learn is quite simple:3\n",
      "\n",
      ">>> from sklearn.linear_model import LinearRegression\n",
      ">>> lin_reg = LinearRegression()\n",
      ">>> lin_reg.fit(X, y)\n",
      ">>> lin_reg.intercept_, lin_reg.coef_\n",
      "(array([4.21509616]), array([[2.77011339]]))\n",
      ">>> lin_reg.predict(X_new)\n",
      "array([[4.21509616],\n",
      "       [9.75532293]])\n",
      "\n",
      "The  LinearRegression  class  is  based  on  the  scipy.linalg.lstsq()  function  (the\n",
      "name stands for “least squares”), which you could call directly:\n",
      "\n",
      ">>> theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\n",
      ">>> theta_best_svd\n",
      "array([[4.21509616],\n",
      "       [2.77011339]])\n",
      "\n",
      "This function computes θ = X+y, where + is the pseudoinverse of X (specifically the\n",
      "Moore-Penrose  inverse).  You  can  use  np.linalg.pinv()  to  compute  the  pseudoin‐\n",
      "verse directly:\n",
      "\n",
      ">>> np.linalg.pinv(X_b).dot(y)\n",
      "array([[4.21509616],\n",
      "       [2.77011339]])\n",
      "\n",
      "3 Note that Scikit-Learn separates the bias term (intercept_) from the feature weights (coef_).\n",
      "\n",
      "118 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Training Models\n",
      "\n",
      "\fThe pseudoinverse itself is computed using a standard matrix factorization technique \n",
      "called  Singular  Value  Decomposition  (SVD)  that  can  decompose  the  training  set\n",
      "into  the  matrix  multiplication  of  three  matrices  U  Σ  VT  (see\n",
      "matrix  X \n",
      "numpy.linalg.svd()). The pseudoinverse is computed as X+ = VΣ+UT. To compute\n",
      "the  matrix  Σ+,  the  algorithm  takes  Σ  and  sets  to  zero  all  values  smaller  than  a  tiny\n",
      "threshold value, then it replaces all the non-zero values with their inverse, and finally\n",
      "it transposes the resulting matrix. This approach is more efficient than computing the\n",
      "Normal Equation, plus it handles edge cases nicely: indeed, the Normal Equation may\n",
      "not work if the matrix XTX is not invertible (i.e., singular), such as if m < n or if some\n",
      "features are redundant, but the pseudoinverse is always defined.\n",
      "\n",
      "Computational Complexity\n",
      "The  Normal  Equation  computes  the  inverse  of  XT  X,  which  is  an  (n  +  1)  ×  (n  +  1)\n",
      "matrix (where n is the number of features). The computational complexity of inverting\n",
      "such a matrix is typically about O(n2.4) to O(n3) (depending on the implementation).\n",
      "In other words, if you double the number of features, you multiply the computation\n",
      "time by roughly 22.4 = 5.3 to 23 = 8.\n",
      "\n",
      "The SVD approach used by Scikit-Learn’s LinearRegression class is about O(n2). If\n",
      "you double the number of features, you multiply the computation time by roughly 4.\n",
      "\n",
      "Both  the  Normal  Equation  and  the  SVD  approach  get  very  slow\n",
      "when  the  number  of  features  grows  large  (e.g.,  100,000).  On  the\n",
      "positive side, both are linear with regards to the number of instan‐\n",
      "ces in the training set (they are O(m)), so they handle large training\n",
      "sets efficiently, provided they can fit in memory.\n",
      "\n",
      "Also, once you have trained your Linear Regression model (using the Normal Equa‐\n",
      "tion or any other algorithm), predictions are very fast: the computational complexity\n",
      "is linear with regards to both the number of instances you want to make predictions\n",
      "on and the number of features. In other words, making predictions on twice as many\n",
      "instances (or twice as many features) will just take roughly twice as much time.\n",
      "\n",
      "Now  we  will  look  at  very  different  ways  to  train  a  Linear  Regression  model,  better\n",
      "suited  for  cases  where  there  are  a  large  number  of  features,  or  too  many  training\n",
      "instances to fit in memory.\n",
      "\n",
      "Gradient Descent\n",
      "Gradient Descent is a very generic optimization algorithm capable of finding optimal\n",
      "solutions  to  a  wide  range  of  problems.  The  general  idea  of  Gradient  Descent  is  to\n",
      "tweak parameters iteratively in order to minimize a cost function.\n",
      "\n",
      "Gradient Descent \n",
      "\n",
      "| \n",
      "\n",
      "119\n",
      "\n",
      "\fSuppose you are lost in the mountains in a dense fog; you can only feel the slope of\n",
      "the ground below your feet. A good strategy to get to the bottom of the valley quickly\n",
      "is to go downhill in the direction of the steepest slope. This is exactly what Gradient\n",
      "Descent does: it measures the local gradient of the error function with regards to the \n",
      "parameter vector θ, and it goes in the direction of descending gradient. Once the gra‐\n",
      "dient is zero, you have reached a minimum!\n",
      "\n",
      "Concretely, you start by filling θ with random values (this is called random initializa‐\n",
      "tion),  and  then  you  improve  it  gradually,  taking  one  baby  step  at  a  time,  each  step\n",
      "attempting to decrease the cost function (e.g., the MSE), until the algorithm converges\n",
      "to a minimum (see Figure 4-3).\n",
      "\n",
      "Figure 4-3. Gradient Descent\n",
      "\n",
      "An important parameter in Gradient Descent is the size of the steps, determined by \n",
      "the learning rate hyperparameter. If the learning rate is too small, then the algorithm\n",
      "will have to go through many iterations to converge, which will take a long time (see\n",
      "Figure 4-4).\n",
      "\n",
      "120 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Training Models\n",
      "\n",
      "\fFigure 4-4. Learning rate too small\n",
      "\n",
      "On the other hand, if the learning rate is too high, you might jump across the valley\n",
      "and  end  up  on  the  other  side,  possibly  even  higher  up  than  you  were  before.  This\n",
      "might make the algorithm diverge, with larger and larger values, failing to find a good\n",
      "solution (see Figure 4-5).\n",
      "\n",
      "Figure 4-5. Learning rate too large\n",
      "\n",
      "Finally, not all cost functions look like nice regular bowls. There may be holes, ridges,\n",
      "plateaus, and all sorts of irregular terrains, making convergence to the minimum very\n",
      "difficult. Figure 4-6 shows the two main challenges with Gradient Descent: if the ran‐\n",
      "dom initialization starts the algorithm on the left, then it will converge to a local mini‐\n",
      "mum, which is not as good as the global minimum. If it starts on the right, then it will\n",
      "take  a  very  long  time  to  cross  the  plateau,  and  if  you  stop  too  early  you  will  never\n",
      "reach the global minimum.\n",
      "\n",
      "Gradient Descent \n",
      "\n",
      "| \n",
      "\n",
      "121\n",
      "\n",
      "\fFigure 4-6. Gradient Descent pitfalls\n",
      "\n",
      "Fortunately,  the  MSE  cost  function  for  a  Linear  Regression  model  happens  to  be  a\n",
      "convex function, which means that if you pick any two points on the curve, the line\n",
      "segment  joining  them  never  crosses  the  curve.  This  implies  that  there  are  no  local\n",
      "minima, just one global minimum. It is also a continuous function with a slope that\n",
      "never changes abruptly.4 These two facts have a great consequence: Gradient Descent\n",
      "is  guaranteed  to  approach  arbitrarily  close  the  global  minimum  (if  you  wait  long\n",
      "enough and if the learning rate is not too high).\n",
      "\n",
      "In fact, the cost function has the shape of a bowl, but it can be an elongated bowl if\n",
      "the features have very different scales. Figure 4-7 shows Gradient Descent on a train‐\n",
      "ing set where features 1 and 2 have the same scale (on the left), and on a training set\n",
      "where feature 1 has much smaller values than feature 2 (on the right).5\n",
      "\n",
      "Figure 4-7. Gradient Descent with and without feature scaling\n",
      "\n",
      "4 Technically speaking, its derivative is Lipschitz continuous.\n",
      "\n",
      "5 Since feature 1 is smaller, it takes a larger change in θ1 to affect the cost function, which is why the bowl is\n",
      "\n",
      "elongated along the θ1 axis.\n",
      "\n",
      "122 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Training Models\n",
      "\n",
      "\fAs you can see, on the left the Gradient Descent algorithm goes straight toward the\n",
      "minimum, thereby reaching it quickly, whereas on the right it first goes in a direction\n",
      "almost  orthogonal  to  the  direction  of  the  global  minimum,  and  it  ends  with  a  long\n",
      "march  down  an  almost  flat  valley.  It  will  eventually  reach  the  minimum,  but  it  will\n",
      "take a long time.\n",
      "\n",
      "When using Gradient Descent, you should ensure that all features\n",
      "have  a  similar  scale  (e.g.,  using  Scikit-Learn’s  StandardScaler\n",
      "class), or else it will take much longer to converge.\n",
      "\n",
      "This  diagram  also  illustrates  the  fact  that  training  a  model  means  searching  for  a\n",
      "combination of model parameters that minimizes a cost function (over the training\n",
      "set). It is a search in the model’s parameter space: the more parameters a model has,\n",
      "the more dimensions this space has, and the harder the search is: searching for a nee‐\n",
      "dle in a 300-dimensional haystack is much trickier than in three dimensions. Fortu‐\n",
      "nately, since the cost function is convex in the case of Linear Regression, the needle is\n",
      "simply at the bottom of the bowl.\n",
      "\n",
      "Batch Gradient Descent\n",
      "To implement Gradient Descent, you need to compute the gradient of the cost func‐\n",
      "tion with regards to each model parameter θj. In other words, you need to calculate\n",
      "how much the cost function will change if you change θj just a little bit. This is called \n",
      "a partial derivative. It is like asking “what is the slope of the mountain under my feet\n",
      "if I face east?” and then asking the same question facing north (and so on for all other\n",
      "dimensions, if you can imagine a universe with more than three dimensions). Equa‐\n",
      "tion 4-5 computes the partial derivative of the cost function with regards to parame‐\n",
      "ter θj, noted  ∂\n",
      "∂θ\n",
      "\n",
      " MSE(θ).\n",
      "\n",
      "j\n",
      "\n",
      "Equation 4-5. Partial derivatives of the cost function\n",
      "\n",
      "∂\n",
      "∂θ j\n",
      "\n",
      "MSE θ =\n",
      "\n",
      "m\n",
      "\n",
      "2\n",
      "m ∑\n",
      "\n",
      "i = 1\n",
      "\n",
      "i\n",
      "θTx i − y i x j\n",
      "\n",
      "Instead of computing these partial derivatives individually, you can use Equation 4-6\n",
      "to compute them all in one go. The gradient vector, noted ∇θMSE(θ), contains all the\n",
      "partial derivatives of the cost function (one for each model parameter).\n",
      "\n",
      "Gradient Descent \n",
      "\n",
      "| \n",
      "\n",
      "123\n",
      "\n",
      "\fEquation 4-6. Gradient vector of the cost function\n",
      "\n",
      "∇θ MSE θ =\n",
      "\n",
      "∂\n",
      "∂θ0\n",
      "∂\n",
      "∂θ1\n",
      "\n",
      "∂\n",
      "∂θn\n",
      "\n",
      "MSE θ\n",
      "\n",
      "MSE θ\n",
      "\n",
      "⋮\n",
      "\n",
      "MSE θ\n",
      "\n",
      "=\n",
      "\n",
      "2\n",
      "m\n",
      "\n",
      "XT Xθ − y\n",
      "\n",
      "Notice that this formula involves calculations over the full training\n",
      "set X, at each Gradient Descent step! This is why the algorithm is\n",
      "called  Batch  Gradient  Descent:  it  uses  the  whole  batch  of  training\n",
      "data  at  every  step  (actually,  Full  Gradient  Descent  would  probably\n",
      "be a better name). As a result it is terribly slow on very large train‐\n",
      "ing sets (but we will see much faster Gradient Descent algorithms\n",
      "shortly). However, Gradient Descent scales well with the number of\n",
      "features;  training  a  Linear  Regression  model  when  there  are  hun‐\n",
      "dreds  of  thousands  of  features  is  much  faster  using  Gradient\n",
      "Descent than using the Normal Equation or SVD decomposition.\n",
      "\n",
      "Once you have the gradient vector, which points uphill, just go in the opposite direc‐\n",
      "tion  to  go  downhill.  This  means  subtracting  ∇θMSE(θ)  from  θ.  This  is  where  the \n",
      "learning rate η comes into play:6 multiply the gradient vector by η to determine the\n",
      "size of the downhill step (Equation 4-7).\n",
      "\n",
      "Equation 4-7. Gradient Descent step\n",
      "θ next step = θ − η ∇θ MSE θ\n",
      "\n",
      "Let’s look at a quick implementation of this algorithm:\n",
      "\n",
      "eta = 0.1  # learning rate\n",
      "n_iterations = 1000\n",
      "m = 100\n",
      "\n",
      "theta = np.random.randn(2,1)  # random initialization\n",
      "\n",
      "for iteration in range(n_iterations):\n",
      "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
      "    theta = theta - eta * gradients\n",
      "\n",
      "6 Eta (η) is the 7th letter of the Greek alphabet.\n",
      "\n",
      "124 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Training Models\n",
      "\n",
      "\fThat wasn’t too hard! Let’s look at the resulting theta:\n",
      "\n",
      ">>> theta\n",
      "array([[4.21509616],\n",
      "       [2.77011339]])\n",
      "\n",
      "Hey, that’s exactly what the Normal Equation found! Gradient Descent worked per‐\n",
      "fectly.  But  what  if  you  had  used  a  different  learning  rate  eta?  Figure  4-8  shows  the\n",
      "first 10 steps of Gradient Descent using three different learning rates (the dashed line\n",
      "represents the starting point).\n",
      "\n",
      "Figure 4-8. Gradient Descent with various learning rates\n",
      "\n",
      "On the left, the learning rate is too low: the algorithm will eventually reach the solu‐\n",
      "tion, but it will take a long time. In the middle, the learning rate looks pretty good: in\n",
      "just a few iterations, it has already converged to the solution. On the right, the learn‐\n",
      "ing  rate  is  too  high:  the  algorithm  diverges,  jumping  all  over  the  place  and  actually\n",
      "getting further and further away from the solution at every step.\n",
      "\n",
      "To find a good learning rate, you can use grid search (see Chapter 2). However, you\n",
      "may want to limit the number of iterations so that grid search can eliminate models\n",
      "that take too long to converge.\n",
      "\n",
      "You may wonder how to set the number of iterations. If it is too low, you will still be\n",
      "far away from the optimal solution when the algorithm stops, but if it is too high, you\n",
      "will waste time while the model parameters do not change anymore. A simple solu‐\n",
      "tion is to set a very large number of iterations but to interrupt the algorithm when the\n",
      "gradient  vector  becomes  tiny—that  is,  when  its  norm  becomes  smaller  than  a  tiny\n",
      "number ϵ  (called  the  tolerance)—because  this  happens  when  Gradient  Descent  has\n",
      "(almost) reached the minimum.\n",
      "\n",
      "Gradient Descent \n",
      "\n",
      "| \n",
      "\n",
      "125\n",
      "\n",
      "\fConvergence Rate\n",
      "When  the  cost  function  is  convex  and  its  slope  does  not  change  abruptly  (as  is  the\n",
      "case  for  the  MSE  cost  function),  Batch  Gradient  Descent  with  a  fixed  learning  rate\n",
      "will eventually converge to the optimal solution, but you may have to wait a while: it\n",
      "can take O(1/ϵ) iterations to reach the optimum within a range of ϵ depending on the\n",
      "shape of the cost function. If you divide the tolerance by 10 to have a more precise\n",
      "solution, then the algorithm may have to run about 10 times longer.\n",
      "\n",
      "Stochastic Gradient Descent\n",
      "The  main  problem  with  Batch  Gradient  Descent  is  the  fact  that  it  uses  the  whole\n",
      "training  set  to  compute  the  gradients  at  every  step,  which  makes  it  very  slow  when\n",
      "the  training  set  is  large.  At  the  opposite  extreme,  Stochastic  Gradient  Descent  just\n",
      "picks a random instance in the training set at every step and computes the gradients\n",
      "based only on that single instance. Obviously this makes the algorithm much faster\n",
      "since it has very little data to manipulate at every iteration. It also makes it possible to\n",
      "train  on  huge  training  sets,  since  only  one  instance  needs  to  be  in  memory  at  each\n",
      "iteration (SGD can be implemented as an out-of-core algorithm.7)\n",
      "\n",
      "On the other hand, due to its stochastic (i.e., random) nature, this algorithm is much\n",
      "less regular than Batch Gradient Descent: instead of gently decreasing until it reaches\n",
      "the minimum, the cost function will bounce up and down, decreasing only on aver‐\n",
      "age. Over time it will end up very close to the minimum, but once it gets there it will\n",
      "continue to bounce around, never settling down (see Figure 4-9). So once the algo‐\n",
      "rithm stops, the final parameter values are good, but not optimal.\n",
      "\n",
      "Figure 4-9. Stochastic Gradient Descent\n",
      "\n",
      "7 Out-of-core algorithms are discussed in Chapter 1.\n",
      "\n",
      "126 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Training Models\n",
      "\n",
      "\fWhen the cost function is very irregular (as in Figure 4-6), this can actually help the\n",
      "algorithm  jump  out  of  local  minima,  so  Stochastic  Gradient  Descent  has  a  better\n",
      "chance of finding the global minimum than Batch Gradient Descent does.\n",
      "\n",
      "Therefore randomness is good to escape from local optima, but bad because it means\n",
      "that the algorithm can never settle at the minimum. One solution to this dilemma is\n",
      "to  gradually  reduce  the  learning  rate.  The  steps  start  out  large  (which  helps  make\n",
      "quick progress and escape local minima), then get smaller and smaller, allowing the\n",
      "algorithm to settle at the global minimum. This process is akin to simulated anneal‐\n",
      "ing, an algorithm inspired from the process of annealing in metallurgy where molten\n",
      "metal is slowly cooled down. The function that determines the learning rate at each\n",
      "iteration is called the learning schedule. If the learning rate is reduced too quickly, you\n",
      "may get stuck in a local minimum, or even end up frozen halfway to the minimum. If\n",
      "the  learning  rate  is  reduced  too  slowly,  you  may  jump  around  the  minimum  for  a\n",
      "long time and end up with a suboptimal solution if you halt training too early.\n",
      "\n",
      "This code implements Stochastic Gradient Descent using a simple learning schedule:\n",
      "\n",
      "n_epochs = 50\n",
      "t0, t1 = 5, 50  # learning schedule hyperparameters\n",
      "\n",
      "def learning_schedule(t):\n",
      "    return t0 / (t + t1)\n",
      "\n",
      "theta = np.random.randn(2,1)  # random initialization\n",
      "\n",
      "for epoch in range(n_epochs):\n",
      "    for i in range(m):\n",
      "        random_index = np.random.randint(m)\n",
      "        xi = X_b[random_index:random_index+1]\n",
      "        yi = y[random_index:random_index+1]\n",
      "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
      "        eta = learning_schedule(epoch * m + i)\n",
      "        theta = theta - eta * gradients\n",
      "\n",
      "By  convention  we  iterate  by  rounds  of  m  iterations;  each  round  is  called  an  epoch. \n",
      "While the Batch Gradient Descent code iterated 1,000 times through the whole train‐\n",
      "ing set, this code goes through the training set only 50 times and reaches a fairly good\n",
      "solution:\n",
      "\n",
      ">>> theta\n",
      "array([[4.21076011],\n",
      "       [2.74856079]])\n",
      "\n",
      "Figure 4-10 shows the first 20 steps of training (notice how irregular the steps are).\n",
      "\n",
      "Gradient Descent \n",
      "\n",
      "| \n",
      "\n",
      "127\n",
      "\n",
      "\fFigure 4-10. Stochastic Gradient Descent first 20 steps\n",
      "\n",
      "Note that since instances are picked randomly, some instances may be picked several\n",
      "times per epoch while others may not be picked at all. If you want to be sure that the\n",
      "algorithm goes through every instance at each epoch, another approach is to shuffle\n",
      "the training set (making sure to shuffle the input features and the labels jointly), then\n",
      "go through it instance by instance, then shuffle it again, and so on. However, this gen‐\n",
      "erally converges more slowly.\n",
      "\n",
      "When  using  Stochastic  Gradient  Descent,  the  training  instances\n",
      "must  be  independent  and  identically  distributed  (IID),  to  ensure\n",
      "that  the  parameters  get  pulled  towards  the  global  optimum,  on\n",
      "average. A simple way to ensure this is to shuffle the instances dur‐\n",
      "ing training (e.g., pick each instance randomly, or shuffle the train‐\n",
      "ing set at the beginning of each epoch). If you do not do this, for\n",
      "example if the instances are sorted by label, then SGD will start by\n",
      "optimizing for one label, then the next, and so on, and it will not\n",
      "settle close to the global minimum.\n",
      "\n",
      "To  perform  Linear  Regression  using  SGD  with  Scikit-Learn,  you  can  use  the  SGDRe\n",
      "gressor class, which defaults to optimizing the squared error cost function. The fol‐\n",
      "lowing code runs for maximum 1000 epochs (max_iter=1000) or until the loss drops\n",
      "by  less  than  1e-3  during  one  epoch  (tol=1e-3),  starting  with  a  learning  rate  of  0.1\n",
      "(eta0=0.1),  using  the  default  learning  schedule  (different  from  the  preceding  one),\n",
      "and it does not use any regularization (penalty=None; more details on this shortly):\n",
      "\n",
      "from sklearn.linear_model import SGDRegressor\n",
      "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)\n",
      "sgd_reg.fit(X, y.ravel())\n",
      "\n",
      "128 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Training Models\n",
      "\n",
      "\fOnce again, you find a solution quite close to the one returned by the Normal Equa‐\n",
      "tion:\n",
      "\n",
      ">>> sgd_reg.intercept_, sgd_reg.coef_\n",
      "(array([4.24365286]), array([2.8250878]))\n",
      "\n",
      "Mini-batch Gradient Descent\n",
      "The  last  Gradient  Descent  algorithm  we  will  look  at  is  called  Mini-batch  Gradient\n",
      "Descent. It is quite simple to understand once you know Batch and Stochastic Gradi‐\n",
      "ent Descent: at each step, instead of computing the gradients based on the full train‐\n",
      "ing set (as in Batch GD) or based on just one instance (as in Stochastic GD), Mini-\n",
      "batch  GD  computes  the  gradients  on  small  random  sets  of  instances  called  mini-\n",
      "batches.  The  main  advantage  of  Mini-batch  GD  over  Stochastic  GD  is  that  you  can\n",
      "get a performance boost from hardware optimization of matrix operations, especially\n",
      "when using GPUs.\n",
      "\n",
      "The algorithm’s progress in parameter space is less erratic than with SGD, especially\n",
      "with  fairly  large  mini-batches.  As  a  result,  Mini-batch  GD  will  end  up  walking\n",
      "around  a  bit  closer  to  the  minimum  than  SGD.  But,  on  the  other  hand,  it  may  be\n",
      "harder  for  it  to  escape  from  local  minima  (in  the  case  of  problems  that  suffer  from\n",
      "local  minima,  unlike  Linear  Regression  as  we  saw  earlier).  Figure  4-11  shows  the\n",
      "paths  taken  by  the  three  Gradient  Descent  algorithms  in  parameter  space  during\n",
      "training. They all end up near the minimum, but Batch GD’s path actually stops at the\n",
      "minimum,  while  both  Stochastic  GD  and  Mini-batch  GD  continue  to  walk  around.\n",
      "However, don’t forget that Batch GD takes a lot of time to take each step, and Stochas‐\n",
      "tic GD and Mini-batch GD would also reach the minimum if you used a good learn‐\n",
      "ing schedule.\n",
      "\n",
      "Figure 4-11. Gradient Descent paths in parameter space\n",
      "\n",
      "Gradient Descent \n",
      "\n",
      "| \n",
      "\n",
      "129\n",
      "\n",
      "\fLet’s compare the algorithms we’ve discussed so far for Linear Regression8 (recall that\n",
      "m is the number of training instances and n is the number of features); see Table 4-1.\n",
      "\n",
      "Table 4-1. Comparison of algorithms for Linear Regression\n",
      "\n",
      "Large m Out-of-core support\n",
      "\n",
      "Algorithm\n",
      "Normal Equation Fast\n",
      "\n",
      "SVD\n",
      "\n",
      "Batch GD\n",
      "\n",
      "Stochastic GD\n",
      "\n",
      "Mini-batch GD\n",
      "\n",
      "Fast\n",
      "\n",
      "Slow\n",
      "\n",
      "Fast\n",
      "\n",
      "Fast\n",
      "\n",
      "No\n",
      "\n",
      "No\n",
      "\n",
      "No\n",
      "\n",
      "Yes\n",
      "\n",
      "Yes\n",
      "\n",
      "Large n Hyperparams\n",
      "0\n",
      "Slow\n",
      "\n",
      "Scaling required Scikit-Learn\n",
      "No\n",
      "\n",
      "n/a\n",
      "\n",
      "Slow\n",
      "\n",
      "Fast\n",
      "\n",
      "Fast\n",
      "\n",
      "Fast\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "≥2\n",
      "\n",
      "≥2\n",
      "\n",
      "No\n",
      "\n",
      "Yes\n",
      "\n",
      "Yes\n",
      "\n",
      "Yes\n",
      "\n",
      "LinearRegression\n",
      "\n",
      "SGDRegressor\n",
      "\n",
      "SGDRegressor\n",
      "\n",
      "SGDRegressor\n",
      "\n",
      "There  is  almost  no  difference  after  training:  all  these  algorithms\n",
      "end up with very similar models and make predictions in exactly \n",
      "the same way.\n",
      "\n",
      "Polynomial Regression\n",
      "What if your data is actually more complex than a simple straight line? Surprisingly,\n",
      "you can actually use a linear model to fit nonlinear data. A simple way to do this is to\n",
      "add powers of each feature as new features, then train a linear model on this extended\n",
      "set of features. This technique is called Polynomial Regression.\n",
      "\n",
      "Let’s look at an example. First, let’s generate some nonlinear data, based on a simple\n",
      "quadratic equation9 (plus some noise; see Figure 4-12):\n",
      "\n",
      "m = 100\n",
      "X = 6 * np.random.rand(m, 1) - 3\n",
      "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
      "\n",
      "8 While the Normal Equation can only perform Linear Regression, the Gradient Descent algorithms can be\n",
      "\n",
      "used to train many other models, as we will see.\n",
      "9 A quadratic equation is of the form y = ax2 + bx + c.\n",
      "\n",
      "130 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Training Models\n",
      "\n",
      "\fFigure 4-12. Generated nonlinear and noisy dataset\n",
      "\n",
      "Clearly, a straight line will never fit this data properly. So let’s use Scikit-Learn’s Poly\n",
      "nomialFeatures class to transform our training data, adding the square (2nd-degree\n",
      "polynomial)  of  each  feature  in  the  training  set  as  new  features  (in  this  case  there  is\n",
      "just one feature):\n",
      "\n",
      ">>> from sklearn.preprocessing import PolynomialFeatures\n",
      ">>> poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
      ">>> X_poly = poly_features.fit_transform(X)\n",
      ">>> X[0]\n",
      "array([-0.75275929])\n",
      ">>> X_poly[0]\n",
      "array([-0.75275929, 0.56664654])\n",
      "\n",
      "X_poly now contains the original feature of X plus the square of this feature. Now you\n",
      "can fit a LinearRegression model to this extended training data (Figure 4-13):\n",
      "\n",
      ">>> lin_reg = LinearRegression()\n",
      ">>> lin_reg.fit(X_poly, y)\n",
      ">>> lin_reg.intercept_, lin_reg.coef_\n",
      "(array([1.78134581]), array([[0.93366893, 0.56456263]]))\n",
      "\n",
      "Polynomial Regression \n",
      "\n",
      "| \n",
      "\n",
      "131\n",
      "\n",
      "\fFigure 4-13. Polynomial Regression model predictions\n",
      "\n",
      "Not bad: the model estimates  y = 0 . 56x1\n",
      "function was y = 0 . 5x1\n",
      "\n",
      "2 + 1 . 0x1 + 2 . 0 + Gaussian noise.\n",
      "\n",
      "2 + 0 . 93x1 + 1 . 78 when in fact the original\n",
      "\n",
      "Note that when there are multiple features, Polynomial Regression is capable of find‐\n",
      "ing  relationships  between  features  (which  is  something  a  plain  Linear  Regression\n",
      "model  cannot  do).  This  is  made  possible  by  the  fact  that  PolynomialFeatures  also\n",
      "adds all combinations of features up to the given degree. For example, if there were\n",
      "two  features  a  and  b,  PolynomialFeatures  with  degree=3  would  not  only  add  the\n",
      "features a2, a3, b2, and b3, but also the combinations ab, a2b, and ab2.\n",
      "\n",
      "PolynomialFeatures(degree=d) transforms an array containing n\n",
      "\n",
      " features, where n! is the\n",
      "features into an array containing \n",
      "factorial of n, equal to 1 × 2 × 3 × ⋯ × n. Beware of the combinato‐\n",
      "rial explosion of the number of features!\n",
      "\n",
      "n + d !\n",
      "d! n!\n",
      "\n",
      "Learning Curves\n",
      "If  you  perform  high-degree  Polynomial  Regression,  you  will  likely  fit  the  training\n",
      "data much better than with plain Linear Regression. For example, Figure 4-14 applies\n",
      "a  300-degree  polynomial  model  to  the  preceding  training  data,  and  compares  the\n",
      "result  with  a  pure  linear  model  and  a  quadratic  model  (2nd-degree  polynomial).\n",
      "Notice how the 300-degree polynomial model wiggles around to get as close as possi‐\n",
      "ble to the training instances.\n",
      "\n",
      "132 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Training Models\n",
      "\n",
      "\fFigure 4-14. High-degree Polynomial Regression\n",
      "\n",
      "Of  course,  this  high-degree  Polynomial  Regression  model  is  severely  overfitting  the\n",
      "training data, while the linear model is underfitting it. The model that will generalize\n",
      "best in this case is the quadratic model. It makes sense since the data was generated\n",
      "using a quadratic model, but in general you won’t know what function generated the\n",
      "data, so how can you decide how complex your model should be? How can you tell\n",
      "that your model is overfitting or underfitting the data?\n",
      "\n",
      "In Chapter 2 you used cross-validation to get an estimate of a model’s generalization\n",
      "performance.  If  a  model  performs  well  on  the  training  data  but  generalizes  poorly\n",
      "according  to  the  cross-validation  metrics,  then  your  model  is  overfitting.  If  it  per‐\n",
      "forms poorly on both, then it is underfitting. This is one way to tell when a model is\n",
      "too simple or too complex.\n",
      "\n",
      "Another  way  is  to  look  at  the  learning  curves:  these  are  plots  of  the  model’s  perfor‐\n",
      "mance on the training set and the validation set as a function of the training set size\n",
      "(or the training iteration). To generate the plots, simply train the model several times\n",
      "on  different  sized  subsets  of  the  training  set.  The  following  code  defines  a  function\n",
      "that plots the learning curves of a model given some training data:\n",
      "\n",
      "from sklearn.metrics import mean_squared_error\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "def plot_learning_curves(model, X, y):\n",
      "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
      "    train_errors, val_errors = [], []\n",
      "    for m in range(1, len(X_train)):\n",
      "        model.fit(X_train[:m], y_train[:m])\n",
      "        y_train_predict = model.predict(X_train[:m])\n",
      "\n",
      "Learning Curves \n",
      "\n",
      "| \n",
      "\n",
      "133\n",
      "\n",
      "\f        y_val_predict = model.predict(X_val)\n",
      "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
      "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
      "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
      "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n",
      "\n",
      "Let’s look at the learning curves of the plain Linear Regression model (a straight line;\n",
      "Figure 4-15):\n",
      "\n",
      "lin_reg = LinearRegression()\n",
      "plot_learning_curves(lin_reg, X, y)\n",
      "\n",
      "Figure 4-15. Learning curves\n",
      "\n",
      "This deserves a bit of explanation. First, let’s look at the performance on the training\n",
      "data: when there are just one or two instances in the training set, the model can fit\n",
      "them perfectly, which is why the curve starts at zero. But as new instances are added\n",
      "to the training set, it becomes impossible for the model to fit the training data per‐\n",
      "fectly, both because the data is noisy and because it is not linear at all. So the error on\n",
      "the training data goes up until it reaches a plateau, at which point adding new instan‐\n",
      "ces to the training set doesn’t make the average error much better or worse. Now let’s\n",
      "look  at  the  performance  of  the  model  on  the  validation  data.  When  the  model  is\n",
      "trained on very few training instances, it is incapable of generalizing properly, which\n",
      "is  why  the  validation  error  is  initially  quite  big.  Then  as  the  model  is  shown  more\n",
      "training examples, it learns and thus the validation error slowly goes down. However,\n",
      "once again a straight line cannot do a good job modeling the data, so the error ends\n",
      "up at a plateau, very close to the other curve.\n",
      "\n",
      "These learning curves are typical of an underfitting model. Both curves have reached\n",
      "a plateau; they are close and fairly high.\n",
      "\n",
      "134 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Training Models\n",
      "\n",
      "\fIf your model is underfitting the training data, adding more train‐\n",
      "ing examples will not help. You need to use a more complex model\n",
      "or come up with better features.\n",
      "\n",
      "Now let’s look at the learning curves of a 10th-degree polynomial model on the same\n",
      "data (Figure 4-16):\n",
      "\n",
      "from sklearn.pipeline import Pipeline\n",
      "\n",
      "polynomial_regression = Pipeline([\n",
      "        (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\n",
      "        (\"lin_reg\", LinearRegression()),\n",
      "    ])\n",
      "\n",
      "plot_learning_curves(polynomial_regression, X, y)\n",
      "\n",
      "These learning curves look a bit like the previous ones, but there are two very impor‐\n",
      "tant differences:\n",
      "\n",
      "• The  error  on  the  training  data  is  much  lower  than  with  the  Linear  Regression\n",
      "\n",
      "model.\n",
      "\n",
      "• There is a gap between the curves. This means that the model performs signifi‐\n",
      "cantly better on the training data than on the validation data, which is the hall‐\n",
      "mark  of  an  overfitting  model.  However,  if  you  used  a  much  larger  training  set,\n",
      "the two curves would continue to get closer.\n",
      "\n",
      "Figure 4-16. Learning curves for the polynomial model\n",
      "\n",
      "Learning Curves \n",
      "\n",
      "| \n",
      "\n",
      "135\n",
      "\n",
      "\fOne way to improve an overfitting model is to feed it more training\n",
      "data until the validation error reaches the training error.\n",
      "\n",
      "The Bias/Variance Tradeoff\n",
      "An  important  theoretical  result  of  statistics  and  Machine  Learning  is  the  fact  that  a\n",
      "model’s  generalization  error  can  be  expressed  as  the  sum  of  three  very  different\n",
      "errors:\n",
      "\n",
      "Bias\n",
      "\n",
      "This part of the generalization error is due to wrong assumptions, such as assum‐\n",
      "ing that the data is linear when it is actually quadratic. A high-bias model is most\n",
      "likely to underfit the training data.10\n",
      "\n",
      "Variance\n",
      "\n",
      "This  part  is  due  to  the  model’s  excessive  sensitivity  to  small  variations  in  the\n",
      "training data. A model with many degrees of freedom (such as a high-degree pol‐\n",
      "ynomial  model)  is  likely  to  have  high  variance,  and  thus  to  overfit  the  training\n",
      "data.\n",
      "\n",
      "Irreducible error\n",
      "\n",
      "This  part  is  due  to  the  noisiness  of  the  data  itself.  The  only  way  to  reduce  this\n",
      "part of the error is to clean up the data (e.g., fix the data sources, such as broken\n",
      "sensors, or detect and remove outliers).\n",
      "\n",
      "Increasing a model’s complexity will typically increase its variance and reduce its bias.\n",
      "Conversely, reducing a model’s complexity increases its bias and reduces its variance. \n",
      "This is why it is called a tradeoff.\n",
      "\n",
      "Regularized Linear Models\n",
      "As we saw in Chapters 1 and 2, a good way to reduce overfitting is to regularize the\n",
      "model (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be\n",
      "for it to overfit the data. For example, a simple way to regularize a polynomial model\n",
      "is to reduce the number of polynomial degrees.\n",
      "\n",
      "For a linear model, regularization is typically achieved by constraining the weights of\n",
      "the model. We will now look at Ridge Regression, Lasso Regression, and Elastic Net,\n",
      "which implement three different ways to constrain the weights.\n",
      "\n",
      "10 This notion of bias is not to be confused with the bias term of linear models.\n",
      "\n",
      "136 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Training Models\n",
      "\n",
      "\fRidge Regression\n",
      "Ridge Regression (also called Tikhonov regularization) is a regularized version of Lin‐\n",
      "2 is added to the cost function. \n",
      "ear Regression: a regularization term equal to α∑i = 1\n",
      "This  forces  the  learning  algorithm  to  not  only  fit  the  data  but  also  keep  the  model\n",
      "weights as small as possible. Note that the regularization term should only be added\n",
      "to the cost function during training. Once the model is trained, you want to evaluate\n",
      "the model’s performance using the unregularized performance measure.\n",
      "\n",
      "θi\n",
      "\n",
      "n\n",
      "\n",
      "It is quite common for the cost function used during training to be\n",
      "different  from  the  performance  measure  used  for  testing.  Apart\n",
      "from regularization, another reason why they might be different is\n",
      "that  a  good  training  cost  function  should  have  optimization-\n",
      "friendly derivatives, while the performance measure used for test‐\n",
      "ing  should  be  as  close  as  possible  to  the  final  objective.  A  good\n",
      "example of this is a classifier trained using a cost function such as\n",
      "the log loss (discussed in a moment) but evaluated using precision/\n",
      "recall.\n",
      "\n",
      "The hyperparameter α controls how much you want to regularize the model. If α = 0\n",
      "then Ridge Regression is just Linear Regression. If α is very large, then all weights end\n",
      "up very close to zero and the result is a flat line going through the data’s mean. Equa‐\n",
      "tion 4-8 presents the Ridge Regression cost function.11\n",
      "\n",
      "Equation 4-8. Ridge Regression cost function\n",
      "\n",
      "J θ = MSE θ + α\n",
      "\n",
      "1\n",
      "n\n",
      "2 ∑i = 1\n",
      "\n",
      "2\n",
      "\n",
      "θi\n",
      "\n",
      "Note  that  the  bias  term  θ0  is  not  regularized  (the  sum  starts  at  i  =  1,  not  0).  If  we\n",
      "define  w  as  the  vector  of  feature  weights  (θ1  to  θn),  then  the  regularization  term  is\n",
      "simply equal to ½(∥ w ∥2)2, where ∥ w ∥2 represents the ℓ2 norm of the weight vector.12\n",
      "For Gradient Descent, just add αw to the MSE gradient vector (Equation 4-6).\n",
      "\n",
      "It  is  important  to  scale  the  data  (e.g.,  using  a  StandardScaler) \n",
      "before performing Ridge Regression, as it is sensitive to the scale of\n",
      "the input features. This is true of most regularized models.\n",
      "\n",
      "11 It is common to use the notation J(θ) for cost functions that don’t have a short name; we will often use this\n",
      "notation throughout the rest of this book. The context will make it clear which cost function is being dis‐\n",
      "cussed.\n",
      "\n",
      "12 Norms are discussed in Chapter 2.\n",
      "\n",
      "Regularized Linear Models \n",
      "\n",
      "| \n",
      "\n",
      "137\n",
      "\n",
      "\fFigure 4-17 shows several Ridge models trained on some linear data using different α\n",
      "value. On the left, plain Ridge models are used, leading to linear predictions. On the\n",
      "right,  the  data  is  first  expanded  using  PolynomialFeatures(degree=10),  then  it  is\n",
      "scaled using a StandardScaler, and finally the Ridge models are applied to the result‐\n",
      "ing  features:  this  is  Polynomial  Regression  with  Ridge  regularization.  Note  how\n",
      "increasing  α  leads  to  flatter  (i.e.,  less  extreme,  more  reasonable)  predictions;  this\n",
      "reduces the model’s variance but increases its bias.\n",
      "\n",
      "As with Linear Regression, we can perform Ridge Regression either by computing a \n",
      "closed-form equation or by performing Gradient Descent. The pros and cons are the\n",
      "same. Equation 4-9 shows the closed-form solution (where A is the (n + 1) × (n + 1)\n",
      "identity matrix13 except with a 0 in the top-left cell, corresponding to the bias term).\n",
      "\n",
      "Figure 4-17. Ridge Regression\n",
      "\n",
      "Equation 4-9. Ridge Regression closed-form solution\n",
      "\n",
      "θ = XTX + αA\n",
      "\n",
      "−1\n",
      "\n",
      "  XT   y\n",
      "\n",
      "Here is how to perform Ridge Regression with Scikit-Learn using a closed-form solu‐\n",
      "tion (a variant of Equation 4-9 using a matrix factorization technique by André-Louis\n",
      "Cholesky):\n",
      "\n",
      ">>> from sklearn.linear_model import Ridge\n",
      ">>> ridge_reg = Ridge(alpha=1, solver=\"cholesky\")\n",
      ">>> ridge_reg.fit(X, y)\n",
      "\n",
      "13 A square matrix full of 0s except for 1s on the main diagonal (top-left to bottom-right).\n",
      "\n",
      "138 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Training Models\n",
      "\n",
      "\f>>> ridge_reg.predict([[1.5]])\n",
      "array([[1.55071465]])\n",
      "\n",
      "And using Stochastic Gradient Descent:14\n",
      "\n",
      ">>> sgd_reg = SGDRegressor(penalty=\"l2\")\n",
      ">>> sgd_reg.fit(X, y.ravel())\n",
      ">>> sgd_reg.predict([[1.5]])\n",
      "array([1.47012588])\n",
      "\n",
      "The  penalty  hyperparameter  sets  the  type  of  regularization  term  to  use.  Specifying\n",
      "\"l2\" indicates that you want SGD to add a regularization term to the cost function \n",
      "equal  to  half  the  square  of  the  ℓ2  norm  of  the  weight  vector:  this  is  simply  Ridge\n",
      "Regression.\n",
      "\n",
      "Lasso Regression\n",
      "Least  Absolute  Shrinkage  and  Selection  Operator  Regression  (simply  called  Lasso\n",
      "Regression)  is  another  regularized  version  of  Linear  Regression:  just  like  Ridge\n",
      "Regression, it adds a regularization term to the cost function, but it uses the ℓ1 norm\n",
      "of the weight vector instead of half the square of the ℓ2 norm (see Equation 4-10).\n",
      "\n",
      "Equation 4-10. Lasso Regression cost function\n",
      "\n",
      "J θ = MSE θ + α∑i = 1\n",
      "\n",
      "n\n",
      "\n",
      "θi\n",
      "\n",
      "Figure  4-18  shows  the  same  thing  as  Figure  4-17  but  replaces  Ridge  models  with\n",
      "Lasso models and uses smaller α values.\n",
      "\n",
      "14 Alternatively you can use the Ridge class with the \"sag\" solver. Stochastic Average GD is a variant of SGD.\n",
      "For more details, see the presentation “Minimizing Finite Sums with the Stochastic Average Gradient Algo‐\n",
      "rithm” by Mark Schmidt et al. from the University of British Columbia.\n",
      "\n",
      "Regularized Linear Models \n",
      "\n",
      "| \n",
      "\n",
      "139\n",
      "\n",
      "\fFigure 4-18. Lasso Regression\n",
      "\n",
      "An important characteristic of Lasso Regression is that it tends to completely elimi‐\n",
      "nate the weights of the least important features (i.e., set them to zero). For example,\n",
      "the dashed line in the right plot on Figure 4-18 (with α = 10-7) looks quadratic, almost\n",
      "linear:  all  the  weights  for  the  high-degree  polynomial  features  are  equal  to  zero.  In\n",
      "other words, Lasso Regression automatically performs feature selection and outputs a\n",
      "sparse model (i.e., with few nonzero feature weights).\n",
      "\n",
      "You can get a sense of why this is the case by looking at Figure 4-19: on the top-left\n",
      "plot, the background contours (ellipses) represent an unregularized MSE cost func‐\n",
      "tion  (α  =  0),  and  the  white  circles  show  the  Batch  Gradient  Descent  path  with  that\n",
      "cost function. The foreground contours (diamonds) represent the ℓ1 penalty, and the\n",
      "triangles show the BGD path for this penalty only (α → ∞). Notice how the path first\n",
      "reaches θ1 = 0, then rolls down a gutter until it reaches θ2 = 0. On the top-right plot,\n",
      "the  contours  represent  the  same  cost  function  plus  an  ℓ1  penalty  with  α  =  0.5.  The\n",
      "global minimum is on the θ2 = 0 axis. BGD first reaches θ2 = 0, then rolls down the\n",
      "gutter  until  it  reaches  the  global  minimum.  The  two  bottom  plots  show  the  same\n",
      "thing but uses an ℓ2 penalty instead. The regularized minimum is closer to θ = 0 than\n",
      "the unregularized minimum, but the weights do not get fully eliminated.\n",
      "\n",
      "140 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Training Models\n",
      "\n",
      "\fFigure 4-19. Lasso versus Ridge regularization\n",
      "\n",
      "On the Lasso cost function, the BGD path tends to bounce across\n",
      "the  gutter  toward  the  end.  This  is  because  the  slope  changes\n",
      "abruptly at θ2 = 0. You need to gradually reduce the learning rate in\n",
      "order to actually converge to the global minimum.\n",
      "\n",
      "The Lasso cost function is not differentiable at θi = 0 (for i = 1, 2, ⋯, n), but Gradient\n",
      "Descent  still  works  fine  if  you  use  a  subgradient  vector  g15  instead  when  any  θi  =  0.\n",
      "Equation 4-11 shows a subgradient vector equation you can use for Gradient Descent\n",
      "with the Lasso cost function.\n",
      "\n",
      "Equation 4-11. Lasso Regression subgradient vector\n",
      "\n",
      "g θ, J = ∇θ MSE θ + α\n",
      "\n",
      "sign θ1\n",
      "sign θ2\n",
      "⋮\n",
      "sign θn\n",
      "\n",
      "   where  sign θi =\n",
      "\n",
      "−1 if θi < 0\n",
      "0 if θi = 0\n",
      "+1 if θi > 0\n",
      "\n",
      "15 You can think of a subgradient vector at a nondifferentiable point as an intermediate vector between the gra‐\n",
      "\n",
      "dient vectors around that point.\n",
      "\n",
      "Regularized Linear Models \n",
      "\n",
      "| \n",
      "\n",
      "141\n",
      "\n",
      "\fHere  is  a  small  Scikit-Learn  example  using  the  Lasso  class.  Note  that  you  could\n",
      "instead use an SGDRegressor(penalty=\"l1\").\n",
      "\n",
      ">>> from sklearn.linear_model import Lasso\n",
      ">>> lasso_reg = Lasso(alpha=0.1)\n",
      ">>> lasso_reg.fit(X, y)\n",
      ">>> lasso_reg.predict([[1.5]])\n",
      "array([1.53788174])\n",
      "\n",
      "Elastic Net\n",
      "Elastic Net is a middle ground between Ridge Regression and Lasso Regression. The\n",
      "regularization  term  is  a  simple  mix  of  both  Ridge  and  Lasso’s  regularization  terms,\n",
      "and  you  can  control  the  mix  ratio  r.  When  r  =  0,  Elastic  Net  is  equivalent  to  Ridge\n",
      "Regression, and when r = 1, it is equivalent to Lasso Regression (see Equation 4-12).\n",
      "\n",
      "Equation 4-12. Elastic Net cost function\n",
      "\n",
      "J θ = MSE θ + rα∑i = 1\n",
      "\n",
      "n\n",
      "\n",
      "θi +\n",
      "\n",
      "1 − r\n",
      "n\n",
      "2 α∑i = 1\n",
      "\n",
      "2\n",
      "\n",
      "θi\n",
      "\n",
      "So  when  should  you  use  plain  Linear  Regression  (i.e.,  without  any  regularization),\n",
      "Ridge, Lasso, or Elastic Net? It is almost always preferable to have at least a little bit of\n",
      "regularization, so generally you should avoid plain Linear Regression. Ridge is a good\n",
      "default, but if you suspect that only a few features are actually useful, you should pre‐\n",
      "fer Lasso or Elastic Net since they tend to reduce the useless features’ weights down to\n",
      "zero as we have discussed. In general, Elastic Net is preferred over Lasso since Lasso\n",
      "may  behave  erratically  when  the  number  of  features  is  greater  than  the  number  of\n",
      "training instances or when several features are strongly correlated.\n",
      "\n",
      "Here  is  a  short  example  using  Scikit-Learn’s  ElasticNet  (l1_ratio  corresponds  to\n",
      "the mix ratio r):\n",
      "\n",
      ">>> from sklearn.linear_model import ElasticNet\n",
      ">>> elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
      ">>> elastic_net.fit(X, y)\n",
      ">>> elastic_net.predict([[1.5]])\n",
      "array([1.54333232])\n",
      "\n",
      "Early Stopping\n",
      "A  very  different  way  to  regularize  iterative  learning  algorithms  such  as  Gradient\n",
      "Descent is to stop training as soon as the validation error reaches a minimum. This is\n",
      "called early stopping. Figure 4-20 shows a complex model (in this case a high-degree\n",
      "Polynomial  Regression  model)  being  trained  using  Batch  Gradient  Descent.  As  the\n",
      "epochs go by, the algorithm learns and its prediction error (RMSE) on the training set\n",
      "naturally goes down, and so does its prediction error on the validation set. However,\n",
      "\n",
      "142 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Training Models\n",
      "\n",
      "\fafter a while the validation error stops decreasing and actually starts to go back up.\n",
      "This indicates that the model has started to overfit the training data. With early stop‐\n",
      "ping you just stop training as soon as the validation error reaches the minimum. It is\n",
      "such a simple and efficient regularization technique that Geoffrey Hinton called it a\n",
      "“beautiful free lunch.”\n",
      "\n",
      "Figure 4-20. Early stopping regularization\n",
      "\n",
      "With  Stochastic  and  Mini-batch  Gradient  Descent,  the  curves  are\n",
      "not  so  smooth,  and  it  may  be  hard  to  know  whether  you  have\n",
      "reached the minimum or not. One solution is to stop only after the\n",
      "validation error has been above the minimum for some time (when\n",
      "you are confident that the model will not do any better), then roll\n",
      "back the model parameters to the point where the validation error\n",
      "was at a minimum.\n",
      "\n",
      "Here is a basic implementation of early stopping:\n",
      "\n",
      "from sklearn.base import clone\n",
      "\n",
      "# prepare the data\n",
      "poly_scaler = Pipeline([\n",
      "        (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\n",
      "        (\"std_scaler\", StandardScaler())\n",
      "    ])\n",
      "X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
      "X_val_poly_scaled = poly_scaler.transform(X_val)\n",
      "\n",
      "sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True,\n",
      "                       penalty=None, learning_rate=\"constant\", eta0=0.0005)\n",
      "\n",
      "Regularized Linear Models \n",
      "\n",
      "| \n",
      "\n",
      "143\n",
      "\n",
      "\fminimum_val_error = float(\"inf\")\n",
      "best_epoch = None\n",
      "best_model = None\n",
      "for epoch in range(1000):\n",
      "    sgd_reg.fit(X_train_poly_scaled, y_train)  # continues where it left off\n",
      "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
      "    val_error = mean_squared_error(y_val, y_val_predict)\n",
      "    if val_error < minimum_val_error:\n",
      "        minimum_val_error = val_error\n",
      "        best_epoch = epoch\n",
      "        best_model = clone(sgd_reg)\n",
      "\n",
      "Note that with warm_start=True, when the fit() method is called, it just continues\n",
      "training where it left off instead of restarting from scratch.\n",
      "\n",
      "Logistic Regression\n",
      "As we discussed in Chapter 1, some regression algorithms can be used for classifica‐\n",
      "tion as well (and vice versa). Logistic Regression (also called Logit Regression) is com‐\n",
      "monly used to estimate the probability that an instance belongs to a particular class\n",
      "(e.g., what is the probability that this email is spam?). If the estimated probability is\n",
      "greater  than  50%,  then  the  model  predicts  that  the  instance  belongs  to  that  class\n",
      "(called  the  positive  class,  labeled  “1”),  or  else  it  predicts  that  it  does  not  (i.e.,  it\n",
      "belongs to the negative class, labeled “0”). This makes it a binary classifier.\n",
      "\n",
      "Estimating Probabilities\n",
      "So  how  does  it  work?  Just  like  a  Linear  Regression  model,  a  Logistic  Regression\n",
      "model computes a weighted sum of the input features (plus a bias term), but instead\n",
      "of outputting the result directly like the Linear Regression model does, it outputs the\n",
      "logistic of this result (see Equation 4-13).\n",
      "\n",
      "Equation 4-13. Logistic Regression model estimated probability (vectorized form)\n",
      "p = hθ x = σ xTθ\n",
      "\n",
      "The logistic—noted σ(·)—is a sigmoid function (i.e., S-shaped) that outputs a number\n",
      "between 0 and 1. It is defined as shown in Equation 4-14 and Figure 4-21.\n",
      "\n",
      "Equation 4-14. Logistic function\n",
      "\n",
      "σ t =\n",
      "\n",
      "1\n",
      "1 + exp − t\n",
      "\n",
      "144 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Training Models\n",
      "\n",
      "\fFigure 4-21. Logistic function\n",
      "\n",
      "Once  the  Logistic  Regression  model  has  estimated  the  probability  p  =  hθ(x)  that  an\n",
      "instance x belongs to the positive class, it can make its prediction ŷ easily (see Equa‐\n",
      "tion 4-15).\n",
      "\n",
      "Equation 4-15. Logistic Regression model prediction\n",
      "\n",
      "y =\n",
      "\n",
      "0 if p < 0 . 5\n",
      "\n",
      "1 if p ≥ 0 . 5\n",
      "\n",
      "Notice that σ(t) < 0.5 when t < 0, and σ(t) ≥ 0.5 when t ≥ 0, so a Logistic Regression\n",
      "model predicts 1 if xT θ is positive, and 0 if it is negative.\n",
      "\n",
      "The score t is often called the logit: this name comes from the fact\n",
      "that the logit function, defined as logit(p) = log(p / (1 - p)), is the\n",
      "inverse of the logistic function. Indeed, if you compute the logit of\n",
      "the  estimated  probability  p,  you  will  find  that  the  result  is  t.  The\n",
      "logit  is  also  called  the  log-odds,  since  it  is  the  log  of  the  ratio\n",
      "between  the  estimated  probability  for  the  positive  class  and  the\n",
      "estimated probability for the negative class.\n",
      "\n",
      "Training and Cost Function\n",
      "Good,  now  you  know  how  a  Logistic  Regression  model  estimates  probabilities  and\n",
      "makes predictions. But how is it trained? The objective of training is to set the param‐\n",
      "eter vector θ so that the model estimates high probabilities for positive instances (y =\n",
      "1)  and  low  probabilities  for  negative  instances  (y  =  0).  This  idea  is  captured  by  the\n",
      "cost function shown in Equation 4-16 for a single training instance x.\n",
      "\n",
      "Equation 4-16. Cost function of a single training instance\n",
      "\n",
      "c θ =\n",
      "\n",
      "−log p\n",
      "\n",
      "if y = 1\n",
      "\n",
      "−log 1 − p if y = 0\n",
      "\n",
      "Logistic Regression \n",
      "\n",
      "| \n",
      "\n",
      "145\n",
      "\n",
      "\fThis cost function makes sense because – log(t) grows very large when t approaches\n",
      "0, so the cost will be large if the model estimates a probability close to 0 for a positive\n",
      "instance, and it will also be very large if the model estimates a probability close to 1\n",
      "for a negative instance. On the other hand, – log(t) is close to 0 when t is close to 1, so\n",
      "the  cost  will  be  close  to  0  if  the  estimated  probability  is  close  to  0  for  a  negative\n",
      "instance or close to 1 for a positive instance, which is precisely what we want.\n",
      "\n",
      "The cost function over the whole training set is simply the average cost over all train‐\n",
      "ing instances. It can be written in a single expression (as you can verify easily), called \n",
      "the log loss, shown in Equation 4-17.\n",
      "\n",
      "Equation 4-17. Logistic Regression cost function (log loss)\n",
      "\n",
      "J θ = −\n",
      "\n",
      "1\n",
      "m\n",
      "m ∑i = 1\n",
      "\n",
      "y i log p i + 1 − y i\n",
      "\n",
      "log 1 − p i\n",
      "\n",
      "The bad news is that there is no known closed-form equation to compute the value of\n",
      "θ that minimizes this cost function (there is no equivalent of the Normal Equation).\n",
      "But the good news is that this cost function is convex, so Gradient Descent (or any\n",
      "other optimization algorithm) is guaranteed to find the global minimum (if the learn‐\n",
      "ing rate is not too large and you wait long enough). The partial derivatives of the cost\n",
      "function with regards to the jth model parameter θj is given by Equation 4-18.\n",
      "\n",
      "Equation 4-18. Logistic cost function partial derivatives\n",
      "\n",
      "∂\n",
      "∂θ j\n",
      "\n",
      "J θ =\n",
      "\n",
      "m\n",
      "\n",
      "1\n",
      "m ∑\n",
      "\n",
      "i = 1\n",
      "\n",
      "i\n",
      "σ θTx i − y i x j\n",
      "\n",
      "This equation looks very much like Equation 4-5: for each instance it computes the\n",
      "prediction  error  and  multiplies  it  by  the  jth  feature  value,  and  then  it  computes  the\n",
      "average over all training instances. Once you have the gradient vector containing all\n",
      "the partial derivatives you can use it in the Batch Gradient Descent algorithm. That’s\n",
      "it: you now know how to train a Logistic Regression model. For Stochastic GD you\n",
      "would of course just take one instance at a time, and for Mini-batch GD you would\n",
      "use a mini-batch at a time.\n",
      "\n",
      "Decision Boundaries\n",
      "Let’s use the iris dataset to illustrate Logistic Regression. This is a famous dataset that\n",
      "contains  the  sepal  and  petal  length  and  width  of  150  iris  flowers  of  three  different\n",
      "species: Iris-Setosa, Iris-Versicolor, and Iris-Virginica (see Figure 4-22).\n",
      "\n",
      "146 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Training Models\n",
      "\n",
      "\fFigure 4-22. Flowers of three iris plant species16\n",
      "\n",
      "Let’s try to build a classifier to detect the Iris-Virginica type based only on the petal\n",
      "width feature. First let’s load the data:\n",
      "\n",
      ">>> from sklearn import datasets\n",
      ">>> iris = datasets.load_iris()\n",
      ">>> list(iris.keys())\n",
      "['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename']\n",
      ">>> X = iris[\"data\"][:, 3:]  # petal width\n",
      ">>> y = (iris[\"target\"] == 2).astype(np.int)  # 1 if Iris-Virginica, else 0\n",
      "\n",
      "Now let’s train a Logistic Regression model:\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(X, y)\n",
      "\n",
      "Let’s look at the model’s estimated probabilities for flowers with petal widths varying\n",
      "from 0 to 3 cm (Figure 4-23)17:\n",
      "\n",
      "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
      "y_proba = log_reg.predict_proba(X_new)\n",
      "plt.plot(X_new, y_proba[:, 1], \"g-\", label=\"Iris-Virginica\")\n",
      "\n",
      "16 Photos reproduced from the corresponding Wikipedia pages. Iris-Virginica photo by Frank Mayfield (Crea‐\n",
      "\n",
      "tive Commons BY-SA 2.0), Iris-Versicolor photo by D. Gordon E. Robertson (Creative Commons BY-SA 3.0),\n",
      "and Iris-Setosa photo is public domain.\n",
      "\n",
      "17 NumPy’s reshape() function allows one dimension to be –1, which means “unspecified”: the value is inferred\n",
      "\n",
      "from the length of the array and the remaining dimensions.\n",
      "\n",
      "Logistic Regression \n",
      "\n",
      "| \n",
      "\n",
      "147\n",
      "\n",
      "\fplt.plot(X_new, y_proba[:, 0], \"b--\", label=\"Not Iris-Virginica\")\n",
      "# + more Matplotlib code to make the image look pretty\n",
      "\n",
      "Figure 4-23. Estimated probabilities and decision boundary\n",
      "\n",
      "The  petal  width  of  Iris-Virginica  flowers  (represented  by  triangles)  ranges  from  1.4\n",
      "cm to 2.5 cm, while the other iris flowers (represented by squares) generally have a\n",
      "smaller petal width, ranging from 0.1 cm to 1.8 cm. Notice that there is a bit of over‐\n",
      "lap.  Above  about  2  cm  the  classifier  is  highly  confident  that  the  flower  is  an  Iris-\n",
      "Virginica  (it  outputs  a  high  probability  to  that  class),  while  below  1  cm  it  is  highly\n",
      "confident that it is not an Iris-Virginica (high probability for the “Not Iris-Virginica”\n",
      "class).  In  between  these  extremes,  the  classifier  is  unsure.  However,  if  you  ask  it  to\n",
      "predict  the  class  (using  the  predict()  method  rather  than  the  predict_proba()\n",
      "method), it will return whichever class is the most likely. Therefore, there is a decision\n",
      "boundary  at  around  1.6  cm  where  both  probabilities  are  equal  to  50%:  if  the  petal\n",
      "width  is  higher  than  1.6  cm,  the  classifier  will  predict  that  the  flower  is  an  Iris-\n",
      "Virginica, or else it will predict that it is not (even if it is not very confident):\n",
      "\n",
      ">>> log_reg.predict([[1.7], [1.5]])\n",
      "array([1, 0])\n",
      "\n",
      "Figure 4-24 shows the same dataset but this time displaying two features: petal width\n",
      "and length. Once trained, the Logistic Regression classifier can estimate the probabil‐\n",
      "ity that a new flower is an Iris-Virginica based on these two features. The dashed line\n",
      "represents the points where the model estimates a 50% probability: this is the model’s\n",
      "decision boundary. Note that it is a linear boundary.18 Each parallel line represents the\n",
      "points where the model outputs a specific probability, from 15% (bottom left) to 90%\n",
      "(top  right).  All  the  flowers  beyond  the  top-right  line  have  an  over  90%  chance  of\n",
      "being Iris-Virginica according to the model.\n",
      "\n",
      "18 It is the the set of points x such that θ0 + θ1x1 + θ2x2 = 0, which defines a straight line.\n",
      "\n",
      "148 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Training Models\n",
      "\n",
      "\fFigure 4-24. Linear decision boundary\n",
      "\n",
      "Just like the other linear models, Logistic Regression models can be regularized using \n",
      "ℓ1 or ℓ2 penalties. Scitkit-Learn actually adds an ℓ2 penalty by default.\n",
      "\n",
      "The  hyperparameter  controlling  the  regularization  strength  of  a\n",
      "Scikit-Learn LogisticRegression model is not alpha (as in other\n",
      "linear models), but its inverse: C. The higher the value of C, the less\n",
      "the model is regularized.\n",
      "\n",
      "Softmax Regression\n",
      "The Logistic Regression model can be generalized to support multiple classes directly,\n",
      "without  having  to  train  and  combine  multiple  binary  classifiers  (as  discussed  in\n",
      "Chapter 3). This is called Softmax Regression, or Multinomial Logistic Regression.\n",
      "\n",
      "The  idea  is  quite  simple:  when  given  an  instance  x,  the  Softmax  Regression  model\n",
      "first  computes  a  score  sk(x)  for  each  class  k,  then  estimates  the  probability  of  each\n",
      "class by applying the softmax function (also called the normalized exponential) to the\n",
      "scores. The equation to compute sk(x) should look familiar, as it is just like the equa‐\n",
      "tion for Linear Regression prediction (see Equation 4-19).\n",
      "\n",
      "Equation 4-19. Softmax score for class k\n",
      "sk x = xTθ k\n",
      "\n",
      "Note that each class has its own dedicated parameter vector θ(k). All these vectors are\n",
      "typically stored as rows in a parameter matrix Θ.\n",
      "\n",
      "Once you have computed the score of every class for the instance x, you can estimate\n",
      "the probability  pk that the instance belongs to class k by running the scores through\n",
      "the  softmax  function  (Equation  4-20):  it  computes  the  exponential  of  every  score,\n",
      "\n",
      "Logistic Regression \n",
      "\n",
      "| \n",
      "\n",
      "149\n",
      "\n",
      "\fthen  normalizes  them  (dividing  by  the  sum  of  all  the  exponentials).  The  scores  are\n",
      "generally  called  logits  or  log-odds  (although  they  are  actually  unnormalized  log-\n",
      "odds).\n",
      "\n",
      "Equation 4-20. Softmax function\n",
      "\n",
      "pk = σ s x k =\n",
      "\n",
      "exp sk x\n",
      "\n",
      "K\n",
      "∑ j = 1\n",
      "\n",
      "exp s j x\n",
      "\n",
      "• K is the number of classes.\n",
      "\n",
      "• s(x) is a vector containing the scores of each class for the instance x.\n",
      "\n",
      "• σ(s(x))k  is  the  estimated  probability  that  the  instance  x  belongs  to  class  k  given\n",
      "\n",
      "the scores of each class for that instance.\n",
      "\n",
      "Just  like  the  Logistic  Regression  classifier,  the  Softmax  Regression  classifier  predicts\n",
      "the  class  with  the  highest  estimated  probability  (which  is  simply  the  class  with  the\n",
      "highest score), as shown in Equation 4-21.\n",
      "\n",
      "Equation 4-21. Softmax Regression classifier prediction\n",
      "θ k T\n",
      "\n",
      "y = argmax\n",
      "\n",
      "σ s x k = argmax\n",
      "\n",
      "sk x = argmax\n",
      "\n",
      "x\n",
      "\n",
      "k\n",
      "\n",
      "k\n",
      "\n",
      "k\n",
      "\n",
      "• The argmax operator returns the value of a variable that maximizes a function. In\n",
      "this equation, it returns the value of k that maximizes the estimated probability\n",
      "σ(s(x))k.\n",
      "\n",
      "The Softmax Regression classifier predicts only one class at a time\n",
      "(i.e., it is multiclass, not multioutput) so it should be used only with\n",
      "mutually  exclusive  classes  such  as  different  types  of  plants.  You\n",
      "cannot use it to recognize multiple people in one picture.\n",
      "\n",
      "Now  that  you  know  how  the  model  estimates  probabilities  and  makes  predictions,\n",
      "let’s  take  a  look  at  training.  The  objective  is  to  have  a  model  that  estimates  a  high\n",
      "probability  for  the  target  class  (and  consequently  a  low  probability  for  the  other\n",
      "classes).  Minimizing  the  cost  function  shown  in  Equation  4-22,  called  the  cross\n",
      "entropy, should lead to this objective because it penalizes the model when it estimates\n",
      "a low probability for a target class. Cross entropy is frequently used to measure how\n",
      "\n",
      "150 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Training Models\n",
      "\n",
      "\fwell a set of estimated class probabilities match the target classes (we will use it again\n",
      "several times in the following chapters).\n",
      "\n",
      "Equation 4-22. Cross entropy cost function\n",
      "\n",
      "J Θ = −\n",
      "\n",
      "1\n",
      "m ∑i = 1\n",
      "\n",
      "K\n",
      "m ∑k = 1\n",
      "\n",
      "i\n",
      "i log pk\n",
      "yk\n",
      "\n",
      "• yk\n",
      "\n",
      "i  is the target probability that the ith instance belongs to class k. In general, it is\n",
      "either equal to 1 or 0, depending on whether the instance belongs to the class or\n",
      "not.\n",
      "\n",
      "Notice that when there are just two classes (K = 2), this cost function is equivalent to\n",
      "the Logistic Regression’s cost function (log loss; see Equation 4-17).\n",
      "\n",
      "Cross Entropy\n",
      "Cross  entropy  originated  from  information  theory.  Suppose  you  want  to  efficiently\n",
      "transmit information about the weather every day. If there are eight options (sunny,\n",
      "rainy, etc.), you could encode each option using 3 bits since 23 = 8. However, if you\n",
      "think  it  will  be  sunny  almost  every  day,  it  would  be  much  more  efficient  to  code\n",
      "“sunny” on just one bit (0) and the other seven options on 4 bits (starting with a 1).\n",
      "Cross entropy measures the average number of bits you actually send per option. If\n",
      "your assumption about the weather is perfect, cross entropy will just be equal to the\n",
      "entropy of the weather itself (i.e., its intrinsic unpredictability). But if your assump‐\n",
      "tions  are  wrong  (e.g.,  if  it  rains  often),  cross  entropy  will  be  greater  by  an  amount \n",
      "called the Kullback–Leibler divergence.\n",
      "\n",
      "The  cross  entropy  between  two  probability  distributions  p  and  q  is  defined  as\n",
      "H p, q = − ∑x p x log q x   (at  least  when  the  distributions  are  discrete).  For  more\n",
      "details, check out this video.\n",
      "\n",
      "The  gradient  vector  of  this  cost  function  with  regards  to  θ(k)  is  given  by  Equation\n",
      "4-23:\n",
      "\n",
      "Equation 4-23. Cross entropy gradient vector for class k\n",
      "\n",
      "∇\n",
      "\n",
      "θ\n",
      "\n",
      "k J Θ =\n",
      "\n",
      "m\n",
      "\n",
      "1\n",
      "m ∑\n",
      "\n",
      "i = 1\n",
      "\n",
      "i − yk\n",
      "pk\n",
      "\n",
      "i x i\n",
      "\n",
      "Now you can compute the gradient vector for every class, then use Gradient Descent\n",
      "(or any other optimization algorithm) to find the parameter matrix Θ that minimizes\n",
      "the cost function.\n",
      "\n",
      "Logistic Regression \n",
      "\n",
      "| \n",
      "\n",
      "151\n",
      "\n",
      "\fLet’s  use  Softmax  Regression  to  classify  the  iris  flowers  into  all  three  classes.  Scikit-\n",
      "Learn’s LogisticRegression uses one-versus-all by default when you train it on more\n",
      "than two classes, but you can set the multi_class hyperparameter to \"multinomial\"\n",
      "to  switch  it  to  Softmax  Regression  instead.  You  must  also  specify  a  solver  that  sup‐\n",
      "ports Softmax Regression, such as the \"lbfgs\" solver (see Scikit-Learn’s documenta‐\n",
      "tion  for  more  details).  It  also  applies  ℓ2  regularization  by  default,  which  you  can\n",
      "control using the hyperparameter C.\n",
      "\n",
      "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
      "y = iris[\"target\"]\n",
      "\n",
      "softmax_reg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\", C=10)\n",
      "softmax_reg.fit(X, y)\n",
      "\n",
      "So the next time you find an iris with 5 cm long and 2 cm wide petals, you can ask\n",
      "your model to tell you what type of iris it is, and it will answer Iris-Virginica (class 2)\n",
      "with 94.2% probability (or Iris-Versicolor with 5.8% probability):\n",
      "\n",
      ">>> softmax_reg.predict([[5, 2]])\n",
      "array([2])\n",
      ">>> softmax_reg.predict_proba([[5, 2]])\n",
      "array([[6.38014896e-07, 5.74929995e-02, 9.42506362e-01]])\n",
      "\n",
      "Figure 4-25 shows the resulting decision boundaries, represented by the background\n",
      "colors.  Notice  that  the  decision  boundaries  between  any  two  classes  are  linear.  The\n",
      "figure  also  shows  the  probabilities  for  the  Iris-Versicolor  class,  represented  by  the\n",
      "curved lines (e.g., the line labeled with 0.450 represents the 45% probability bound‐\n",
      "ary). Notice that the model can predict a class that has an estimated probability below\n",
      "50%. For example, at the point where all decision boundaries meet, all classes have an\n",
      "equal estimated probability of 33%.\n",
      "\n",
      "Figure 4-25. Softmax Regression decision boundaries\n",
      "\n",
      "152 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 4: Training Models\n",
      "\n",
      "\fExercises\n",
      "\n",
      "1. What Linear Regression training algorithm can you use if you have a training set\n",
      "\n",
      "with millions of features?\n",
      "\n",
      "2. Suppose  the  features  in  your  training  set  have  very  different  scales.  What  algo‐\n",
      "\n",
      "rithms might suffer from this, and how? What can you do about it?\n",
      "\n",
      "3. Can  Gradient  Descent  get  stuck  in  a  local  minimum  when  training  a  Logistic\n",
      "\n",
      "Regression model?\n",
      "\n",
      "4. Do  all  Gradient  Descent  algorithms  lead  to  the  same  model  provided  you  let\n",
      "\n",
      "them run long enough?\n",
      "\n",
      "5. Suppose  you  use  Batch  Gradient  Descent  and  you  plot  the  validation  error  at\n",
      "every epoch. If you notice that the validation error consistently goes up, what is\n",
      "likely going on? How can you fix this?\n",
      "\n",
      "6. Is it a good idea to stop Mini-batch Gradient Descent immediately when the vali‐\n",
      "\n",
      "dation error goes up?\n",
      "\n",
      "7. Which  Gradient  Descent  algorithm  (among  those  we  discussed)  will  reach  the\n",
      "vicinity  of  the  optimal  solution  the  fastest?  Which  will  actually  converge?  How\n",
      "can you make the others converge as well?\n",
      "\n",
      "8. Suppose you are using Polynomial Regression. You plot the learning curves and\n",
      "you notice that there is a large gap between the training error and the validation\n",
      "error. What is happening? What are three ways to solve this?\n",
      "\n",
      "9. Suppose  you  are  using  Ridge  Regression  and  you  notice  that  the  training  error\n",
      "and the validation error are almost equal and fairly high. Would you say that the\n",
      "model suffers from high bias or high variance? Should you increase the regulari‐\n",
      "zation hyperparameter α or reduce it?\n",
      "\n",
      "10. Why would you want to use:\n",
      "\n",
      "• Ridge Regression instead of plain Linear Regression (i.e., without any regulari‐\n",
      "\n",
      "zation)?\n",
      "\n",
      "• Lasso instead of Ridge Regression?\n",
      "\n",
      "• Elastic Net instead of Lasso?\n",
      "\n",
      "11. Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime.\n",
      "Should you implement two Logistic Regression classifiers or one Softmax Regres‐\n",
      "sion classifier?\n",
      "\n",
      "12. Implement Batch Gradient Descent with early stopping for Softmax Regression \n",
      "\n",
      "(without using Scikit-Learn).\n",
      "\n",
      "Solutions to these exercises are available in ???.\n",
      "\n",
      "Exercises \n",
      "\n",
      "| \n",
      "\n",
      "153\n",
      "\n",
      "\f\fCHAPTER 5\n",
      "Support Vector Machines\n",
      "\n",
      "With  Early  Release  ebooks,  you  get  books  in  their  earliest  form—\n",
      "the author’s raw and unedited content as he or she writes—so you\n",
      "can  take  advantage  of  these  technologies  long  before  the  official\n",
      "release of these titles. The following will be Chapter 5 in the final\n",
      "release of the book.\n",
      "\n",
      "A Support Vector Machine (SVM) is a very powerful and versatile Machine Learning\n",
      "model, capable of performing linear or nonlinear classification, regression, and even\n",
      "outlier detection. It is one of the most popular models in Machine Learning, and any‐\n",
      "one interested in Machine Learning should have it in their toolbox. SVMs are partic‐\n",
      "ularly well suited for classification of complex but small- or medium-sized datasets.\n",
      "\n",
      "This chapter will explain the core concepts of SVMs, how to use them, and how they\n",
      "work.\n",
      "\n",
      "Linear SVM Classification\n",
      "The fundamental idea behind SVMs is best explained with some pictures. Figure 5-1\n",
      "shows  part  of  the  iris  dataset  that  was  introduced  at  the  end  of  Chapter  4.  The  two\n",
      "classes can clearly be separated easily with a straight line (they are linearly separable).\n",
      "The  left  plot  shows  the  decision  boundaries  of  three  possible  linear  classifiers.  The\n",
      "model  whose  decision  boundary  is  represented  by  the  dashed  line  is  so  bad  that  it\n",
      "does not even separate the classes properly. The other two models work perfectly on\n",
      "this  training  set,  but  their  decision  boundaries  come  so  close  to  the  instances  that\n",
      "these  models  will  probably  not  perform  as  well  on  new  instances.  In  contrast,  the\n",
      "solid line in the plot on the right represents the decision boundary of an SVM classi‐\n",
      "fier;  this  line  not  only  separates  the  two  classes  but  also  stays  as  far  away  from  the\n",
      "closest training instances as possible. You can think of an SVM classifier as fitting the\n",
      "\n",
      "155\n",
      "\n",
      "\fwidest possible street (represented by the parallel dashed lines) between the classes.\n",
      "This is called large margin classification.\n",
      "\n",
      "Figure 5-1. Large margin classification\n",
      "\n",
      "Notice that adding more training instances “off the street” will not affect the decision\n",
      "boundary at all: it is fully determined (or “supported”) by the instances located on the\n",
      "edge  of  the  street.  These  instances  are  called  the  support  vectors  (they  are  circled  in\n",
      "Figure 5-1).\n",
      "\n",
      "SVMs  are  sensitive  to  the  feature  scales,  as  you  can  see  in\n",
      "Figure 5-2: on the left plot, the vertical scale is much larger than the\n",
      "horizontal scale, so the widest possible street is close to horizontal.\n",
      "After  feature  scaling  (e.g.,  using  Scikit-Learn’s  StandardScaler), \n",
      "the decision boundary looks much better (on the right plot).\n",
      "\n",
      "Figure 5-2. Sensitivity to feature scales\n",
      "\n",
      "Soft Margin Classification\n",
      "If we strictly impose that all instances be off the street and on the right side, this is\n",
      "called hard margin classification. There are two main issues with hard margin classifi‐\n",
      "cation. First, it only works if the data is linearly separable, and second it is quite sensi‐\n",
      "tive to outliers. Figure 5-3 shows the iris dataset with just one additional outlier: on\n",
      "the left, it is impossible to find a hard margin, and on the right the decision boundary\n",
      "ends up very different from the one we saw in Figure 5-1 without the outlier, and it\n",
      "will probably not generalize as well.\n",
      "\n",
      "156 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 5: Support Vector Machines\n",
      "\n",
      "\fFigure 5-3. Hard margin sensitivity to outliers\n",
      "\n",
      "To avoid these issues it is preferable to use a more flexible model. The objective is to\n",
      "find a good balance between keeping the street as large as possible and limiting the\n",
      "margin violations (i.e., instances that end up in the middle of the street or even on the\n",
      "wrong side). This is called soft margin classification.\n",
      "\n",
      "In Scikit-Learn’s SVM classes, you can control this balance using the C hyperparame‐\n",
      "ter: a smaller C value leads to a wider street but more margin violations. Figure 5-4\n",
      "shows the decision boundaries and margins of two soft margin SVM classifiers on a\n",
      "nonlinearly  separable  dataset.  On  the  left,  using  a  low  C  value  the  margin  is  quite\n",
      "large, but many instances end up on the street. On the right, using a high C value the\n",
      "classifier makes fewer margin violations but ends up with a smaller margin. However,\n",
      "it seems likely that the first classifier will generalize better: in fact even on this train‐\n",
      "ing  set  it  makes  fewer  prediction  errors,  since  most  of  the  margin  violations  are\n",
      "actually on the correct side of the decision boundary.\n",
      "\n",
      "Figure 5-4. Large margin (left) versus fewer margin violations (right)\n",
      "\n",
      "If  your  SVM  model  is  overfitting,  you  can  try  regularizing  it  by\n",
      "reducing C.\n",
      "\n",
      "The  following  Scikit-Learn  code  loads  the  iris  dataset,  scales  the  features,  and  then\n",
      "trains a linear SVM model (using the LinearSVC class with C = 1 and the hinge loss\n",
      "function,  described  shortly)  to  detect  Iris-Virginica  flowers.  The  resulting  model  is\n",
      "represented on the left of Figure 5-4.\n",
      "\n",
      "Linear SVM Classification \n",
      "\n",
      "| \n",
      "\n",
      "157\n",
      "\n",
      "\fimport numpy as np\n",
      "from sklearn import datasets\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.svm import LinearSVC\n",
      "\n",
      "iris = datasets.load_iris()\n",
      "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
      "y = (iris[\"target\"] == 2).astype(np.float64)  # Iris-Virginica\n",
      "\n",
      "svm_clf = Pipeline([\n",
      "        (\"scaler\", StandardScaler()),\n",
      "        (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")),\n",
      "    ])\n",
      "\n",
      "svm_clf.fit(X, y)\n",
      "\n",
      "Then, as usual, you can use the model to make predictions:\n",
      "\n",
      ">>> svm_clf.predict([[5.5, 1.7]])\n",
      "array([1.])\n",
      "\n",
      "Unlike  Logistic  Regression  classifiers,  SVM  classifiers  do  not  out‐\n",
      "put probabilities for each class.\n",
      "\n",
      "Alternatively, you could use the SVC class, using SVC(kernel=\"linear\", C=1), but it\n",
      "is much slower, especially with large training sets, so it is not recommended. Another\n",
      "option  is  to  use  the  SGDClassifier  class,  with  SGDClassifier(loss=\"hinge\",\n",
      "alpha=1/(m*C)). This applies regular Stochastic Gradient Descent (see Chapter 4) to\n",
      "train a linear SVM classifier. It does not converge as fast as the LinearSVC class, but it\n",
      "can  be  useful  to  handle  huge  datasets  that  do  not  fit  in  memory  (out-of-core  train‐\n",
      "ing), or to handle online classification tasks.\n",
      "\n",
      "The LinearSVC class regularizes the bias term, so you should center\n",
      "the  training  set  first  by  subtracting  its  mean.  This  is  automatic  if\n",
      "you scale the data using the StandardScaler. Moreover, make sure\n",
      "you set the loss hyperparameter to \"hinge\", as it is not the default\n",
      "value.  Finally,  for  better  performance  you  should  set  the  dual\n",
      "hyperparameter  to  False,  unless  there  are  more  features  than\n",
      "training instances (we will discuss duality later in the chapter).\n",
      "\n",
      "158 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 5: Support Vector Machines\n",
      "\n",
      "\fNonlinear SVM Classification\n",
      "Although  linear  SVM  classifiers  are  efficient  and  work  surprisingly  well  in  many\n",
      "cases, many datasets are not even close to being linearly separable. One approach to\n",
      "handling nonlinear datasets is to add more features, such as polynomial features (as\n",
      "you  did  in  Chapter  4);  in  some  cases  this  can  result  in  a  linearly  separable  dataset.\n",
      "Consider the left plot in Figure 5-5: it represents a simple dataset with just one feature\n",
      "x1. This dataset is not linearly separable, as you can see. But if you add a second fea‐\n",
      "ture x2 = (x1)2, the resulting 2D dataset is perfectly linearly separable.\n",
      "\n",
      "Figure 5-5. Adding features to make a dataset linearly separable\n",
      "\n",
      "To  implement  this  idea  using  Scikit-Learn,  you  can  create  a  Pipeline  containing  a\n",
      "PolynomialFeatures  transformer  (discussed  in  “Polynomial  Regression”  on  page\n",
      "130),  followed  by  a  StandardScaler  and  a  LinearSVC.  Let’s  test  this  on  the  moons\n",
      "dataset: this is a toy dataset for binary classification in which the data points are sha‐\n",
      "ped  as  two  interleaving  half  circles  (see  Figure  5-6).  You  can  generate  this  dataset\n",
      "using the make_moons() function:\n",
      "\n",
      "from sklearn.datasets import make_moons\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.preprocessing import PolynomialFeatures\n",
      "\n",
      "polynomial_svm_clf = Pipeline([\n",
      "        (\"poly_features\", PolynomialFeatures(degree=3)),\n",
      "        (\"scaler\", StandardScaler()),\n",
      "        (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))\n",
      "    ])\n",
      "\n",
      "polynomial_svm_clf.fit(X, y)\n",
      "\n",
      "Nonlinear SVM Classification \n",
      "\n",
      "| \n",
      "\n",
      "159\n",
      "\n",
      "\fFigure 5-6. Linear SVM classifier using polynomial features\n",
      "\n",
      "Polynomial Kernel\n",
      "Adding polynomial features is simple to implement and can work great with all sorts\n",
      "of Machine Learning algorithms (not just SVMs), but at a low polynomial degree it\n",
      "cannot deal with very complex datasets, and with a high polynomial degree it creates\n",
      "a huge number of features, making the model too slow.\n",
      "\n",
      "Fortunately,  when  using  SVMs  you  can  apply  an  almost  miraculous  mathematical\n",
      "technique called the kernel trick (it is explained in a moment). It makes it possible to\n",
      "get the same result as if you added many polynomial features, even with very high-\n",
      "degree polynomials, without actually having to add them. So there is no combinato‐\n",
      "rial explosion of the number of features since you don’t actually add any features. This\n",
      "trick is implemented by the SVC class. Let’s test it on the moons dataset:\n",
      "\n",
      "from sklearn.svm import SVC\n",
      "poly_kernel_svm_clf = Pipeline([\n",
      "        (\"scaler\", StandardScaler()),\n",
      "        (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
      "    ])\n",
      "poly_kernel_svm_clf.fit(X, y)\n",
      "\n",
      "This code trains an SVM classifier using a 3rd-degree polynomial kernel. It is repre‐\n",
      "sented on the left of Figure 5-7. On the right is another SVM classifier using a 10th-\n",
      "degree polynomial kernel. Obviously, if your model is overfitting, you might want to\n",
      "\n",
      "160 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 5: Support Vector Machines\n",
      "\n",
      "\freduce the polynomial degree. Conversely, if it is underfitting, you can try increasing\n",
      "it.  The  hyperparameter  coef0  controls  how  much  the  model  is  influenced  by  high-\n",
      "degree polynomials versus low-degree polynomials.\n",
      "\n",
      "Figure 5-7. SVM classifiers with a polynomial kernel\n",
      "\n",
      "A common approach to find the right hyperparameter values is to\n",
      "use grid search (see Chapter 2). It is often faster to first do a very\n",
      "coarse grid search, then a finer grid search around the best values\n",
      "found. Having a good sense of what each hyperparameter actually\n",
      "does can also help you search in the right part of the hyperparame‐\n",
      "ter space.\n",
      "\n",
      "Adding Similarity Features\n",
      "Another technique to tackle nonlinear problems is to add features computed using a\n",
      "similarity  function  that  measures  how  much  each  instance  resembles  a  particular\n",
      "landmark.  For  example,  let’s  take  the  one-dimensional  dataset  discussed  earlier  and\n",
      "add two landmarks to it at x1 = –2 and x1 = 1 (see the left plot in Figure 5-8). Next,\n",
      "let’s  define  the  similarity  function  to  be  the  Gaussian  Radial  Basis  Function  (RBF)\n",
      "with γ = 0.3 (see Equation 5-1).\n",
      "\n",
      "Equation 5-1. Gaussian RBF\n",
      "ϕγ x, ℓ = exp −γ∥ x − ℓ ∥2\n",
      "\n",
      "It is a bell-shaped function varying from 0 (very far away from the landmark) to 1 (at\n",
      "the landmark). Now we are ready to compute the new features. For example, let’s look\n",
      "at the instance x1 = –1: it is located at a distance of 1 from the first landmark, and 2\n",
      "from the second landmark. Therefore its new features are x2 = exp (–0.3 × 12) ≈ 0.74\n",
      "and x3 = exp (–0.3 × 22) ≈ 0.30. The plot on the right of Figure 5-8 shows the trans‐\n",
      "formed  dataset  (dropping  the  original  features).  As  you  can  see,  it  is  now  linearly\n",
      "separable.\n",
      "\n",
      "Nonlinear SVM Classification \n",
      "\n",
      "| \n",
      "\n",
      "161\n",
      "\n",
      "\fFigure 5-8. Similarity features using the Gaussian RBF\n",
      "\n",
      "You  may  wonder  how  to  select  the  landmarks.  The  simplest  approach  is  to  create  a\n",
      "landmark at the location of each and every instance in the dataset. This creates many\n",
      "dimensions and thus increases the chances that the transformed training set will be\n",
      "linearly separable. The downside is that a training set with m instances and n features\n",
      "gets transformed into a training set with m instances and m features (assuming you\n",
      "drop  the  original  features).  If  your  training  set  is  very  large,  you  end  up  with  an\n",
      "equally large number of features.\n",
      "\n",
      "Gaussian RBF Kernel\n",
      "Just like the polynomial features method, the similarity features method can be useful\n",
      "with  any  Machine  Learning  algorithm,  but  it  may  be  computationally  expensive  to\n",
      "compute  all  the  additional  features,  especially  on  large  training  sets.  However,  once\n",
      "again  the  kernel  trick  does  its  SVM  magic:  it  makes  it  possible  to  obtain  a  similar\n",
      "result  as  if  you  had  added  many  similarity  features,  without  actually  having  to  add\n",
      "them. Let’s try the Gaussian RBF kernel using the SVC class:\n",
      "\n",
      "rbf_kernel_svm_clf = Pipeline([\n",
      "        (\"scaler\", StandardScaler()),\n",
      "        (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
      "    ])\n",
      "rbf_kernel_svm_clf.fit(X, y)\n",
      "\n",
      "This  model  is  represented  on  the  bottom  left  of  Figure  5-9.  The  other  plots  show\n",
      "models trained with different values of hyperparameters gamma (γ) and C. Increasing\n",
      "gamma makes the bell-shape curve narrower (see the left plot of Figure 5-8), and as a\n",
      "result  each  instance’s  range  of  influence  is  smaller:  the  decision  boundary  ends  up\n",
      "being more irregular, wiggling around individual instances. Conversely, a small gamma \n",
      "value  makes  the  bell-shaped  curve  wider,  so  instances  have  a  larger  range  of  influ‐\n",
      "ence,  and  the  decision  boundary  ends  up  smoother.  So  γ  acts  like  a  regularization\n",
      "hyperparameter: if your model is overfitting, you should reduce it, and if it is under‐\n",
      "fitting, you should increase it (similar to the C hyperparameter).\n",
      "\n",
      "162 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 5: Support Vector Machines\n",
      "\n",
      "\fFigure 5-9. SVM classifiers using an RBF kernel\n",
      "\n",
      "Other  kernels  exist  but  are  used  much  more  rarely.  For  example,  some  kernels  are\n",
      "specialized for specific data structures. String kernels are sometimes used when classi‐\n",
      "fying text documents or DNA sequences (e.g., using the string subsequence kernel or\n",
      "kernels based on the Levenshtein distance).\n",
      "\n",
      "With so many kernels to choose from, how can you decide which\n",
      "one  to  use?  As  a  rule  of  thumb,  you  should  always  try  the  linear\n",
      "kernel first (remember that LinearSVC is much faster than SVC(ker\n",
      "nel=\"linear\")),  especially  if  the  training  set  is  very  large  or  if  it\n",
      "has plenty of features. If the training set is not too large, you should\n",
      "try  the  Gaussian  RBF  kernel  as  well;  it  works  well  in  most  cases.\n",
      "Then  if  you  have  spare  time  and  computing  power,  you  can  also\n",
      "experiment with a few other kernels using cross-validation and grid\n",
      "search,  especially  if  there  are  kernels  specialized  for  your  training\n",
      "set’s data structure.\n",
      "\n",
      "Computational Complexity\n",
      "The LinearSVC class is based on the liblinear library, which implements an optimized\n",
      "algorithm for linear SVMs.1 It does not support the kernel trick, but it scales almost\n",
      "\n",
      "1 “A Dual Coordinate Descent Method for Large-scale Linear SVM,” Lin et al. (2008).\n",
      "\n",
      "Nonlinear SVM Classification \n",
      "\n",
      "| \n",
      "\n",
      "163\n",
      "\n",
      "\flinearly with the number of training instances and the number of features: its training\n",
      "time complexity is roughly O(m × n).\n",
      "\n",
      "The algorithm takes longer if you require a very high precision. This is controlled by\n",
      "the  tolerance  hyperparameter  ϵ  (called  tol  in  Scikit-Learn).  In  most  classification\n",
      "tasks, the default tolerance is fine.\n",
      "\n",
      "The SVC class is based on the libsvm library, which implements an algorithm that sup‐\n",
      "ports  the  kernel  trick.2  The  training  time  complexity  is  usually  between  O(m2  ×  n)\n",
      "and O(m3 × n). Unfortunately, this means that it gets dreadfully slow when the num‐\n",
      "ber  of  training  instances  gets  large  (e.g.,  hundreds  of  thousands  of  instances).  This\n",
      "algorithm is perfect for complex but small or medium training sets. However, it scales\n",
      "well  with  the  number  of  features,  especially  with  sparse  features  (i.e.,  when  each\n",
      "instance has few nonzero features). In this case, the algorithm scales roughly with the\n",
      "average  number  of  nonzero  features  per  instance.  Table  5-1  compares  Scikit-Learn’s\n",
      "SVM classification classes.\n",
      "\n",
      "Table 5-1. Comparison of Scikit-Learn classes for SVM classification\n",
      "\n",
      "Class\n",
      "\n",
      "LinearSVC\n",
      "\n",
      "Time complexity\n",
      "O(m × n)\n",
      "\n",
      "Out-of-core support\n",
      "No\n",
      "\n",
      "Scaling required Kernel trick\n",
      "Yes\n",
      "\n",
      "No\n",
      "\n",
      "SGDClassifier O(m × n)\n",
      "\n",
      "Yes\n",
      "\n",
      "SVC\n",
      "\n",
      "O(m² × n) to O(m³ × n) No\n",
      "\n",
      "Yes\n",
      "\n",
      "Yes\n",
      "\n",
      "No\n",
      "\n",
      "Yes\n",
      "\n",
      "SVM Regression\n",
      "As we mentioned earlier, the SVM algorithm is quite versatile: not only does it sup‐\n",
      "port  linear  and  nonlinear  classification,  but  it  also  supports  linear  and  nonlinear\n",
      "regression. The trick is to reverse the objective: instead of trying to fit the largest pos‐\n",
      "sible  street  between  two  classes  while  limiting  margin  violations,  SVM  Regression\n",
      "tries to fit as many instances as possible on the street while limiting margin violations\n",
      "(i.e., instances off the street). The width of the street is controlled by a hyperparame‐\n",
      "ter ϵ. Figure 5-10 shows two linear SVM Regression models trained on some random\n",
      "linear data, one with a large margin (ϵ = 1.5) and the other with a small margin (ϵ =\n",
      "0.5).\n",
      "\n",
      "2 “Sequential Minimal Optimization (SMO),” J. Platt (1998).\n",
      "\n",
      "164 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 5: Support Vector Machines\n",
      "\n",
      "\fFigure 5-10. SVM Regression\n",
      "\n",
      "Adding more training instances within the margin does not affect the model’s predic‐\n",
      "tions; thus, the model is said to be ϵ-insensitive.\n",
      "\n",
      "You  can  use  Scikit-Learn’s  LinearSVR  class  to  perform  linear  SVM  Regression.  The\n",
      "following code produces the model represented on the left of Figure 5-10 (the train‐\n",
      "ing data should be scaled and centered first):\n",
      "\n",
      "from sklearn.svm import LinearSVR\n",
      "\n",
      "svm_reg = LinearSVR(epsilon=1.5)\n",
      "svm_reg.fit(X, y)\n",
      "\n",
      "To tackle nonlinear regression tasks, you can use a kernelized SVM model. For exam‐\n",
      "ple, Figure 5-11 shows SVM Regression on a random quadratic training set, using a\n",
      "2nd-degree polynomial kernel. There is little regularization on the left plot (i.e., a large\n",
      "C value), and much more regularization on the right plot (i.e., a small C value).\n",
      "\n",
      "Figure 5-11. SVM regression using a 2nd-degree polynomial kernel\n",
      "\n",
      "SVM Regression \n",
      "\n",
      "| \n",
      "\n",
      "165\n",
      "\n",
      "\fThe following code produces the model represented on the left of Figure 5-11 using\n",
      "Scikit-Learn’s SVR class (which supports the kernel trick). The SVR class is the regres‐\n",
      "sion equivalent of the SVC class, and the LinearSVR class is the regression equivalent\n",
      "of the LinearSVC class. The LinearSVR class scales linearly with the size of the train‐\n",
      "ing set (just like the LinearSVC class), while the SVR class gets much too slow when\n",
      "the training set grows large (just like the SVC class).\n",
      "\n",
      "from sklearn.svm import SVR\n",
      "\n",
      "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n",
      "svm_poly_reg.fit(X, y)\n",
      "\n",
      "SVMs can also be used for outlier detection; see Scikit-Learn’s doc‐\n",
      "umentation for more details.\n",
      "\n",
      "Under the Hood\n",
      "This section explains how SVMs make predictions and how their training algorithms\n",
      "work, starting with linear SVM classifiers. You can safely skip it and go straight to the\n",
      "exercises at the end of this chapter if you are just getting started with Machine Learn‐\n",
      "ing, and come back later when you want to get a deeper understanding of SVMs.\n",
      "\n",
      "First, a word about notations: in Chapter 4 we used the convention of putting all the \n",
      "model  parameters  in  one  vector  θ,  including  the  bias  term  θ0  and  the  input  feature\n",
      "weights θ1 to θn, and adding a bias input x0 = 1 to all instances. In this chapter, we will\n",
      "use a different convention, which is more convenient (and more common) when you\n",
      "are dealing with SVMs: the bias term will be called b and the feature weights vector\n",
      "will be called w. No bias feature will be added to the input feature vectors.\n",
      "\n",
      "Decision Function and Predictions\n",
      "The linear SVM classifier model predicts the class of a new instance x by simply com‐\n",
      "puting the decision function wT x + b = w1 x1 + ⋯ + wn xn + b: if the result is positive,\n",
      "the  predicted  class  ŷ  is  the  positive  class  (1),  or  else  it  is  the  negative  class  (0);  see\n",
      "Equation 5-2.\n",
      "\n",
      "Equation 5-2. Linear SVM classifier prediction\n",
      "\n",
      "y =\n",
      "\n",
      "0 if wTx + b < 0,\n",
      "1 if wTx + b ≥ 0\n",
      "\n",
      "166 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 5: Support Vector Machines\n",
      "\n",
      "\fFigure 5-12 shows the decision function that corresponds to the model on the left of\n",
      "Figure  5-4:  it  is  a  two-dimensional  plane  since  this  dataset  has  two  features  (petal\n",
      "width and petal length). The decision boundary is the set of points where the decision\n",
      "function is equal to 0: it is the intersection of two planes, which is a straight line (rep‐\n",
      "resented by the thick solid line).3\n",
      "\n",
      "Figure 5-12. Decision function for the iris dataset\n",
      "\n",
      "The dashed lines represent the points where the decision function is equal to 1 or –1:\n",
      "they  are  parallel  and  at  equal  distance  to  the  decision  boundary,  forming  a  margin\n",
      "around it. Training a linear SVM classifier means finding the value of w and b that\n",
      "make this margin as wide as possible while avoiding margin violations (hard margin)\n",
      "or limiting them (soft margin).\n",
      "\n",
      "Training Objective\n",
      "Consider the slope of the decision function: it is equal to the norm of the weight vec‐\n",
      "tor, ∥ w ∥. If we divide this slope by 2, the points where the decision function is equal\n",
      "to ±1 are going to be twice as far away from the decision boundary. In other words,\n",
      "dividing the slope by 2 will multiply the margin by 2. Perhaps this is easier to visual‐\n",
      "ize in 2D in Figure 5-13. The smaller the weight vector w, the larger the margin.\n",
      "\n",
      "3 More generally, when there are n features, the decision function is an n-dimensional hyperplane, and the deci‐\n",
      "\n",
      "sion boundary is an (n – 1)-dimensional hyperplane.\n",
      "\n",
      "Under the Hood \n",
      "\n",
      "| \n",
      "\n",
      "167\n",
      "\n",
      "\fFigure 5-13. A smaller weight vector results in a larger margin\n",
      "\n",
      "So we want to minimize ∥ w ∥ to get a large margin. However, if we also want to avoid\n",
      "any margin violation (hard margin), then we need the decision function to be greater\n",
      "than  1  for  all  positive  training  instances,  and  lower  than  –1  for  negative  training\n",
      "instances. If we define t(i) = –1 for negative instances (if y(i) = 0) and t(i) = 1 for positive\n",
      "instances (if y(i) = 1), then we can express this constraint as t(i)(wT x(i) + b) ≥ 1 for all\n",
      "instances.\n",
      "\n",
      "We can therefore express the hard margin linear SVM classifier objective as the con‐\n",
      "strained optimization problem in Equation 5-3.\n",
      "\n",
      "Equation 5-3. Hard margin linear SVM classifier objective\n",
      "\n",
      "minimize\n",
      "w, b\n",
      "\n",
      "wTw\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "subject to t i wTx i + b ≥ 1\n",
      "\n",
      "for i = 1, 2, ⋯, m\n",
      "\n",
      "We are minimizing \n",
      "\n",
      "1\n",
      "2 wT w, which is equal to \n",
      "\n",
      "1\n",
      "2\n",
      "∥ w ∥2 has a nice and simple derivative\n",
      "minimizing ∥ w ∥. Indeed, \n",
      "(it is just w) while ∥ w ∥ is not differentiable at w = 0. Optimization\n",
      "algorithms work much better on differentiable functions.\n",
      "\n",
      "∥ w ∥2, rather than\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "To get the soft margin objective, we need to introduce a slack variable ζ(i) ≥ 0 for each\n",
      "instance:4 ζ(i) measures how much the ith instance is allowed to violate the margin. We\n",
      "now have two conflicting objectives: making the slack variables as small as possible to\n",
      "reduce the margin violations, and making  1\n",
      "2 wT w as small as possible to increase the\n",
      "margin. This is where the C hyperparameter comes in: it allows us to define the trade‐\n",
      "\n",
      "4 Zeta (ζ) is the 6th letter of the Greek alphabet.\n",
      "\n",
      "168 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 5: Support Vector Machines\n",
      "\n",
      "\foff between these two objectives. This gives us the constrained optimization problem\n",
      "in Equation 5-4.\n",
      "\n",
      "Equation 5-4. Soft margin linear SVM classifier objective\n",
      "\n",
      "minimize\n",
      "w, b, ζ\n",
      "\n",
      "m\n",
      "wTw + C ∑\n",
      "i = 1\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "ζ i\n",
      "\n",
      "subject to t i wTx i + b ≥ 1 − ζ i\n",
      "\n",
      "and ζ i ≥ 0\n",
      "\n",
      "for i = 1, 2, ⋯, m\n",
      "\n",
      "Quadratic Programming\n",
      "The hard margin and soft margin problems are both convex quadratic optimization\n",
      "problems  with  linear  constraints.  Such  problems  are  known  as  Quadratic  Program‐\n",
      "ming  (QP)  problems.  Many  off-the-shelf  solvers  are  available  to  solve  QP  problems\n",
      "using  a  variety  of  techniques  that  are  outside  the  scope  of  this  book.5  The  general\n",
      "problem formulation is given by Equation 5-5.\n",
      "\n",
      "Equation 5-5. Quadratic Programming problem\n",
      "\n",
      "Minimize\n",
      "p\n",
      "subject to\n",
      "\n",
      "where\n",
      "\n",
      "pTHp\n",
      "\n",
      "+\n",
      "\n",
      "fTp\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "Ap ≤ b\n",
      "p is an np‐dimensional vector (np = number of parameters),\n",
      "H is an np × np matrix,\n",
      "f\n",
      "\n",
      "is an np‐dimensional vector,\n",
      "\n",
      "A is an nc × np matrix (nc = number of constraints),\n",
      "b\n",
      "\n",
      "is an nc‐dimensional vector.\n",
      "\n",
      "Note that the expression A p ≤ b actually defines nc constraints: pT a(i) ≤ b(i) for i = 1,\n",
      "2, ⋯, nc, where a(i) is the vector containing the elements of the ith row of A and b(i) is\n",
      "the ith element of b.\n",
      "\n",
      "You can easily verify that if you set the QP parameters in the following way, you get\n",
      "the hard margin linear SVM classifier objective:\n",
      "\n",
      "• np = n + 1, where n is the number of features (the +1 is for the bias term).\n",
      "\n",
      "5 To learn more about Quadratic Programming, you can start by reading Stephen Boyd and Lieven Vanden‐\n",
      "\n",
      "berghe, Convex Optimization (Cambridge, UK: Cambridge University Press, 2004) or watch Richard Brown’s\n",
      "series of video lectures.\n",
      "\n",
      "Under the Hood \n",
      "\n",
      "| \n",
      "\n",
      "169\n",
      "\n",
      "\f• nc = m, where m is the number of training instances.\n",
      "• H is the np × np identity matrix, except with a zero in the top-left cell (to ignore\n",
      "\n",
      "the bias term).\n",
      "\n",
      "• f = 0, an np-dimensional vector full of 0s.\n",
      "• b = –1, an nc-dimensional vector full of –1s.\n",
      "• a(i) = –t(i) x˙ (i), where x˙ (i) is equal to x(i) with an extra bias feature x˙ 0 = 1.\n",
      "\n",
      "So one way to train a hard margin linear SVM classifier is just to use an off-the-shelf\n",
      "QP solver by passing it the preceding parameters. The resulting vector p will contain\n",
      "the bias term b = p0 and the feature weights wi = pi for i = 1, 2, ⋯, n. Similarly, you\n",
      "can use a QP solver to solve the soft margin problem (see the exercises at the end of\n",
      "the chapter).\n",
      "\n",
      "However, to use the kernel trick we are going to look at a different constrained opti‐\n",
      "mization problem.\n",
      "\n",
      "The Dual Problem\n",
      "Given a constrained optimization problem, known as the primal problem, it is possi‐\n",
      "ble to express a different but closely related problem, called its dual problem. The sol‐\n",
      "ution to the dual problem typically gives a lower bound to the solution of the primal\n",
      "problem, but under some conditions it can even have the same solutions as the pri‐\n",
      "mal  problem.  Luckily,  the  SVM  problem  happens  to  meet  these  conditions,6  so  you\n",
      "can choose to solve the primal problem or the dual problem; both will have the same\n",
      "solution.  Equation  5-6  shows  the  dual  form  of  the  linear  SVM  objective  (if  you  are\n",
      "interested  in  knowing  how  to  derive  the  dual  problem  from  the  primal  problem,\n",
      "see ???).\n",
      "\n",
      "Equation 5-6. Dual form of the linear SVM objective\n",
      "\n",
      "minimize\n",
      "α\n",
      "\n",
      "m\n",
      "\n",
      "1\n",
      "2 ∑\n",
      "\n",
      "i = 1\n",
      "\n",
      "α i α j t i t j x i T\n",
      "\n",
      "m\n",
      "∑\n",
      "j = 1\n",
      "subject to α i ≥ 0\n",
      "\n",
      "x j\n",
      "\n",
      "−\n",
      "\n",
      "m\n",
      "∑\n",
      "i = 1\n",
      "\n",
      "α i\n",
      "\n",
      "for i = 1, 2, ⋯, m\n",
      "\n",
      "6 The objective function is convex, and the inequality constraints are continuously differentiable and convex\n",
      "\n",
      "functions.\n",
      "\n",
      "170 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 5: Support Vector Machines\n",
      "\n",
      "\fOnce you find the vector α that minimizes this equation (using a QP solver), you can\n",
      "compute w and b that minimize the primal problem by using Equation 5-7.\n",
      "\n",
      "Equation 5-7. From the dual solution to the primal solution\n",
      "\n",
      "m\n",
      "w = ∑\n",
      "i = 1\n",
      "\n",
      "α i t i x i\n",
      "\n",
      "b =\n",
      "\n",
      "1\n",
      "ns\n",
      "\n",
      "m\n",
      "∑\n",
      "i = 1\n",
      "i\n",
      "\n",
      "> 0\n",
      "\n",
      "α\n",
      "\n",
      "t i − wTx i\n",
      "\n",
      "The  dual  problem  is  faster  to  solve  than  the  primal  when  the  number  of  training\n",
      "instances is smaller than the number of features. More importantly, it makes the ker‐\n",
      "nel trick possible, while the primal does not. So what is this kernel trick anyway?\n",
      "\n",
      "Kernelized SVM\n",
      "Suppose  you  want  to  apply  a  2nd-degree  polynomial  transformation  to  a  two-\n",
      "dimensional  training  set  (such  as  the  moons  training  set),  then  train  a  linear  SVM\n",
      "classifier on the transformed training set. Equation 5-8 shows the 2nd-degree polyno‐\n",
      "mial mapping function ϕ that you want to apply.\n",
      "\n",
      "Equation 5-8. Second-degree polynomial mapping\n",
      "\n",
      "ϕ x = ϕ\n",
      "\n",
      "x1\n",
      "x2\n",
      "\n",
      "=\n",
      "\n",
      "2\n",
      "\n",
      "x1\n",
      "2 x1x2\n",
      "2\n",
      "x2\n",
      "\n",
      "Notice that the transformed vector is three-dimensional instead of two-dimensional.\n",
      "Now let’s look at what happens to a couple of two-dimensional vectors, a and b, if we\n",
      "apply this 2nd-degree polynomial mapping and then compute the dot product7 of the\n",
      "transformed vectors (See Equation 5-9).\n",
      "\n",
      "7 As explained in Chapter 4, the dot product of two vectors a and b is normally noted a · b. However, in\n",
      "\n",
      "Machine Learning, vectors are frequently represented as column vectors (i.e., single-column matrices), so the\n",
      "dot product is achieved by computing aTb. To remain consistent with the rest of the book, we will use this\n",
      "notation here, ignoring the fact that this technically results in a single-cell matrix rather than a scalar value.\n",
      "\n",
      "Under the Hood \n",
      "\n",
      "| \n",
      "\n",
      "171\n",
      "\n",
      "\fEquation 5-9. Kernel trick for a 2nd-degree polynomial mapping\n",
      "\n",
      "ϕ a Tϕ b\n",
      "\n",
      "=\n",
      "\n",
      "2\n",
      "\n",
      "a1\n",
      "2 a1a2\n",
      "2\n",
      "\n",
      "a2\n",
      "\n",
      "T\n",
      "\n",
      "2\n",
      "\n",
      "b1\n",
      "2 b1b2\n",
      "2\n",
      "b2\n",
      "\n",
      "= a1\n",
      "\n",
      "2b1\n",
      "\n",
      "2 + 2a1b1a2b2 + a2\n",
      "\n",
      "2b2\n",
      "\n",
      "2\n",
      "\n",
      "= a1b1 + a2b2\n",
      "\n",
      "2 =\n",
      "\n",
      "2\n",
      "\n",
      "a1\n",
      "a2\n",
      "\n",
      "T b1\n",
      "b2\n",
      "\n",
      "= aTb\n",
      "\n",
      "2\n",
      "\n",
      "How about that? The dot product of the transformed vectors is equal to the square of\n",
      "the dot product of the original vectors: ϕ(a)T ϕ(b) = (aT b)2.\n",
      "\n",
      "Now here is the key insight: if you apply the transformation ϕ to all training instan‐\n",
      "ces,  then  the  dual  problem  (see  Equation  5-6)  will  contain  the  dot  product  ϕ(x(i))T\n",
      "ϕ(x(j)). But if ϕ is the 2nd-degree polynomial transformation defined in Equation 5-8,\n",
      "then you can replace this dot product of transformed vectors simply by  x i T\n",
      "you don’t actually need to transform the training instances at all: just replace the dot\n",
      "product  by  its  square  in  Equation  5-6.  The  result  will  be  strictly  the  same  as  if  you\n",
      "went through the trouble of actually transforming the training set then fitting a linear\n",
      "SVM algorithm, but this trick makes the whole process much more computationally\n",
      "efficient. This is the essence of the kernel trick.\n",
      "\n",
      "2\n",
      ". So\n",
      "\n",
      "x j\n",
      "\n",
      "The  function  K(a,  b)  =  (aT  b)2  is  called  a  2nd-degree  polynomial  kernel.  In  Machine\n",
      "Learning,  a  kernel  is  a  function  capable  of  computing  the  dot  product  ϕ(a)T  ϕ(b)\n",
      "based  only  on  the  original  vectors  a  and  b,  without  having  to  compute  (or  even  to\n",
      "know about) the transformation ϕ. Equation 5-10 lists some of the most commonly\n",
      "used kernels.\n",
      "\n",
      "Equation 5-10. Common kernels\n",
      "\n",
      "Linear:\n",
      "\n",
      "Polynomial:\n",
      "\n",
      "Gaussian RBF:\n",
      "\n",
      "Sigmoid:\n",
      "\n",
      "d\n",
      "\n",
      "K a, b = aTb\n",
      "K a, b = γaTb + r\n",
      "K a, b = exp −γ∥ a − b ∥2\n",
      "K a, b = tanh γaTb + r\n",
      "\n",
      "172 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 5: Support Vector Machines\n",
      "\n",
      "\fMercer’s Theorem\n",
      "According to Mercer’s theorem, if a function K(a, b) respects a few mathematical con‐\n",
      "ditions called Mercer’s conditions (K must be continuous, symmetric in its arguments\n",
      "so  K(a,  b)  =  K(b,  a),  etc.),  then  there  exists  a  function  ϕ  that  maps  a  and  b  into\n",
      "another space (possibly with much higher dimensions) such that K(a, b) = ϕ(a)T ϕ(b).\n",
      "So you can use K as a kernel since you know ϕ exists, even if you don’t know what ϕ\n",
      "is. In the case of the Gaussian RBF kernel, it can be shown that ϕ actually maps each\n",
      "training instance to an infinite-dimensional space, so it’s a good thing you don’t need\n",
      "to actually perform the mapping!\n",
      "\n",
      "Note that some frequently used kernels (such as the Sigmoid kernel) don’t respect all\n",
      "of Mercer’s conditions, yet they generally work well in practice.\n",
      "\n",
      "There is still one loose end we must tie. Equation 5-7 shows how to go from the dual\n",
      "solution to the primal solution in the case of a linear SVM classifier, but if you apply\n",
      "the kernel trick you end up with equations that include ϕ(x(i)). In fact, w must have\n",
      "the same number of dimensions as ϕ(x(i)), which may be huge or even infinite, so you\n",
      "can’t  compute  it.  But  how  can  you  make  predictions  without  knowing  w?  Well,  the\n",
      "good news is that you can plug in the formula for w from Equation 5-7 into the deci‐\n",
      "sion function for a new instance x(n), and you get an equation with only dot products\n",
      "between  input  vectors.  This  makes  it  possible  to  use  the  kernel  trick,  once  again\n",
      "(Equation 5-11).\n",
      "\n",
      "Equation 5-11. Making predictions with a kernelized SVM\n",
      "\n",
      "ϕ x n\n",
      "\n",
      "h\n",
      "\n",
      "w, b\n",
      "\n",
      "m\n",
      "= wTϕ x n + b = ∑\n",
      "i = 1\n",
      "\n",
      "T\n",
      "\n",
      "α i t i ϕ x i\n",
      "\n",
      "ϕ x n + b\n",
      "\n",
      "m\n",
      "= ∑\n",
      "i = 1\n",
      "\n",
      "α i t i ϕ x i T\n",
      "\n",
      "ϕ x n\n",
      "\n",
      "+ b\n",
      "\n",
      "α i t i K x i , x n + b\n",
      "\n",
      "m\n",
      "= ∑\n",
      "i = 1\n",
      "i\n",
      "\n",
      "α\n",
      "\n",
      "> 0\n",
      "\n",
      "Note that since α(i) ≠ 0 only for support vectors, making predictions involves comput‐\n",
      "ing the dot product of the new input vector x(n) with only the support vectors, not all\n",
      "the training instances. Of course, you also need to compute the bias term b, using the\n",
      "same trick (Equation 5-12).\n",
      "\n",
      "Under the Hood \n",
      "\n",
      "| \n",
      "\n",
      "173\n",
      "\n",
      "\fEquation 5-12. Computing the bias term using the kernel trick\n",
      "\n",
      "b =\n",
      "\n",
      "1\n",
      "ns\n",
      "\n",
      "m\n",
      "∑\n",
      "i = 1\n",
      "i\n",
      "\n",
      "> 0\n",
      "\n",
      "α\n",
      "\n",
      "=\n",
      "\n",
      "1\n",
      "ns\n",
      "\n",
      "m\n",
      "∑\n",
      "i = 1\n",
      "i\n",
      "\n",
      "> 0\n",
      "\n",
      "α\n",
      "\n",
      "t i − wTϕ x i\n",
      "\n",
      "=\n",
      "\n",
      "1\n",
      "ns\n",
      "\n",
      "m\n",
      "∑\n",
      "i = 1\n",
      "i\n",
      "\n",
      "> 0\n",
      "\n",
      "α\n",
      "\n",
      "m\n",
      "t i − ∑\n",
      "j = 1\n",
      "\n",
      "T\n",
      "\n",
      "α j t j ϕ x j\n",
      "\n",
      "ϕ x i\n",
      "\n",
      "m\n",
      "t i − ∑\n",
      "j = 1\n",
      "j\n",
      "\n",
      "α\n",
      "\n",
      "> 0\n",
      "\n",
      "α j t j K x i , x j\n",
      "\n",
      "If  you  are  starting  to  get  a  headache,  it’s  perfectly  normal:  it’s  an  unfortunate  side\n",
      "effect of the kernel trick.\n",
      "\n",
      "Online SVMs\n",
      "Before concluding this chapter, let’s take a quick look at online SVM classifiers (recall\n",
      "that online learning means learning incrementally, typically as new instances arrive).\n",
      "\n",
      "For  linear  SVM  classifiers,  one  method  is  to  use  Gradient  Descent  (e.g.,  using\n",
      "SGDClassifier)  to  minimize  the  cost  function  in  Equation  5-13,  which  is  derived\n",
      "from  the  primal  problem.  Unfortunately  it  converges  much  more  slowly  than  the\n",
      "methods based on QP.\n",
      "\n",
      "Equation 5-13. Linear SVM classifier cost function\n",
      "\n",
      "J w, b =\n",
      "\n",
      "m\n",
      "wTw + C ∑\n",
      "i = 1\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "max 0, 1 − t i wTx i + b\n",
      "\n",
      "The first sum in the cost function will push the model to have a small weight vector\n",
      "w, leading to a larger margin. The second sum computes the total of all margin viola‐\n",
      "tions. An instance’s margin violation is equal to 0 if it is located off the street and on\n",
      "the  correct  side,  or  else  it  is  proportional  to  the  distance  to  the  correct  side  of  the\n",
      "street. Minimizing this term ensures that the model makes the margin violations as\n",
      "small and as few as possible\n",
      "\n",
      "Hinge Loss\n",
      "The function max(0, 1 – t) is called the hinge loss function (represented below). It is\n",
      "equal to 0 when t ≥ 1. Its derivative (slope) is equal to –1 if t < 1 and 0 if t > 1. It is not\n",
      "differentiable  at  t  =  1,  but  just  like  for  Lasso  Regression  (see  “Lasso  Regression”  on\n",
      "page 139) you can still use Gradient Descent using any subderivative at t = 1 (i.e., any\n",
      "value between –1 and 0).\n",
      "\n",
      "174 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 5: Support Vector Machines\n",
      "\n",
      "\fIt is also possible to implement online kernelized SVMs—for example, using “Incre‐\n",
      "mental and Decremental SVM Learning”8 or “Fast Kernel Classifiers with Online and\n",
      "Active  Learning.”9  However,  these  are  implemented  in  Matlab  and  C++.  For  large-\n",
      "scale nonlinear problems, you may want to consider using neural networks instead \n",
      "(see Part II).\n",
      "\n",
      "Exercises\n",
      "\n",
      "1. What is the fundamental idea behind Support Vector Machines?\n",
      "\n",
      "2. What is a support vector?\n",
      "\n",
      "3. Why is it important to scale the inputs when using SVMs?\n",
      "\n",
      "4. Can an SVM classifier output a confidence score when it classifies an instance?\n",
      "\n",
      "What about a probability?\n",
      "\n",
      "5. Should you use the primal or the dual form of the SVM problem to train a model\n",
      "\n",
      "on a training set with millions of instances and hundreds of features?\n",
      "\n",
      "6. Say  you  trained  an  SVM  classifier  with  an  RBF  kernel.  It  seems  to  underfit  the\n",
      "\n",
      "training set: should you increase or decrease γ (gamma)? What about C?\n",
      "\n",
      "7. How should you set the QP parameters (H, f, A, and b) to solve the soft margin\n",
      "\n",
      "linear SVM classifier problem using an off-the-shelf QP solver?\n",
      "\n",
      "8. Train  a  LinearSVC  on  a  linearly  separable  dataset.  Then  train  an  SVC  and  a\n",
      "SGDClassifier on the same dataset. See if you can get them to produce roughly\n",
      "the same model.\n",
      "\n",
      "9. Train an SVM classifier on the MNIST dataset. Since SVM classifiers are binary\n",
      "classifiers,  you  will  need  to  use  one-versus-all  to  classify  all  10  digits.  You  may\n",
      "\n",
      "8 “Incremental and Decremental Support Vector Machine Learning,” G. Cauwenberghs, T. Poggio (2001).\n",
      "\n",
      "9 “Fast Kernel Classifiers with Online and Active Learning,“ A. Bordes, S. Ertekin, J. Weston, L. Bottou (2005).\n",
      "\n",
      "Exercises \n",
      "\n",
      "| \n",
      "\n",
      "175\n",
      "\n",
      "\fwant to tune the hyperparameters using small validation sets to speed up the pro‐\n",
      "cess. What accuracy can you reach?\n",
      "\n",
      "10. Train an SVM regressor on the California housing dataset.\n",
      "\n",
      "Solutions to these exercises are available in ???.\n",
      "\n",
      "176 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 5: Support Vector Machines\n",
      "\n",
      "\fCHAPTER 6\n",
      "Decision Trees\n",
      "\n",
      "With  Early  Release  ebooks,  you  get  books  in  their  earliest  form—\n",
      "the author’s raw and unedited content as he or she writes—so you\n",
      "can  take  advantage  of  these  technologies  long  before  the  official\n",
      "release of these titles. The following will be Chapter 6 in the final\n",
      "release of the book.\n",
      "\n",
      "Like  SVMs,  Decision  Trees  are  versatile  Machine  Learning  algorithms  that  can  per‐\n",
      "form  both  classification  and  regression  tasks,  and  even  multioutput  tasks.  They  are\n",
      "very powerful algorithms, capable of fitting complex datasets. For example, in Chap‐\n",
      "ter 2 you trained a DecisionTreeRegressor model on the California housing dataset,\n",
      "fitting it perfectly (actually overfitting it).\n",
      "\n",
      "Decision Trees are also the fundamental components of Random Forests (see Chap‐\n",
      "ter  7),  which  are  among  the  most  powerful  Machine  Learning  algorithms  available\n",
      "today.\n",
      "\n",
      "In this chapter we will start by discussing how to train, visualize, and make predic‐\n",
      "tions  with  Decision  Trees.  Then  we  will  go  through  the  CART  training  algorithm\n",
      "used  by  Scikit-Learn,  and  we  will  discuss  how  to  regularize  trees  and  use  them  for\n",
      "regression tasks. Finally, we will discuss some of the limitations of Decision Trees.\n",
      "\n",
      "Training and Visualizing a Decision Tree\n",
      "To understand Decision Trees, let’s just build one and take a look at how it makes pre‐\n",
      "dictions.  The  following  code  trains  a  DecisionTreeClassifier  on  the  iris  dataset\n",
      "(see Chapter 4):\n",
      "\n",
      "from sklearn.datasets import load_iris\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "177\n",
      "\n",
      "\firis = load_iris()\n",
      "X = iris.data[:, 2:] # petal length and width\n",
      "y = iris.target\n",
      "\n",
      "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
      "tree_clf.fit(X, y)\n",
      "\n",
      "You  can  visualize  the  trained  Decision  Tree  by  first  using  the  export_graphviz() \n",
      "method to output a graph definition file called iris_tree.dot:\n",
      "\n",
      "from sklearn.tree import export_graphviz\n",
      "\n",
      "export_graphviz(\n",
      "        tree_clf,\n",
      "        out_file=image_path(\"iris_tree.dot\"),\n",
      "        feature_names=iris.feature_names[2:],\n",
      "        class_names=iris.target_names,\n",
      "        rounded=True,\n",
      "        filled=True\n",
      "    )\n",
      "\n",
      "Then you can convert this .dot file to a variety of formats such as PDF or PNG using\n",
      "the dot command-line tool from the graphviz package.1 This command line converts\n",
      "the .dot file to a .png image file:\n",
      "\n",
      "$ dot -Tpng iris_tree.dot -o iris_tree.png\n",
      "\n",
      "Your first decision tree looks like Figure 6-1.\n",
      "\n",
      "1 Graphviz is an open source graph visualization software package, available at http://www.graphviz.org/.\n",
      "\n",
      "178 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 6: Decision Trees\n",
      "\n",
      "\fFigure 6-1. Iris Decision Tree\n",
      "\n",
      "Making Predictions\n",
      "Let’s see how the tree represented in Figure 6-1 makes predictions. Suppose you find\n",
      "an iris flower and you want to classify it. You start at the root node (depth 0, at the\n",
      "top): this node asks whether the flower’s petal length is smaller than 2.45 cm. If it is,\n",
      "then you move down to the root’s left child node (depth 1, left). In this case, it is a leaf\n",
      "node (i.e., it does not have any children nodes), so it does not ask any questions: you\n",
      "can  simply  look  at  the  predicted  class  for  that  node  and  the  Decision  Tree  predicts\n",
      "that your flower is an Iris-Setosa (class=setosa).\n",
      "\n",
      "Now suppose you find another flower, but this time the petal length is greater than\n",
      "2.45 cm. You must move down to the root’s right child node (depth 1, right), which is\n",
      "not a leaf node, so it asks another question: is the petal width smaller than 1.75 cm? If\n",
      "it is, then your flower is most likely an Iris-Versicolor (depth 2, left). If not, it is likely\n",
      "an Iris-Virginica (depth 2, right). It’s really that simple.\n",
      "\n",
      "One  of  the  many  qualities  of  Decision  Trees  is  that  they  require\n",
      "very little data preparation. In particular, they don’t require feature\n",
      "scaling or centering at all.\n",
      "\n",
      "Making Predictions \n",
      "\n",
      "| \n",
      "\n",
      "179\n",
      "\n",
      "\fA  node’s  samples  attribute  counts  how  many  training  instances  it  applies  to.  For\n",
      "example,  100  training  instances  have  a  petal  length  greater  than  2.45  cm  (depth  1,\n",
      "right),  among  which  54  have  a  petal  width  smaller  than  1.75  cm  (depth  2,  left).  A\n",
      "node’s value attribute tells you how many training instances of each class this node\n",
      "applies  to:  for  example,  the  bottom-right  node  applies  to  0  Iris-Setosa,  1  Iris-\n",
      "Versicolor, and 45 Iris-Virginica. Finally, a node’s gini attribute measures its impur‐\n",
      "ity: a node is “pure” (gini=0) if all training instances it applies to belong to the same\n",
      "class.  For  example,  since  the  depth-1  left  node  applies  only  to  Iris-Setosa  training\n",
      "instances, it is pure and its gini score is 0. Equation 6-1 shows how the training algo‐\n",
      "rithm computes the gini score Gi of the ith node. For example, the depth-2 left node\n",
      "has a gini score equal to 1 – (0/54)2 – (49/54)2 – (5/54)2 ≈ 0.168. Another impurity\n",
      "measure is discussed shortly.\n",
      "\n",
      "Equation 6-1. Gini impurity\n",
      "\n",
      "n\n",
      "Gi = 1 − ∑\n",
      "k = 1\n",
      "\n",
      "2\n",
      "\n",
      "pi, k\n",
      "\n",
      "• pi,k is the ratio of class k instances among the training instances in the ith node.\n",
      "\n",
      "Scikit-Learn uses the CART algorithm, which produces only binary\n",
      "trees:  nonleaf  nodes  always  have  two  children  (i.e.,  questions  only\n",
      "have yes/no answers). However, other algorithms such as ID3 can\n",
      "produce Decision Trees with nodes that have more than two chil‐\n",
      "dren.\n",
      "\n",
      "Figure 6-2 shows this Decision Tree’s decision boundaries. The thick vertical line rep‐\n",
      "resents  the  decision  boundary  of  the  root  node  (depth  0):  petal  length  =  2.45  cm.\n",
      "Since the left area is pure (only Iris-Setosa), it cannot be split any further. However,\n",
      "the right area is impure, so the depth-1 right node splits it at petal width = 1.75 cm\n",
      "(represented  by  the  dashed  line).  Since  max_depth  was  set  to  2,  the  Decision  Tree\n",
      "stops  right  there.  However,  if  you  set  max_depth  to  3,  then  the  two  depth-2  nodes\n",
      "would each add another decision boundary (represented by the dotted lines).\n",
      "\n",
      "180 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 6: Decision Trees\n",
      "\n",
      "\fFigure 6-2. Decision Tree decision boundaries\n",
      "\n",
      "Model Interpretation: White Box Versus Black Box\n",
      "As you can see Decision Trees are fairly intuitive and their decisions are easy to inter‐\n",
      "pret. Such models are often called white box models. In contrast, as we will see, Ran‐\n",
      "dom  Forests  or  neural  networks  are  generally  considered  black  box  models.  They\n",
      "make great predictions, and you can easily check the calculations that they performed\n",
      "to make these predictions; nevertheless, it is usually hard to explain in simple terms\n",
      "why the predictions were made. For example, if a neural network says that a particu‐\n",
      "lar person appears on a picture, it is hard to know what actually contributed to this\n",
      "prediction:  did  the  model  recognize  that  person’s  eyes?  Her  mouth?  Her  nose?  Her\n",
      "shoes? Or even the couch that she was sitting on? Conversely, Decision Trees provide\n",
      "nice and simple classification rules that can even be applied manually if need be (e.g.,\n",
      "for flower classification).\n",
      "\n",
      "Estimating Class Probabilities\n",
      "A Decision Tree can also estimate the probability that an instance belongs to a partic‐\n",
      "ular class k: first it traverses the tree to find the leaf node for this instance, and then it\n",
      "returns  the  ratio  of  training  instances  of  class  k  in  this  node.  For  example,  suppose\n",
      "you  have  found  a  flower  whose  petals  are  5  cm  long  and  1.5  cm  wide.  The  corre‐\n",
      "sponding leaf node is the depth-2 left node, so the Decision Tree should output the\n",
      "following  probabilities:  0%  for  Iris-Setosa  (0/54),  90.7%  for  Iris-Versicolor  (49/54),\n",
      "and 9.3% for Iris-Virginica (5/54). And of course if you ask it to predict the class, it\n",
      "should output Iris-Versicolor (class 1) since it has the highest probability. Let’s check\n",
      "this:\n",
      "\n",
      ">>> tree_clf.predict_proba([[5, 1.5]])\n",
      "array([[0.        , 0.90740741, 0.09259259]])\n",
      "\n",
      "Estimating Class Probabilities \n",
      "\n",
      "| \n",
      "\n",
      "181\n",
      "\n",
      "\f>>> tree_clf.predict([[5, 1.5]])\n",
      "array([1])\n",
      "\n",
      "Perfect!  Notice  that  the  estimated  probabilities  would  be  identical  anywhere  else  in\n",
      "the bottom-right rectangle of Figure 6-2—for example, if the petals were 6 cm long\n",
      "and 1.5 cm wide (even though it seems obvious that it would most likely be an Iris-\n",
      "Virginica in this case).\n",
      "\n",
      "The CART Training Algorithm\n",
      "Scikit-Learn  uses  the  Classification  And  Regression  Tree  (CART)  algorithm  to  train\n",
      "Decision Trees (also called “growing” trees). The idea is really quite simple: the algo‐\n",
      "rithm first splits the training set in two subsets using a single feature k and a thres‐\n",
      "hold tk (e.g., “petal length ≤ 2.45 cm”). How does it choose k and tk? It searches for the\n",
      "pair (k, tk) that produces the purest subsets (weighted by their size). The cost function\n",
      "that the algorithm tries to minimize is given by Equation 6-2.\n",
      "\n",
      "Equation 6-2. CART cost function for classification\n",
      "\n",
      "J k, tk =\n",
      "\n",
      "mleft\n",
      "m\n",
      "\n",
      "Gleft +\n",
      "\n",
      "mright\n",
      "m\n",
      "\n",
      "Gright\n",
      "\n",
      "where\n",
      "\n",
      "Gleft/right measures the impurity of the left/right subset,\n",
      "mleft/right is the number of instances in the left/right subset.\n",
      "\n",
      "Once  it  has  successfully  split  the  training  set  in  two,  it  splits  the  subsets  using  the\n",
      "same logic, then the sub-subsets and so on, recursively. It stops recursing once it rea‐\n",
      "ches the maximum depth (defined by the max_depth hyperparameter), or if it cannot\n",
      "find  a  split  that  will  reduce  impurity.  A  few  other  hyperparameters  (described  in  a\n",
      "moment)  control  additional  stopping  conditions  (min_samples_split,  min_sam\n",
      "ples_leaf, min_weight_fraction_leaf, and max_leaf_nodes).\n",
      "\n",
      "182 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 6: Decision Trees\n",
      "\n",
      "\fAs you can see, the CART algorithm is a greedy algorithm: it greed‐\n",
      "ily searches for an optimum split at the top level, then repeats the\n",
      "process at each level. It does not check whether or not the split will\n",
      "lead to the lowest possible impurity several levels down. A greedy\n",
      "algorithm often produces a reasonably good solution, but it is not\n",
      "guaranteed to be the optimal solution.\n",
      "\n",
      "Unfortunately,  finding  the  optimal  tree  is  known  to  be  an  NP-\n",
      "Complete  problem:2  it  requires  O(exp(m))  time,  making  the  prob‐\n",
      "lem  intractable  even  for  fairly  small  training  sets.  This  is  why  we\n",
      "must settle for a “reasonably good” solution.\n",
      "\n",
      "Computational Complexity\n",
      "Making  predictions  requires  traversing  the  Decision  Tree  from  the  root  to  a  leaf.\n",
      "Decision Trees are generally approximately balanced, so traversing the Decision Tree\n",
      "requires  going  through  roughly  O(log2(m))  nodes.3  Since  each  node  only  requires\n",
      "checking the value of one feature, the overall prediction complexity is just O(log2(m)),\n",
      "independent of the number of features. So predictions are very fast, even when deal‐\n",
      "ing with large training sets.\n",
      "\n",
      "However, the training algorithm compares all features (or less if max_features is set)\n",
      "on all samples at each node. This results in a training complexity of O(n × m log(m)).\n",
      "For small training sets (less than a few thousand instances), Scikit-Learn can speed up\n",
      "training by presorting the data (set presort=True), but this slows down training con‐\n",
      "siderably for larger training sets.\n",
      "\n",
      "Gini Impurity or Entropy?\n",
      "By default, the Gini impurity measure is used, but you can select the entropy impurity\n",
      "measure instead by setting the criterion hyperparameter to \"entropy\". The concept\n",
      "of  entropy  originated  in  thermodynamics  as  a  measure  of  molecular  disorder:\n",
      "entropy approaches zero when molecules are still and well ordered. It later spread to a\n",
      "wide variety of domains, including Shannon’s information theory, where it measures\n",
      "the average information content of a message:4 entropy is zero when all messages are\n",
      "identical.  In  Machine  Learning,  it  is  frequently  used  as  an  impurity  measure:  a  set’s\n",
      "\n",
      "2 P is the set of problems that can be solved in polynomial time. NP is the set of problems whose solutions can\n",
      "be verified in polynomial time. An NP-Hard problem is a problem to which any NP problem can be reduced\n",
      "in polynomial time. An NP-Complete problem is both NP and NP-Hard. A major open mathematical ques‐\n",
      "tion is whether or not P = NP. If P ≠ NP (which seems likely), then no polynomial algorithm will ever be\n",
      "found for any NP-Complete problem (except perhaps on a quantum computer).\n",
      "\n",
      "3 log2 is the binary logarithm. It is equal to log2(m) = log(m) / log(2).\n",
      "4 A reduction of entropy is often called an information gain.\n",
      "\n",
      "Computational Complexity \n",
      "\n",
      "| \n",
      "\n",
      "183\n",
      "\n",
      "\fentropy is zero when it contains instances of only one class. Equation 6-3 shows the\n",
      "definition  of  the  entropy  of  the  ith  node.  For  example,  the  depth-2  left  node  in\n",
      "Figure 6-1 has an entropy equal to − 49\n",
      "\n",
      "49\n",
      "\n",
      "54 − 5\n",
      "\n",
      "54 log2\n",
      "\n",
      "5\n",
      "54  ≈ 0.445.\n",
      "\n",
      "54 log2\n",
      "\n",
      "Equation 6-3. Entropy\n",
      "\n",
      "n\n",
      "Hi = − ∑\n",
      "k = 1\n",
      "\n",
      "p\n",
      "\n",
      "i, k\n",
      "\n",
      "≠ 0\n",
      "\n",
      "pi, k log2 pi, k\n",
      "\n",
      "So should you use Gini impurity or entropy? The truth is, most of the time it does not\n",
      "make  a  big  difference:  they  lead  to  similar  trees.  Gini  impurity  is  slightly  faster  to\n",
      "compute,  so  it  is  a  good  default.  However,  when  they  differ,  Gini  impurity  tends  to\n",
      "isolate the most frequent class in its own branch of the tree, while entropy tends to\n",
      "produce slightly more balanced trees.5\n",
      "\n",
      "Regularization Hyperparameters\n",
      "Decision Trees make very few assumptions about the training data (as opposed to lin‐\n",
      "ear  models,  which  obviously  assume  that  the  data  is  linear,  for  example).  If  left\n",
      "unconstrained,  the  tree  structure  will  adapt  itself  to  the  training  data,  fitting  it  very\n",
      "closely,  and  most  likely  overfitting  it.  Such  a  model  is  often  called  a  nonparametric\n",
      "model, not because it does not have any parameters (it often has a lot) but because the\n",
      "number of parameters is not determined prior to training, so the model structure is\n",
      "free to stick closely to the data. In contrast, a parametric model such as a linear model\n",
      "has  a  predetermined  number  of  parameters,  so  its  degree  of  freedom  is  limited,\n",
      "reducing the risk of overfitting (but increasing the risk of underfitting).\n",
      "\n",
      "To avoid overfitting the training data, you need to restrict the Decision Tree’s freedom\n",
      "during training. As you know by now, this is called regularization. The regularization\n",
      "hyperparameters depend on the algorithm used, but generally you can at least restrict\n",
      "the  maximum  depth  of  the  Decision  Tree.  In  Scikit-Learn,  this  is  controlled  by  the\n",
      "max_depth  hyperparameter  (the  default  value  is  None,  which  means  unlimited).\n",
      "Reducing max_depth will regularize the model and thus reduce the risk of overfitting.\n",
      "\n",
      "The DecisionTreeClassifier class has a few other parameters that similarly restrict\n",
      "the shape of the Decision Tree: min_samples_split (the minimum number of sam‐\n",
      "ples a node must have before it can be split), min_samples_leaf (the minimum num‐\n",
      "ber  of  samples  a  leaf  node  must  have),  min_weight_fraction_leaf  (same  as\n",
      "min_samples_leaf  but  expressed  as  a  fraction  of  the  total  number  of  weighted\n",
      "\n",
      "5 See Sebastian Raschka’s interesting analysis for more details.\n",
      "\n",
      "184 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 6: Decision Trees\n",
      "\n",
      "\finstances),  max_leaf_nodes  (maximum  number  of  leaf  nodes),  and  max_features\n",
      "(maximum number of features that are evaluated for splitting at each node). Increas‐\n",
      "ing  min_*  hyperparameters  or  reducing  max_*  hyperparameters  will  regularize  the\n",
      "model.\n",
      "\n",
      "Other algorithms work by first training the Decision Tree without\n",
      "restrictions,  then  pruning  (deleting)  unnecessary  nodes.  A  node\n",
      "whose  children  are  all  leaf  nodes  is  considered  unnecessary  if  the\n",
      "purity improvement it provides is not statistically significant. Stan‐\n",
      "dard  statistical  tests,  such  as  the  χ2  test,  are  used  to  estimate  the\n",
      "probability  that  the  improvement  is  purely  the  result  of  chance\n",
      "(which is called the null hypothesis). If this probability, called the p-\n",
      "value, is higher than a given threshold (typically 5%, controlled by\n",
      "a hyperparameter), then the node is considered unnecessary and its\n",
      "children  are  deleted.  The  pruning  continues  until  all  unnecessary\n",
      "nodes have been pruned.\n",
      "\n",
      "Figure  6-3  shows  two  Decision  Trees  trained  on  the  moons  dataset  (introduced  in\n",
      "Chapter 5). On the left, the Decision Tree is trained with the default hyperparameters\n",
      "(i.e.,  no  restrictions),  and  on  the  right  the  Decision  Tree  is  trained  with  min_sam\n",
      "ples_leaf=4.  It  is  quite  obvious  that  the  model  on  the  left  is  overfitting,  and  the\n",
      "model on the right will probably generalize better.\n",
      "\n",
      "Figure 6-3. Regularization using min_samples_leaf\n",
      "\n",
      "Regression\n",
      "Decision  Trees  are  also  capable  of  performing  regression  tasks.  Let’s  build  a  regres‐\n",
      "sion  tree  using  Scikit-Learn’s  DecisionTreeRegressor  class,  training  it  on  a  noisy\n",
      "quadratic dataset with max_depth=2:\n",
      "\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "\n",
      "Regression \n",
      "\n",
      "| \n",
      "\n",
      "185\n",
      "\n",
      "\ftree_reg = DecisionTreeRegressor(max_depth=2)\n",
      "tree_reg.fit(X, y)\n",
      "\n",
      "The resulting tree is represented on Figure 6-4.\n",
      "\n",
      "Figure 6-4. A Decision Tree for regression\n",
      "\n",
      "This tree looks very similar to the classification tree you built earlier. The main differ‐\n",
      "ence is that instead of predicting a class in each node, it predicts a value. For example,\n",
      "suppose you want to make a prediction for a new instance with x1 = 0.6. You traverse\n",
      "the  tree  starting  at  the  root,  and  you  eventually  reach  the  leaf  node  that  predicts\n",
      "value=0.1106. This prediction is simply the average target value of the 110 training\n",
      "instances associated to this leaf node. This prediction results in a Mean Squared Error\n",
      "(MSE) equal to 0.0151 over these 110 instances.\n",
      "\n",
      "This  model’s  predictions  are  represented  on  the  left  of  Figure  6-5.  If  you  set\n",
      "max_depth=3, you get the predictions represented on the right. Notice how the pre‐\n",
      "dicted value for each region is always the average target value of the instances in that\n",
      "region. The algorithm splits each region in a way that makes most training instances\n",
      "as close as possible to that predicted value.\n",
      "\n",
      "186 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 6: Decision Trees\n",
      "\n",
      "\fFigure 6-5. Predictions of two Decision Tree regression models\n",
      "\n",
      "The CART algorithm works mostly the same way as earlier, except that instead of try‐\n",
      "ing to split the training set in a way that minimizes impurity, it now tries to split the\n",
      "training set in a way that minimizes the MSE. Equation 6-4 shows the cost function\n",
      "that the algorithm tries to minimize.\n",
      "\n",
      "Equation 6-4. CART cost function for regression\n",
      "\n",
      "J k, tk =\n",
      "\n",
      "mleft\n",
      "m\n",
      "\n",
      "MSEleft +\n",
      "\n",
      "mright\n",
      "m\n",
      "\n",
      "MSEright where\n",
      "\n",
      "MSEnode = ∑\n",
      "\n",
      "ynode − y i 2\n",
      "\n",
      "i ∈ node\n",
      "1\n",
      "mnode\n",
      "\n",
      "∑\n",
      "i ∈ node\n",
      "\n",
      "y i\n",
      "\n",
      "ynode =\n",
      "\n",
      "Just like for classification tasks, Decision Trees are prone to overfitting when dealing\n",
      "with  regression  tasks.  Without  any  regularization  (i.e.,  using  the  default  hyperpara‐\n",
      "meters),  you  get  the  predictions  on  the  left  of  Figure  6-6.  It  is  obviously  overfitting\n",
      "the training set very badly. Just setting min_samples_leaf=10 results in a much more\n",
      "reasonable model, represented on the right of Figure 6-6.\n",
      "\n",
      "Figure 6-6. Regularizing a Decision Tree regressor\n",
      "\n",
      "Regression \n",
      "\n",
      "| \n",
      "\n",
      "187\n",
      "\n",
      "\fInstability\n",
      "Hopefully by now you are convinced that Decision Trees have a lot going for them:\n",
      "they  are  simple  to  understand  and  interpret,  easy  to  use,  versatile,  and  powerful.\n",
      "However  they  do  have  a  few  limitations.  First,  as  you  may  have  noticed,  Decision\n",
      "Trees  love  orthogonal  decision  boundaries  (all  splits  are  perpendicular  to  an  axis),\n",
      "which makes them sensitive to training set rotation. For example, Figure 6-7 shows a\n",
      "simple linearly separable dataset: on the left, a Decision Tree can split it easily, while\n",
      "on the right, after the dataset is rotated by 45°, the decision boundary looks unneces‐\n",
      "sarily convoluted. Although both Decision Trees fit the training set perfectly, it is very\n",
      "likely that the model on the right will not generalize well. One way to limit this prob‐\n",
      "lem is to use PCA (see Chapter 8), which often results in a better orientation of the\n",
      "training data.\n",
      "\n",
      "Figure 6-7. Sensitivity to training set rotation\n",
      "\n",
      "More generally, the main issue with Decision Trees is that they are very sensitive to\n",
      "small variations in the training data. For example, if you just remove the widest Iris-\n",
      "Versicolor from the iris training set (the one with petals 4.8 cm long and 1.8 cm wide)\n",
      "and train a new Decision Tree, you may get the model represented in Figure 6-8. As\n",
      "you  can  see,  it  looks  very  different  from  the  previous  Decision  Tree  (Figure  6-2).\n",
      "Actually,  since  the  training  algorithm  used  by  Scikit-Learn  is  stochastic6  you  may\n",
      "get  very  different  models  even  on  the  same  training  data  (unless  you  set  the\n",
      "random_state hyperparameter).\n",
      "\n",
      "6 It randomly selects the set of features to evaluate at each node.\n",
      "\n",
      "188 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 6: Decision Trees\n",
      "\n",
      "\fFigure 6-8. Sensitivity to training set details\n",
      "\n",
      "Random Forests can limit this instability by averaging predictions over many trees, as\n",
      "we will see in the next chapter.\n",
      "\n",
      "Exercises\n",
      "\n",
      "1. What is the approximate depth of a Decision Tree trained (without restrictions)\n",
      "\n",
      "on a training set with 1 million instances?\n",
      "\n",
      "2. Is a node’s Gini impurity generally lower or greater than its parent’s? Is it gener‐\n",
      "\n",
      "ally lower/greater, or always lower/greater?\n",
      "\n",
      "3. If a Decision Tree is overfitting the training set, is it a good idea to try decreasing\n",
      "\n",
      "max_depth?\n",
      "\n",
      "4. If a Decision Tree is underfitting the training set, is it a good idea to try scaling\n",
      "\n",
      "the input features?\n",
      "\n",
      "5. If it takes one hour to train a Decision Tree on a training set containing 1 million\n",
      "instances, roughly how much time will it take to train another Decision Tree on a\n",
      "training set containing 10 million instances?\n",
      "\n",
      "6. If your training set contains 100,000 instances, will setting  presort=True speed\n",
      "\n",
      "up training?\n",
      "\n",
      "7. Train and fine-tune a Decision Tree for the moons dataset.\n",
      "\n",
      "a. Generate a moons dataset using make_moons(n_samples=10000, noise=0.4).\n",
      "b. Split it into a training set and a test set using train_test_split().\n",
      "\n",
      "Exercises \n",
      "\n",
      "| \n",
      "\n",
      "189\n",
      "\n",
      "\fc. Use  grid  search  with  cross-validation  (with  the  help  of  the  GridSearchCV\n",
      "class)  to  find  good  hyperparameter  values  for  a  DecisionTreeClassifier. \n",
      "Hint: try various values for max_leaf_nodes.\n",
      "\n",
      "d. Train  it  on  the  full  training  set  using  these  hyperparameters,  and  measure\n",
      "your model’s performance on the test set. You should get roughly 85% to 87%\n",
      "accuracy.\n",
      "\n",
      "8. Grow a forest.\n",
      "\n",
      "a. Continuing  the  previous  exercise,  generate  1,000  subsets  of  the  training  set,\n",
      "each  containing  100  instances  selected  randomly.  Hint:  you  can  use  Scikit-\n",
      "Learn’s ShuffleSplit class for this.\n",
      "\n",
      "b. Train one Decision Tree on each subset, using the best hyperparameter values\n",
      "found  above.  Evaluate  these  1,000  Decision  Trees  on  the  test  set.  Since  they\n",
      "were  trained  on  smaller  sets,  these  Decision  Trees  will  likely  perform  worse\n",
      "than the first Decision Tree, achieving only about 80% accuracy.\n",
      "\n",
      "c. Now comes the magic. For each test set instance, generate the predictions of\n",
      "the 1,000 Decision Trees, and keep only the most frequent prediction (you can\n",
      "use SciPy’s mode() function for this). This gives you majority-vote predictions\n",
      "over the test set.\n",
      "\n",
      "d. Evaluate these predictions on the test set: you should obtain a slightly higher\n",
      "accuracy  than  your  first  model  (about  0.5  to  1.5%  higher).  Congratulations,\n",
      "you have trained a Random Forest classifier!\n",
      "\n",
      "Solutions to these exercises are available in ???.\n",
      "\n",
      "190 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 6: Decision Trees\n",
      "\n",
      "\fCHAPTER 7\n",
      "Ensemble Learning and Random Forests\n",
      "\n",
      "With  Early  Release  ebooks,  you  get  books  in  their  earliest  form—\n",
      "the author’s raw and unedited content as he or she writes—so you\n",
      "can  take  advantage  of  these  technologies  long  before  the  official\n",
      "release of these titles. The following will be Chapter 7 in the final\n",
      "release of the book.\n",
      "\n",
      "Suppose you ask a complex question to thousands of random people, then aggregate\n",
      "their answers. In many cases you will find that this aggregated answer is better than\n",
      "an expert’s answer. This is called the wisdom of the crowd. Similarly, if you aggregate\n",
      "the  predictions  of  a  group  of  predictors  (such  as  classifiers  or  regressors),  you  will\n",
      "often get better predictions than with the best individual predictor. A group of pre‐\n",
      "dictors is called an ensemble; thus, this technique is called Ensemble Learning, and an\n",
      "Ensemble Learning algorithm is called an Ensemble method.\n",
      "\n",
      "For  example,  you  can  train  a  group  of  Decision  Tree  classifiers,  each  on  a  different\n",
      "random  subset  of  the  training  set.  To  make  predictions,  you  just  obtain  the  predic‐\n",
      "tions of all individual trees, then predict the class that gets the most votes (see the last\n",
      "exercise in Chapter 6). Such an ensemble of Decision Trees is called a Random Forest, \n",
      "and  despite  its  simplicity,  this  is  one  of  the  most  powerful  Machine  Learning  algo‐\n",
      "rithms available today.\n",
      "\n",
      "Moreover, as we discussed in Chapter 2, you will often use Ensemble methods near\n",
      "the end of a project, once you have already built a few good predictors, to combine\n",
      "them into an even better predictor. In fact, the winning solutions in Machine Learn‐\n",
      "ing competitions often involve several Ensemble methods (most famously in the Net‐\n",
      "flix Prize competition).\n",
      "\n",
      "In this chapter we will discuss the most popular Ensemble methods, including bag‐\n",
      "ging, boosting, stacking, and a few others. We will also explore Random Forests.\n",
      "\n",
      "191\n",
      "\n",
      "\fVoting Classifiers\n",
      "Suppose  you  have  trained  a  few  classifiers,  each  one  achieving  about  80%  accuracy.\n",
      "You  may  have  a  Logistic  Regression  classifier,  an  SVM  classifier,  a  Random  Forest\n",
      "classifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1).\n",
      "\n",
      "Figure 7-1. Training diverse classifiers\n",
      "\n",
      "A very simple way to create an even better classifier is to aggregate the predictions of\n",
      "each classifier and predict the class that gets the most votes. This majority-vote classi‐\n",
      "fier is called a hard voting classifier (see Figure 7-2).\n",
      "\n",
      "Figure 7-2. Hard voting classifier predictions\n",
      "\n",
      "192 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 7: Ensemble Learning and Random Forests\n",
      "\n",
      "\fSomewhat surprisingly, this voting classifier often achieves a higher accuracy than the\n",
      "best classifier in the ensemble. In fact, even if each classifier is a weak learner (mean‐\n",
      "ing  it  does  only  slightly  better  than  random  guessing),  the  ensemble  can  still  be  a\n",
      "strong  learner  (achieving  high  accuracy),  provided  there  are  a  sufficient  number  of\n",
      "weak learners and they are sufficiently diverse.\n",
      "\n",
      "How is this possible? The following analogy can help shed some light on this mystery.\n",
      "Suppose you have a slightly biased coin that has a 51% chance of coming up heads,\n",
      "and 49% chance of coming up tails. If you toss it 1,000 times, you will generally get\n",
      "more  or  less  510  heads  and  490  tails,  and  hence  a  majority  of  heads.  If  you  do  the\n",
      "math, you will find that the probability of obtaining a majority of heads after 1,000\n",
      "tosses  is  close  to  75%.  The  more  you  toss  the  coin,  the  higher  the  probability  (e.g.,\n",
      "with  10,000  tosses,  the  probability  climbs  over  97%).  This  is  due  to  the  law  of  large\n",
      "numbers: as you keep tossing the coin, the ratio of heads gets closer and closer to the\n",
      "probability of heads (51%). Figure 7-3 shows 10 series of biased coin tosses. You can\n",
      "see that as the number of tosses increases, the ratio of heads approaches 51%. Eventu‐\n",
      "ally all 10 series end up so close to 51% that they are consistently above 50%.\n",
      "\n",
      "Figure 7-3. The law of large numbers\n",
      "\n",
      "Similarly, suppose you build an ensemble containing 1,000 classifiers that are individ‐\n",
      "ually correct only 51% of the time (barely better than random guessing). If you pre‐\n",
      "dict the majority voted class, you can hope for up to 75% accuracy! However, this is\n",
      "only  true  if  all  classifiers  are  perfectly  independent,  making  uncorrelated  errors,\n",
      "which is clearly not the case since they are trained on the same data. They are likely to\n",
      "make  the  same  types  of  errors,  so  there  will  be  many  majority  votes  for  the  wrong\n",
      "class, reducing the ensemble’s accuracy.\n",
      "\n",
      "Voting Classifiers \n",
      "\n",
      "| \n",
      "\n",
      "193\n",
      "\n",
      "\fEnsemble methods work best when the predictors are as independ‐\n",
      "ent from one another as possible. One way to get diverse classifiers\n",
      "is to train them using very different algorithms. This increases the\n",
      "chance that they will make very different types of errors, improving\n",
      "the ensemble’s accuracy.\n",
      "\n",
      "The following code creates and trains a voting classifier in Scikit-Learn, composed of\n",
      "three  diverse  classifiers  (the  training  set  is  the  moons  dataset,  introduced  in  Chap‐\n",
      "ter 5):\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import VotingClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "log_clf = LogisticRegression()\n",
      "rnd_clf = RandomForestClassifier()\n",
      "svm_clf = SVC()\n",
      "\n",
      "voting_clf = VotingClassifier(\n",
      "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
      "    voting='hard')\n",
      "voting_clf.fit(X_train, y_train)\n",
      "\n",
      "Let’s look at each classifier’s accuracy on the test set:\n",
      "\n",
      ">>> from sklearn.metrics import accuracy_score\n",
      ">>> for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
      "...     clf.fit(X_train, y_train)\n",
      "...     y_pred = clf.predict(X_test)\n",
      "...     print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
      "...\n",
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.896\n",
      "SVC 0.888\n",
      "VotingClassifier 0.904\n",
      "\n",
      "There you have it! The voting classifier slightly outperforms all the individual classifi‐\n",
      "ers.\n",
      "\n",
      "If  all  classifiers  are  able  to  estimate  class  probabilities  (i.e.,  they  have  a  pre\n",
      "dict_proba()  method),  then  you  can  tell  Scikit-Learn  to  predict  the  class  with  the\n",
      "highest class probability, averaged over all the individual classifiers. This is called soft\n",
      "voting. It often achieves higher performance than hard voting because it gives more\n",
      "weight to highly confident votes. All you need to do is replace  voting=\"hard\" with\n",
      "voting=\"soft\" and ensure that all classifiers can estimate class probabilities. This is\n",
      "not the case of the SVC class by default, so you need to set its probability hyperpara‐\n",
      "meter to True (this will make the SVC class use cross-validation to estimate class prob‐\n",
      "abilities, slowing down training, and it will add a predict_proba() method). If you\n",
      "\n",
      "194 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 7: Ensemble Learning and Random Forests\n",
      "\n",
      "\fmodify the preceding code to use soft voting, you will find that the voting classifier\n",
      "achieves over 91.2% accuracy!\n",
      "\n",
      "Bagging and Pasting\n",
      "One way to get a diverse set of classifiers is to use very different training algorithms,\n",
      "as just discussed. Another approach is to use the same training algorithm for every\n",
      "predictor,  but  to  train  them  on  different  random  subsets  of  the  training  set.  When\n",
      "sampling  is  performed  with  replacement,  this  method  is  called  bagging1  (short  for\n",
      "bootstrap aggregating2). When sampling is performed without replacement, it is called\n",
      "pasting.3\n",
      "\n",
      "In other words, both bagging and pasting allow training instances to be sampled sev‐\n",
      "eral times across multiple predictors, but only bagging allows training instances to be\n",
      "sampled several times for the same predictor. This sampling and training process is\n",
      "represented in Figure 7-4.\n",
      "\n",
      "Figure 7-4. Pasting/bagging training set sampling and training\n",
      "\n",
      "Once  all  predictors  are  trained,  the  ensemble  can  make  a  prediction  for  a  new\n",
      "instance  by  simply  aggregating  the  predictions  of  all  predictors.  The  aggregation\n",
      "function is typically the statistical mode (i.e., the most frequent prediction, just like a\n",
      "hard voting classifier) for classification, or the average for regression. Each individual\n",
      "\n",
      "1 “Bagging Predictors,” L. Breiman (1996).\n",
      "\n",
      "2 In statistics, resampling with replacement is called bootstrapping.\n",
      "\n",
      "3 “Pasting small votes for classification in large databases and on-line,” L. Breiman (1999).\n",
      "\n",
      "Bagging and Pasting \n",
      "\n",
      "| \n",
      "\n",
      "195\n",
      "\n",
      "\fpredictor  has  a  higher  bias  than  if  it  were  trained  on  the  original  training  set,  but\n",
      "aggregation  reduces  both  bias  and  variance.4  Generally,  the  net  result  is  that  the\n",
      "ensemble has a similar bias but a lower variance than a single predictor trained on the\n",
      "original training set.\n",
      "\n",
      "As  you  can  see  in  Figure  7-4,  predictors  can  all  be  trained  in  parallel,  via  different\n",
      "CPU  cores  or  even  different  servers.  Similarly,  predictions  can  be  made  in  parallel.\n",
      "This is one of the reasons why bagging and pasting are such popular methods: they\n",
      "scale very well.\n",
      "\n",
      "Bagging and Pasting in Scikit-Learn\n",
      "Scikit-Learn offers a simple API for both bagging and pasting with the BaggingClas\n",
      "sifier  class  (or  BaggingRegressor  for  regression).  The  following  code  trains  an\n",
      "ensemble of 500 Decision Tree classifiers,5 each trained on 100 training instances ran‐\n",
      "domly sampled from the training set with replacement (this is an example of bagging,\n",
      "but if you want to use pasting instead, just set bootstrap=False). The n_jobs param‐\n",
      "eter  tells  Scikit-Learn  the  number  of  CPU  cores  to  use  for  training  and  predictions\n",
      "(–1 tells Scikit-Learn to use all available cores):\n",
      "\n",
      "from sklearn.ensemble import BaggingClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "bag_clf = BaggingClassifier(\n",
      "    DecisionTreeClassifier(), n_estimators=500,\n",
      "    max_samples=100, bootstrap=True, n_jobs=-1)\n",
      "bag_clf.fit(X_train, y_train)\n",
      "y_pred = bag_clf.predict(X_test)\n",
      "\n",
      "The  BaggingClassifier  automatically  performs  soft  voting\n",
      "instead of hard voting if the base classifier can estimate class proba‐\n",
      "bilities (i.e., if it has a predict_proba() method), which is the case\n",
      "with Decision Trees classifiers.\n",
      "\n",
      "Figure 7-5 compares the decision boundary of a single Decision Tree with the deci‐\n",
      "sion  boundary  of  a  bagging  ensemble  of  500  trees  (from  the  preceding  code),  both\n",
      "trained  on  the  moons  dataset.  As  you  can  see,  the  ensemble’s  predictions  will  likely\n",
      "generalize much better than the single Decision Tree’s predictions: the ensemble has a\n",
      "comparable bias but a smaller variance (it makes roughly the same number of errors\n",
      "on the training set, but the decision boundary is less irregular).\n",
      "\n",
      "4 Bias and variance were introduced in Chapter 4.\n",
      "5 max_samples can alternatively be set to a float between 0.0 and 1.0, in which case the max number of instances\n",
      "\n",
      "to sample is equal to the size of the training set times max_samples.\n",
      "\n",
      "196 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 7: Ensemble Learning and Random Forests\n",
      "\n",
      "\fFigure 7-5. A single Decision Tree versus a bagging ensemble of 500 trees\n",
      "\n",
      "Bootstrapping  introduces  a  bit  more  diversity  in  the  subsets  that  each  predictor  is\n",
      "trained on, so bagging ends up with a slightly higher bias than pasting, but this also\n",
      "means  that  predictors  end  up  being  less  correlated  so  the  ensemble’s  variance  is\n",
      "reduced. Overall, bagging often results in better models, which explains why it is gen‐\n",
      "erally preferred. However, if you have spare time and CPU power you can use cross-\n",
      "validation to evaluate both bagging and pasting and select the one that works best.\n",
      "\n",
      "Out-of-Bag Evaluation\n",
      "With bagging, some instances may be sampled several times for any given predictor,\n",
      "while others may not be sampled at all. By default a BaggingClassifier samples m\n",
      "training  instances  with  replacement  (bootstrap=True),  where  m  is  the  size  of  the\n",
      "training set. This means that only about 63% of the training instances are sampled on\n",
      "average for each predictor.6 The remaining 37% of the training instances that are not\n",
      "sampled  are  called  out-of-bag  (oob)  instances.  Note  that  they  are  not  the  same  37%\n",
      "for all predictors.\n",
      "\n",
      "Since a predictor never sees the oob instances during training, it can be evaluated on\n",
      "these  instances,  without  the  need  for  a  separate  validation  set.  You  can  evaluate  the\n",
      "ensemble itself by averaging out the oob evaluations of each predictor.\n",
      "\n",
      "In Scikit-Learn, you can set oob_score=True when creating a BaggingClassifier to\n",
      "request an automatic oob evaluation after training. The following code demonstrates\n",
      "this. The resulting evaluation score is available through the oob_score_ variable:\n",
      "\n",
      ">>> bag_clf = BaggingClassifier(\n",
      "...     DecisionTreeClassifier(), n_estimators=500,\n",
      "...     bootstrap=True, n_jobs=-1, oob_score=True)\n",
      "...\n",
      ">>> bag_clf.fit(X_train, y_train)\n",
      "\n",
      "6 As m grows, this ratio approaches 1 – exp(–1) ≈ 63.212%.\n",
      "\n",
      "Bagging and Pasting \n",
      "\n",
      "| \n",
      "\n",
      "197\n",
      "\n",
      "\f>>> bag_clf.oob_score_\n",
      "0.90133333333333332\n",
      "\n",
      "According to this oob evaluation, this BaggingClassifier is likely to achieve about\n",
      "90.1% accuracy on the test set. Let’s verify this:\n",
      "\n",
      ">>> from sklearn.metrics import accuracy_score\n",
      ">>> y_pred = bag_clf.predict(X_test)\n",
      ">>> accuracy_score(y_test, y_pred)\n",
      "0.91200000000000003\n",
      "\n",
      "We get 91.2% accuracy on the test set—close enough!\n",
      "\n",
      "The  oob  decision  function  for  each  training  instance  is  also  available  through  the\n",
      "oob_decision_function_  variable.  In  this  case  (since  the  base  estimator  has  a  pre\n",
      "dict_proba() method) the decision function returns the class probabilities for each\n",
      "training  instance.  For  example,  the  oob  evaluation  estimates  that  the  first  training\n",
      "instance  has  a  68.25%  probability  of  belonging  to  the  positive  class  (and  31.75%  of\n",
      "belonging to the negative class):\n",
      "\n",
      ">>> bag_clf.oob_decision_function_\n",
      "array([[0.31746032, 0.68253968],\n",
      "       [0.34117647, 0.65882353],\n",
      "       [1.        , 0.        ],\n",
      "       ...\n",
      "       [1.        , 0.        ],\n",
      "       [0.03108808, 0.96891192],\n",
      "       [0.57291667, 0.42708333]])\n",
      "\n",
      "Random Patches and Random Subspaces\n",
      "The  BaggingClassifier  class  supports  sampling  the  features  as  well.  This  is  con‐\n",
      "trolled by two hyperparameters: max_features and bootstrap_features. They work\n",
      "the  same  way  as  max_samples  and  bootstrap,  but  for  feature  sampling  instead  of\n",
      "instance  sampling.  Thus,  each  predictor  will  be  trained  on  a  random  subset  of  the\n",
      "input features.\n",
      "\n",
      "This is particularly useful when you are dealing with high-dimensional inputs (such\n",
      "as  images).  Sampling  both  training  instances  and  features  is  called  the  Random\n",
      "Patches method.7 Keeping all training instances (i.e., bootstrap=False and max_sam\n",
      "ples=1.0)  but  sampling  features  (i.e.,  bootstrap_features=True  and/or  max_fea\n",
      "tures smaller than 1.0) is called the Random Subspaces method.8\n",
      "\n",
      "7 “Ensembles on Random Patches,” G. Louppe and P. Geurts (2012).\n",
      "\n",
      "8 “The random subspace method for constructing decision forests,” Tin Kam Ho (1998).\n",
      "\n",
      "198 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 7: Ensemble Learning and Random Forests\n",
      "\n",
      "\fSampling features results in even more predictor diversity, trading a bit more bias for\n",
      "a lower variance.\n",
      "\n",
      "Random Forests\n",
      "As we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally\n",
      "trained via the bagging method (or sometimes pasting), typically with  max_samples\n",
      "set to the size of the training set. Instead of building a BaggingClassifier and pass‐\n",
      "ing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier\n",
      "class, which is more convenient and optimized for Decision Trees10 (similarly, there is\n",
      "a  RandomForestRegressor  class  for  regression  tasks).  The  following  code  trains  a\n",
      "Random Forest classifier with 500 trees (each limited to maximum 16 nodes), using\n",
      "all available CPU cores:\n",
      "\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
      "rnd_clf.fit(X_train, y_train)\n",
      "\n",
      "y_pred_rf = rnd_clf.predict(X_test)\n",
      "\n",
      "With a few exceptions, a RandomForestClassifier has all the hyperparameters of a\n",
      "DecisionTreeClassifier  (to  control  how  trees  are  grown),  plus  all  the  hyperpara‐\n",
      "meters of a BaggingClassifier to control the ensemble itself.11\n",
      "\n",
      "The  Random  Forest  algorithm  introduces  extra  randomness  when  growing  trees;\n",
      "instead of searching for the very best feature when splitting a node (see Chapter 6), it\n",
      "searches  for  the  best  feature  among  a  random  subset  of  features.  This  results  in  a\n",
      "greater  tree  diversity,  which  (once  again)  trades  a  higher  bias  for  a  lower  variance,\n",
      "generally  yielding  an  overall  better  model.  The  following  BaggingClassifier  is\n",
      "roughly equivalent to the previous RandomForestClassifier:\n",
      "\n",
      "bag_clf = BaggingClassifier(\n",
      "    DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\n",
      "    n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)\n",
      "\n",
      "9 “Random Decision Forests,” T. Ho (1995).\n",
      "10 The BaggingClassifier class remains useful if you want a bag of something other than Decision Trees.\n",
      "11 There are a few notable exceptions: splitter is absent (forced to \"random\"), presort is absent (forced to\n",
      "\n",
      "False), max_samples is absent (forced to 1.0), and base_estimator is absent (forced to DecisionTreeClassi\n",
      "fier with the provided hyperparameters).\n",
      "\n",
      "Random Forests \n",
      "\n",
      "| \n",
      "\n",
      "199\n",
      "\n",
      "\fExtra-Trees\n",
      "When you are growing a tree in a Random Forest, at each node only a random subset\n",
      "of the features is considered for splitting (as discussed earlier). It is possible to make\n",
      "trees even more random by also using random thresholds for each feature rather than\n",
      "searching for the best possible thresholds (like regular Decision Trees do).\n",
      "\n",
      "A  forest  of  such  extremely  random  trees  is  simply  called  an  Extremely  Randomized\n",
      "Trees  ensemble12  (or  Extra-Trees  for  short).  Once  again,  this  trades  more  bias  for  a\n",
      "lower variance. It also makes Extra-Trees much faster to train than regular Random\n",
      "Forests since finding the best possible threshold for each feature at every node is one\n",
      "of the most time-consuming tasks of growing a tree.\n",
      "\n",
      "You  can  create  an  Extra-Trees  classifier  using  Scikit-Learn’s  ExtraTreesClassifier\n",
      "class. Its API is identical to the RandomForestClassifier class. Similarly, the Extra\n",
      "TreesRegressor class has the same API as the RandomForestRegressor class.\n",
      "\n",
      "It  is  hard  to  tell  in  advance  whether  a  RandomForestClassifier\n",
      "will perform better or worse than an ExtraTreesClassifier. Gen‐\n",
      "erally, the only way to know is to try both and compare them using\n",
      "cross-validation  (and  tuning  the  hyperparameters  using  grid\n",
      "search).\n",
      "\n",
      "Feature Importance\n",
      "Yet another great quality of Random Forests is that they make it easy to measure the \n",
      "relative importance of each feature. Scikit-Learn measures a feature’s importance by\n",
      "looking at how much the tree nodes that use that feature reduce impurity on average\n",
      "(across  all  trees  in  the  forest).  More  precisely,  it  is  a  weighted  average,  where  each\n",
      "node’s  weight  is  equal  to  the  number  of  training  samples  that  are  associated  with  it\n",
      "(see Chapter 6).\n",
      "\n",
      "Scikit-Learn computes this score automatically for each feature after training, then it\n",
      "scales the results so that the sum of all importances is equal to 1. You can access the\n",
      "result  using  the  feature_importances_  variable.  For  example,  the  following  code\n",
      "trains a  RandomForestClassifier on the iris dataset (introduced in Chapter 4) and\n",
      "outputs each feature’s importance. It seems that the most important features are the\n",
      "petal length (44%) and width (42%), while sepal length and width are rather unim‐\n",
      "portant in comparison (11% and 2%, respectively).\n",
      "\n",
      "12 “Extremely randomized trees,” P. Geurts, D. Ernst, L. Wehenkel (2005).\n",
      "\n",
      "200 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 7: Ensemble Learning and Random Forests\n",
      "\n",
      "\f>>> from sklearn.datasets import load_iris\n",
      ">>> iris = load_iris()\n",
      ">>> rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
      ">>> rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
      ">>> for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
      "...     print(name, score)\n",
      "...\n",
      "sepal length (cm) 0.112492250999\n",
      "sepal width (cm) 0.0231192882825\n",
      "petal length (cm) 0.441030464364\n",
      "petal width (cm) 0.423357996355\n",
      "\n",
      "Similarly, if you train a Random Forest classifier on the MNIST dataset (introduced\n",
      "in  Chapter  3)  and  plot  each  pixel’s  importance,  you  get  the  image  represented  in\n",
      "Figure 7-6.\n",
      "\n",
      "Figure 7-6. MNIST pixel importance (according to a Random Forest classifier)\n",
      "\n",
      "Random  Forests  are  very  handy  to  get  a  quick  understanding  of  what  features\n",
      "actually matter, in particular if you need to perform feature selection.\n",
      "\n",
      "Boosting\n",
      "Boosting  (originally  called  hypothesis  boosting)  refers  to  any  Ensemble  method  that\n",
      "can  combine  several  weak  learners  into  a  strong  learner.  The  general  idea  of  most\n",
      "boosting methods is to train predictors sequentially, each trying to correct its prede‐\n",
      "cessor. There are many boosting methods available, but by far the most popular are\n",
      "\n",
      "Boosting \n",
      "\n",
      "| \n",
      "\n",
      "201\n",
      "\n",
      "\fAdaBoost13 (short for Adaptive Boosting) and Gradient Boosting. Let’s start with Ada‐\n",
      "Boost.\n",
      "\n",
      "AdaBoost\n",
      "One way for a new predictor to correct its predecessor is to pay a bit more attention\n",
      "to the training instances that the predecessor underfitted. This results in new predic‐\n",
      "tors focusing more and more on the hard cases. This is the technique used by Ada‐\n",
      "Boost.\n",
      "\n",
      "For example, to build an AdaBoost classifier, a first base classifier (such as a Decision\n",
      "Tree) is trained and used to make predictions on the training set. The relative weight\n",
      "of  misclassified  training  instances  is  then  increased.  A  second  classifier  is  trained\n",
      "using the updated weights and again it makes predictions on the training set, weights\n",
      "are updated, and so on (see Figure 7-7).\n",
      "\n",
      "Figure 7-7. AdaBoost sequential training with instance weight updates\n",
      "\n",
      "Figure  7-8  shows  the  decision  boundaries  of  five  consecutive  predictors  on  the\n",
      "moons dataset (in this example, each predictor is a highly regularized SVM classifier\n",
      "with an RBF kernel14). The first classifier gets many instances wrong, so their weights\n",
      "\n",
      "13 “A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting,” Yoav Freund,\n",
      "\n",
      "Robert E. Schapire (1997).\n",
      "\n",
      "14 This is just for illustrative purposes. SVMs are generally not good base predictors for AdaBoost, because they\n",
      "\n",
      "are slow and tend to be unstable with AdaBoost.\n",
      "\n",
      "202 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 7: Ensemble Learning and Random Forests\n",
      "\n",
      "\fget boosted. The second classifier therefore does a better job on these instances, and\n",
      "so  on.  The  plot  on  the  right  represents  the  same  sequence  of  predictors  except  that\n",
      "the learning rate is halved (i.e., the misclassified instance weights are boosted half as\n",
      "much at every iteration). As you can see, this sequential learning technique has some\n",
      "similarities with Gradient Descent, except that instead of tweaking a single predictor’s\n",
      "parameters to minimize a cost function, AdaBoost adds predictors to the ensemble,\n",
      "gradually making it better.\n",
      "\n",
      "Figure 7-8. Decision boundaries of consecutive predictors\n",
      "\n",
      "Once all predictors are trained, the ensemble makes predictions very much like bag‐\n",
      "ging  or  pasting,  except  that  predictors  have  different  weights  depending  on  their\n",
      "overall accuracy on the weighted training set.\n",
      "\n",
      "There is one important drawback to this sequential learning techni‐\n",
      "que: it cannot be parallelized (or only partially), since each predic‐\n",
      "tor  can  only  be  trained  after  the  previous  predictor  has  been\n",
      "trained and evaluated. As a result, it does not scale as well as bag‐\n",
      "ging or pasting.\n",
      "\n",
      "Let’s take a closer look at the AdaBoost algorithm. Each instance weight w(i) is initially\n",
      "set to  1\n",
      "m . A first predictor is trained and its weighted error rate r1 is computed on the\n",
      "training set; see Equation 7-1.\n",
      "\n",
      "Equation 7-1. Weighted error rate of the jth predictor\n",
      "\n",
      "w i\n",
      "\n",
      "m\n",
      "∑\n",
      "i = 1\n",
      "i\n",
      "j\n",
      "\n",
      "≠ y\n",
      "\n",
      "y\n",
      "\n",
      "i\n",
      "\n",
      "m\n",
      "∑\n",
      "i = 1\n",
      "\n",
      "w i\n",
      "\n",
      "r j =\n",
      "\n",
      "i\n",
      "where y j\n",
      "\n",
      "is the jth predictor’s prediction for the ith instance.\n",
      "\n",
      "Boosting \n",
      "\n",
      "| \n",
      "\n",
      "203\n",
      "\n",
      "\fThe predictor’s weight αj is then computed using Equation 7-2, where η is the learn‐\n",
      "ing  rate  hyperparameter  (defaults  to  1).15  The  more  accurate  the  predictor  is,  the\n",
      "higher its weight will be. If it is just guessing randomly, then its weight will be close to\n",
      "zero.  However,  if  it  is  most  often  wrong  (i.e.,  less  accurate  than  random  guessing),\n",
      "then its weight will be negative.\n",
      "\n",
      "Equation 7-2. Predictor weight\n",
      "\n",
      "α j = η log\n",
      "\n",
      "1 − r j\n",
      "r j\n",
      "\n",
      "Next the instance weights are updated using Equation 7-3: the misclassified instances\n",
      "are boosted.\n",
      "\n",
      "Equation 7-3. Weight update rule\n",
      "\n",
      "for i = 1, 2, ⋯, m\n",
      "w i\n",
      "w i exp α j\n",
      "\n",
      "w i\n",
      "\n",
      "if y j\n",
      "\n",
      "if y j\n",
      "\n",
      "i = y i\n",
      "\n",
      "i ≠ y i\n",
      "\n",
      "Then all the instance weights are normalized (i.e., divided by ∑i = 1\n",
      "\n",
      "m w i ).\n",
      "\n",
      "Finally, a new predictor is trained using the updated weights, and the whole process is\n",
      "repeated (the new predictor’s weight is computed, the instance weights are updated,\n",
      "then another predictor is trained, and so on). The algorithm stops when the desired\n",
      "number of predictors is reached, or when a perfect predictor is found.\n",
      "\n",
      "To make predictions, AdaBoost simply computes the predictions of all the predictors\n",
      "and  weighs  them  using  the  predictor  weights  αj.  The  predicted  class  is  the  one  that\n",
      "receives the majority of weighted votes (see Equation 7-4).\n",
      "\n",
      "Equation 7-4. AdaBoost predictions\n",
      "\n",
      "y x = argmax\n",
      "\n",
      "k\n",
      "\n",
      "N\n",
      "∑\n",
      "j = 1\n",
      "x = k\n",
      "\n",
      "y\n",
      "\n",
      "j\n",
      "\n",
      "α j where N is the number of predictors.\n",
      "\n",
      "15 The original AdaBoost algorithm does not use a learning rate hyperparameter.\n",
      "\n",
      "204 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 7: Ensemble Learning and Random Forests\n",
      "\n",
      "\fScikit-Learn  actually  uses  a  multiclass  version  of  AdaBoost  called  SAMME16  (which\n",
      "stands  for  Stagewise  Additive  Modeling  using  a  Multiclass  Exponential  loss  function).\n",
      "When there are just two classes, SAMME is equivalent to AdaBoost. Moreover, if the\n",
      "predictors  can  estimate  class  probabilities  (i.e.,  if  they  have  a  predict_proba()\n",
      "method),  Scikit-Learn  can  use  a  variant  of  SAMME  called  SAMME.R  (the  R  stands\n",
      "for  “Real”),  which  relies  on  class  probabilities  rather  than  predictions  and  generally\n",
      "performs better.\n",
      "\n",
      "The following code trains an AdaBoost classifier based on 200 Decision Stumps using\n",
      "Scikit-Learn’s  AdaBoostClassifier  class  (as  you  might  expect,  there  is  also  an  Ada\n",
      "BoostRegressor class). A Decision Stump is a Decision Tree with max_depth=1—in\n",
      "other words, a tree composed of a single decision node plus two leaf nodes. This is\n",
      "the default base estimator for the AdaBoostClassifier class:\n",
      "\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "\n",
      "ada_clf = AdaBoostClassifier(\n",
      "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
      "    algorithm=\"SAMME.R\", learning_rate=0.5)\n",
      "ada_clf.fit(X_train, y_train)\n",
      "\n",
      "If your AdaBoost ensemble is overfitting the training set, you can\n",
      "try reducing the number of estimators or more strongly regulariz‐\n",
      "ing the base estimator.\n",
      "\n",
      "Gradient Boosting\n",
      "Another very popular Boosting algorithm is Gradient Boosting.17 Just like AdaBoost,\n",
      "Gradient Boosting works by sequentially adding predictors to an ensemble, each one\n",
      "correcting its predecessor. However, instead of tweaking the instance weights at every\n",
      "iteration like AdaBoost does, this method tries to fit the new predictor to the residual\n",
      "errors made by the previous predictor.\n",
      "\n",
      "Let’s go through a simple regression example using Decision Trees as the base predic‐\n",
      "tors  (of  course  Gradient  Boosting  also  works  great  with  regression  tasks).  This  is\n",
      "called Gradient Tree Boosting, or Gradient Boosted Regression Trees (GBRT). First, let’s\n",
      "fit a DecisionTreeRegressor to the training set (for example, a noisy quadratic train‐\n",
      "ing set):\n",
      "\n",
      "16 For more details, see “Multi-Class AdaBoost,” J. Zhu et al. (2006).\n",
      "\n",
      "17 First introduced in “Arcing the Edge,” L. Breiman (1997), and further developed in the paper “Greedy Func‐\n",
      "\n",
      "tion Approximation: A Gradient Boosting Machine,” Jerome H. Friedman (1999).\n",
      "\n",
      "Boosting \n",
      "\n",
      "| \n",
      "\n",
      "205\n",
      "\n",
      "\ffrom sklearn.tree import DecisionTreeRegressor\n",
      "\n",
      "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
      "tree_reg1.fit(X, y)\n",
      "\n",
      "Now train a second DecisionTreeRegressor on the residual errors made by the first\n",
      "predictor:\n",
      "\n",
      "y2 = y - tree_reg1.predict(X)\n",
      "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
      "tree_reg2.fit(X, y2)\n",
      "\n",
      "Then we train a third regressor on the residual errors made by the second predictor:\n",
      "\n",
      "y3 = y2 - tree_reg2.predict(X)\n",
      "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
      "tree_reg3.fit(X, y3)\n",
      "\n",
      "Now we have an ensemble containing three trees. It can make predictions on a new\n",
      "instance simply by adding up the predictions of all the trees:\n",
      "\n",
      "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\n",
      "\n",
      "Figure 7-9 represents the predictions of these three trees in the left column, and the\n",
      "ensemble’s predictions in the right column. In the first row, the ensemble has just one\n",
      "tree, so its predictions are exactly the same as the first tree’s predictions. In the second\n",
      "row, a new tree is trained on the residual errors of the first tree. On the right you can\n",
      "see that the ensemble’s predictions are equal to the sum of the predictions of the first\n",
      "two trees. Similarly, in the third row another tree is trained on the residual errors of\n",
      "the  second  tree.  You  can  see  that  the  ensemble’s  predictions  gradually  get  better  as\n",
      "trees are added to the ensemble.\n",
      "\n",
      "A simpler way to train GBRT ensembles is to use Scikit-Learn’s GradientBoostingRe\n",
      "gressor class. Much like the RandomForestRegressor class, it has hyperparameters to\n",
      "control the growth of Decision Trees (e.g., max_depth, min_samples_leaf, and so on),\n",
      "as well as hyperparameters to control the ensemble training, such as the number of\n",
      "trees (n_estimators). The following code creates the same ensemble as the previous\n",
      "one:\n",
      "\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "\n",
      "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
      "gbrt.fit(X, y)\n",
      "\n",
      "206 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 7: Ensemble Learning and Random Forests\n",
      "\n",
      "\fFigure 7-9. Gradient Boosting\n",
      "\n",
      "The learning_rate hyperparameter scales the contribution of each tree. If you set it\n",
      "to a low value, such as 0.1, you will need more trees in the ensemble to fit the train‐\n",
      "ing set, but the predictions will usually generalize better. This is a regularization tech‐\n",
      "nique  called  shrinkage.  Figure  7-10  shows  two  GBRT  ensembles  trained  with  a  low\n",
      "learning  rate:  the  one  on  the  left  does  not  have  enough  trees  to  fit  the  training  set,\n",
      "while the one on the right has too many trees and overfits the training set.\n",
      "\n",
      "Boosting \n",
      "\n",
      "| \n",
      "\n",
      "207\n",
      "\n",
      "\fFigure 7-10. GBRT ensembles with not enough predictors (left) and too many (right)\n",
      "\n",
      "In order to find the optimal number of trees, you can use early stopping (see Chap‐\n",
      "ter  4).  A  simple  way  to  implement  this  is  to  use  the  staged_predict()  method:  it\n",
      "returns an iterator over the predictions made by the ensemble at each stage of train‐\n",
      "ing (with one tree, two trees, etc.). The following code trains a GBRT ensemble with\n",
      "120 trees, then measures the validation error at each stage of training to find the opti‐\n",
      "mal  number  of  trees,  and  finally  trains  another  GBRT  ensemble  using  the  optimal\n",
      "number of trees:\n",
      "\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import mean_squared_error\n",
      "\n",
      "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
      "\n",
      "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
      "gbrt.fit(X_train, y_train)\n",
      "\n",
      "errors = [mean_squared_error(y_val, y_pred)\n",
      "          for y_pred in gbrt.staged_predict(X_val)]\n",
      "bst_n_estimators = np.argmin(errors)\n",
      "\n",
      "gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators)\n",
      "gbrt_best.fit(X_train, y_train)\n",
      "\n",
      "The validation errors are represented on the left of Figure 7-11, and the best model’s\n",
      "predictions are represented on the right.\n",
      "\n",
      "208 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 7: Ensemble Learning and Random Forests\n",
      "\n",
      "\fFigure 7-11. Tuning the number of trees using early stopping\n",
      "\n",
      "It  is  also  possible  to  implement  early  stopping  by  actually  stopping  training  early\n",
      "(instead  of  training  a  large  number  of  trees  first  and  then  looking  back  to  find  the\n",
      "optimal  number).  You  can  do  so  by  setting  warm_start=True,  which  makes  Scikit-\n",
      "Learn  keep  existing  trees  when  the  fit()  method  is  called,  allowing  incremental\n",
      "training.  The  following  code  stops  training  when  the  validation  error  does  not\n",
      "improve for five iterations in a row:\n",
      "\n",
      "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
      "\n",
      "min_val_error = float(\"inf\")\n",
      "error_going_up = 0\n",
      "for n_estimators in range(1, 120):\n",
      "    gbrt.n_estimators = n_estimators\n",
      "    gbrt.fit(X_train, y_train)\n",
      "    y_pred = gbrt.predict(X_val)\n",
      "    val_error = mean_squared_error(y_val, y_pred)\n",
      "    if val_error < min_val_error:\n",
      "        min_val_error = val_error\n",
      "        error_going_up = 0\n",
      "    else:\n",
      "        error_going_up += 1\n",
      "        if error_going_up == 5:\n",
      "            break  # early stopping\n",
      "\n",
      "The  GradientBoostingRegressor  class  also  supports  a  subsample  hyperparameter,\n",
      "which specifies the fraction of training instances to be used for training each tree. For\n",
      "example, if subsample=0.25, then each tree is trained on 25% of the training instan‐\n",
      "ces, selected randomly. As you can probably guess by now, this trades a higher bias\n",
      "for a lower variance. It also speeds up training considerably. This technique is called\n",
      "Stochastic Gradient Boosting.\n",
      "\n",
      "Boosting \n",
      "\n",
      "| \n",
      "\n",
      "209\n",
      "\n",
      "\fIt  is  possible  to  use  Gradient  Boosting  with  other  cost  functions.\n",
      "This  is  controlled  by  the  loss  hyperparameter  (see  Scikit-Learn’s\n",
      "documentation for more details).\n",
      "\n",
      "It is worth noting that an optimized implementation of Gradient Boosting is available\n",
      "in the popular python library XGBoost, which stands for Extreme Gradient Boosting.\n",
      "This package was initially developed by Tianqi Chen as part of the Distributed (Deep)\n",
      "Machine Learning Community (DMLC), and it aims at being extremely fast, scalable\n",
      "and  portable.  In  fact,  XGBoost  is  often  an  important  component  of  the  winning\n",
      "entries in ML competitions. XGBoost’s API is quite similar to Scikit-Learn’s:\n",
      "\n",
      "import xgboost\n",
      "\n",
      "xgb_reg = xgboost.XGBRegressor()\n",
      "xgb_reg.fit(X_train, y_train)\n",
      "y_pred = xgb_reg.predict(X_val)\n",
      "\n",
      "XGBoost  also  offers  several  nice  features,  such  as  automatically  taking  care  of  early\n",
      "stopping:\n",
      "\n",
      "xgb_reg.fit(X_train, y_train,\n",
      "            eval_set=[(X_val, y_val)], early_stopping_rounds=2)\n",
      "y_pred = xgb_reg.predict(X_val)\n",
      "\n",
      "You should definitely check it out!\n",
      "\n",
      "Stacking\n",
      "The last Ensemble method we will discuss in this chapter is called stacking (short for\n",
      "stacked generalization).18 It is based on a simple idea: instead of using trivial functions\n",
      "(such  as  hard  voting)  to  aggregate  the  predictions  of  all  predictors  in  an  ensemble,\n",
      "why don’t we train a model to perform this aggregation? Figure 7-12 shows such an\n",
      "ensemble performing a regression task on a new instance. Each of the bottom three\n",
      "predictors predicts a different value (3.1, 2.7, and 2.9), and then the final predictor \n",
      "(called a blender, or a meta learner) takes these predictions as inputs and makes the\n",
      "final prediction (3.0).\n",
      "\n",
      "18 “Stacked Generalization,” D. Wolpert (1992).\n",
      "\n",
      "210 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 7: Ensemble Learning and Random Forests\n",
      "\n",
      "\fFigure 7-12. Aggregating predictions using a blending predictor\n",
      "\n",
      "To train the blender, a common approach is to use a hold-out set.19 Let’s see how it\n",
      "works. First, the training set is split in two subsets. The first subset is used to train the\n",
      "predictors in the first layer (see Figure 7-13).\n",
      "\n",
      "Figure 7-13. Training the first layer\n",
      "\n",
      "Next, the first layer predictors are used to make predictions on the second (held-out)\n",
      "set (see Figure 7-14). This ensures that the predictions are “clean,” since the predictors\n",
      "never saw these instances during training. Now for each instance in the hold-out set\n",
      "\n",
      "19 Alternatively, it is possible to use out-of-fold predictions. In some contexts this is called stacking, while using a\n",
      "\n",
      "hold-out set is called blending. However, for many people these terms are synonymous.\n",
      "\n",
      "Stacking \n",
      "\n",
      "| \n",
      "\n",
      "211\n",
      "\n",
      "\fthere are three predicted values. We can create a new training set using these predic‐\n",
      "ted  values  as  input  features  (which  makes  this  new  training  set  three-dimensional),\n",
      "and  keeping  the  target  values.  The  blender  is  trained  on  this  new  training  set,  so  it\n",
      "learns to predict the target value given the first layer’s predictions.\n",
      "\n",
      "Figure 7-14. Training the blender\n",
      "\n",
      "It is actually possible to train several different blenders this way (e.g., one using Lin‐\n",
      "ear Regression, another using Random Forest Regression, and so on): we get a whole\n",
      "layer of blenders. The trick is to split the training set into three subsets: the first one is\n",
      "used to train the first layer, the second one is used to create the training set used to\n",
      "train  the  second  layer  (using  predictions  made  by  the  predictors  of  the  first  layer),\n",
      "and the third one is used to create the training set to train the third layer (using pre‐\n",
      "dictions made by the predictors of the second layer). Once this is done, we can make\n",
      "a prediction for a new instance by going through each layer sequentially, as shown in\n",
      "Figure 7-15.\n",
      "\n",
      "212 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 7: Ensemble Learning and Random Forests\n",
      "\n",
      "\fFigure 7-15. Predictions in a multilayer stacking ensemble\n",
      "\n",
      "Unfortunately, Scikit-Learn does not support stacking directly, but it is not too hard\n",
      "to roll out your own implementation (see the following exercises). Alternatively, you\n",
      "can use an open source implementation such as brew (available at https://github.com/\n",
      "viisar/brew).\n",
      "\n",
      "Exercises\n",
      "\n",
      "1. If  you  have  trained  five  different  models  on  the  exact  same  training  data,  and\n",
      "they  all  achieve  95%  precision,  is  there  any  chance  that  you  can  combine  these\n",
      "models to get better results? If so, how? If not, why?\n",
      "\n",
      "2. What is the difference between hard and soft voting classifiers?\n",
      "\n",
      "3. Is it possible to speed up training of a bagging ensemble by distributing it across\n",
      "multiple  servers?  What  about  pasting  ensembles,  boosting  ensembles,  random\n",
      "forests, or stacking ensembles?\n",
      "\n",
      "4. What is the benefit of out-of-bag evaluation?\n",
      "\n",
      "5. What makes Extra-Trees more random than regular Random Forests? How can\n",
      "this extra randomness help? Are Extra-Trees slower or faster than regular Ran‐\n",
      "dom Forests?\n",
      "\n",
      "6. If  your  AdaBoost  ensemble  underfits  the  training  data,  what  hyperparameters\n",
      "\n",
      "should you tweak and how?\n",
      "\n",
      "Exercises \n",
      "\n",
      "| \n",
      "\n",
      "213\n",
      "\n",
      "\f7. If your Gradient Boosting ensemble overfits the training set, should you increase\n",
      "\n",
      "or decrease the learning rate?\n",
      "\n",
      "8. Load the MNIST data (introduced in Chapter 3), and split it into a training set, a\n",
      "validation set, and a test set (e.g., use 50,000 instances for training, 10,000 for val‐\n",
      "idation, and 10,000 for testing). Then train various classifiers, such as a Random\n",
      "Forest  classifier,  an  Extra-Trees  classifier,  and  an  SVM.  Next,  try  to  combine\n",
      "them into an ensemble that outperforms them all on the validation set, using a\n",
      "soft or hard voting classifier. Once you have found one, try it on the test set. How\n",
      "much better does it perform compared to the individual classifiers?\n",
      "\n",
      "9. Run the individual classifiers from the previous exercise to make predictions on\n",
      "the  validation  set,  and  create  a  new  training  set  with  the  resulting  predictions:\n",
      "each training instance is a vector containing the set of predictions from all your\n",
      "classifiers  for  an  image,  and  the  target  is  the  image’s  class.  Train  a  classifier  on\n",
      "this  new  training  set.  Congratulations,  you  have  just  trained  a  blender,  and\n",
      "together with the classifiers they form a stacking ensemble! Now let’s evaluate the\n",
      "ensemble on the test set. For each image in the test set, make predictions with all\n",
      "your classifiers, then feed the predictions to the blender to get the ensemble’s pre‐\n",
      "dictions. How does it compare to the voting classifier you trained earlier?\n",
      "\n",
      "Solutions to these exercises are available in ???.\n",
      "\n",
      "214 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 7: Ensemble Learning and Random Forests\n",
      "\n",
      "\fCHAPTER 8\n",
      "Dimensionality Reduction\n",
      "\n",
      "With  Early  Release  ebooks,  you  get  books  in  their  earliest  form—\n",
      "the author’s raw and unedited content as he or she writes—so you\n",
      "can  take  advantage  of  these  technologies  long  before  the  official\n",
      "release of these titles. The following will be Chapter 8 in the final\n",
      "release of the book.\n",
      "\n",
      "Many Machine Learning problems involve thousands or even millions of features for\n",
      "each training instance. Not only does this make training extremely slow, it can also\n",
      "make  it  much  harder  to  find  a  good  solution,  as  we  will  see.  This  problem  is  often\n",
      "referred to as the curse of dimensionality.\n",
      "\n",
      "Fortunately, in real-world problems, it is often possible to reduce the number of fea‐\n",
      "tures considerably, turning an intractable problem into a tractable one. For example,\n",
      "consider the MNIST images (introduced in Chapter 3): the pixels on the image bor‐\n",
      "ders  are  almost  always  white,  so  you  could  completely  drop  these  pixels  from  the\n",
      "training  set  without  losing  much  information.  Figure  7-6  confirms  that  these  pixels\n",
      "are utterly unimportant for the classification task. Moreover, two neighboring pixels\n",
      "are often highly correlated: if you merge them into a single pixel (e.g., by taking the\n",
      "mean of the two pixel intensities), you will not lose much information.\n",
      "\n",
      "215\n",
      "\n",
      "\fReducing  dimensionality  does  lose  some  information  (just  like\n",
      "compressing  an  image  to  JPEG  can  degrade  its  quality),  so  even\n",
      "though it will speed up training, it may also make your system per‐\n",
      "form slightly worse. It also makes your pipelines a bit more com‐\n",
      "plex  and  thus  harder  to  maintain.  So  you  should  first  try  to  train\n",
      "your system with the original data before considering using dimen‐\n",
      "sionality reduction if training is too slow. In some cases, however,\n",
      "reducing  the  dimensionality  of  the  training  data  may  filter  out\n",
      "some noise and unnecessary details and thus result in higher per‐\n",
      "formance (but in general it won’t; it will just speed up training).\n",
      "\n",
      "Apart  from  speeding  up  training,  dimensionality  reduction  is  also  extremely  useful\n",
      "for data visualization (or DataViz). Reducing the number of dimensions down to two\n",
      "(or three) makes it possible to plot a condensed view of a high-dimensional training\n",
      "set on a graph and often gain some important insights by visually detecting patterns,\n",
      "such as clusters. Moreover, DataViz is essential to communicate your conclusions to\n",
      "people  who  are  not  data  scientists,  in  particular  decision  makers  who  will  use  your\n",
      "results.\n",
      "\n",
      "In  this  chapter  we  will  discuss  the  curse  of  dimensionality  and  get  a  sense  of  what\n",
      "goes on in high-dimensional space. Then, we will present the two main approaches to\n",
      "dimensionality  reduction  (projection  and  Manifold  Learning),  and  we  will  go\n",
      "through three of the most popular dimensionality reduction techniques: PCA, Kernel\n",
      "PCA, and LLE.\n",
      "\n",
      "The Curse of Dimensionality\n",
      "We are so used to living in three dimensions1 that our intuition fails us when we try\n",
      "to imagine a high-dimensional space. Even a basic 4D hypercube is incredibly hard to\n",
      "picture in our mind (see Figure 8-1), let alone a 200-dimensional ellipsoid bent in a\n",
      "1,000-dimensional space.\n",
      "\n",
      "1 Well, four dimensions if you count time, and a few more if you are a string theorist.\n",
      "\n",
      "216 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Dimensionality Reduction\n",
      "\n",
      "\fFigure 8-1. Point, segment, square, cube, and tesseract (0D to 4D hypercubes)2\n",
      "\n",
      "It turns out that many things behave very differently in high-dimensional space. For\n",
      "example, if you pick a random point in a unit square (a 1 × 1 square), it will have only\n",
      "about a 0.4% chance of being located less than 0.001 from a border (in other words, it\n",
      "is very unlikely that a random point will be “extreme” along any dimension). But in a\n",
      "10,000-dimensional unit hypercube (a 1 × 1 × ⋯ × 1 cube, with ten thousand 1s), this\n",
      "probability is greater than 99.999999%. Most points in a high-dimensional hypercube\n",
      "are very close to the border.3\n",
      "\n",
      "Here  is  a  more  troublesome  difference:  if  you  pick  two  points  randomly  in  a  unit\n",
      "square, the distance between these two points will be, on average, roughly 0.52. If you\n",
      "pick two random points in a unit 3D cube, the average distance will be roughly 0.66.\n",
      "But what about two points picked randomly in a 1,000,000-dimensional hypercube?\n",
      "Well,  the  average  distance,  believe  it  or  not,  will  be  about  408.25  (roughly\n",
      "1, 000, 000/6)!  This  is  quite  counterintuitive:  how  can  two  points  be  so  far  apart\n",
      "when  they  both  lie  within  the  same  unit  hypercube?  This  fact  implies  that  high-\n",
      "dimensional  datasets  are  at  risk  of  being  very  sparse:  most  training  instances  are\n",
      "likely to be far away from each other. Of course, this also means that a new instance\n",
      "will likely be far away from any training instance, making predictions much less relia‐\n",
      "ble than in lower dimensions, since they will be based on much larger extrapolations.\n",
      "In short, the more dimensions the training set has, the greater the risk of overfitting\n",
      "it.\n",
      "\n",
      "In theory, one solution to the curse of dimensionality could be to increase the size of\n",
      "the  training  set  to  reach  a  sufficient  density  of  training  instances.  Unfortunately,  in\n",
      "practice,  the  number  of  training  instances  required  to  reach  a  given  density  grows\n",
      "exponentially with the number of dimensions. With just 100 features (much less than\n",
      "\n",
      "2 Watch a rotating tesseract projected into 3D space at https://homl.info/30. Image by Wikipedia user Nerd‐\n",
      "\n",
      "Boy1392 (Creative Commons BY-SA 3.0). Reproduced from https://en.wikipedia.org/wiki/Tesseract.\n",
      "\n",
      "3 Fun fact: anyone you know is probably an extremist in at least one dimension (e.g., how much sugar they put\n",
      "\n",
      "in their coffee), if you consider enough dimensions.\n",
      "\n",
      "The Curse of Dimensionality \n",
      "\n",
      "| \n",
      "\n",
      "217\n",
      "\n",
      "\fin the MNIST problem), you would need more training instances than atoms in the\n",
      "observable universe in order for training instances to be within 0.1 of each other on\n",
      "average, assuming they were spread out uniformly across all dimensions.\n",
      "\n",
      "Main Approaches for Dimensionality Reduction\n",
      "Before we dive into specific dimensionality reduction algorithms, let’s take a look at\n",
      "the  two  main  approaches  to  reducing  dimensionality:  projection  and  Manifold\n",
      "Learning.\n",
      "\n",
      "Projection\n",
      "In most real-world problems, training instances are not spread out uniformly across\n",
      "all dimensions. Many features are almost constant, while others are highly correlated\n",
      "(as discussed earlier for MNIST). As a result, all training instances actually lie within\n",
      "(or close to) a much lower-dimensional subspace of the high-dimensional space. This\n",
      "sounds very abstract, so let’s look at an example. In Figure 8-2 you can see a 3D data‐\n",
      "set represented by the circles.\n",
      "\n",
      "Figure 8-2. A 3D dataset lying close to a 2D subspace\n",
      "\n",
      "Notice that all training instances lie close to a plane: this is a lower-dimensional (2D)\n",
      "subspace  of  the  high-dimensional  (3D)  space.  Now  if  we  project  every  training\n",
      "instance  perpendicularly  onto  this  subspace  (as  represented  by  the  short  lines  con‐\n",
      "necting the instances to the plane), we get the new 2D dataset shown in Figure 8-3.\n",
      "Ta-da!  We  have  just  reduced  the  dataset’s  dimensionality  from  3D  to  2D.  Note  that\n",
      "the axes correspond to new features z1 and z2 (the coordinates of the projections on\n",
      "the plane).\n",
      "\n",
      "218 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Dimensionality Reduction\n",
      "\n",
      "\fFigure 8-3. The new 2D dataset after projection\n",
      "\n",
      "However, projection is not always the best approach to dimensionality reduction. In\n",
      "many cases the subspace may twist and turn, such as in the famous Swiss roll toy data‐\n",
      "set represented in Figure 8-4.\n",
      "\n",
      "Figure 8-4. Swiss roll dataset\n",
      "\n",
      "Main Approaches for Dimensionality Reduction \n",
      "\n",
      "| \n",
      "\n",
      "219\n",
      "\n",
      "\fSimply projecting onto a plane (e.g., by dropping x3) would squash different layers of\n",
      "the Swiss roll together, as shown on the left of Figure 8-5. However, what you really\n",
      "want is to unroll the Swiss roll to obtain the 2D dataset on the right of Figure 8-5.\n",
      "\n",
      "Figure 8-5. Squashing by projecting onto a plane (left) versus unrolling the Swiss roll\n",
      "(right)\n",
      "\n",
      "Manifold Learning\n",
      "The  Swiss  roll  is  an  example  of  a  2D  manifold.  Put  simply,  a  2D  manifold  is  a  2D\n",
      "shape that can be bent and twisted in a higher-dimensional space. More generally, a\n",
      "d-dimensional manifold is a part of an n-dimensional space (where d < n) that locally\n",
      "resembles a d-dimensional hyperplane. In the case of the Swiss roll, d = 2 and n = 3: it\n",
      "locally resembles a 2D plane, but it is rolled in the third dimension.\n",
      "\n",
      "Many dimensionality reduction algorithms work by modeling the manifold on which\n",
      "the  training  instances  lie;  this  is  called  Manifold  Learning.  It  relies  on  the  manifold\n",
      "assumption,  also  called  the  manifold  hypothesis,  which  holds  that  most  real-world\n",
      "high-dimensional  datasets  lie  close  to  a  much  lower-dimensional  manifold.  This\n",
      "assumption is very often empirically observed.\n",
      "\n",
      "Once again, think about the MNIST dataset: all handwritten digit images have some\n",
      "similarities. They are made of connected lines, the borders are white, they are more\n",
      "or  less  centered,  and  so  on.  If  you  randomly  generated  images,  only  a  ridiculously\n",
      "tiny fraction of them would look like handwritten digits. In other words, the degrees\n",
      "of freedom available to you if you try to create a digit image are dramatically lower\n",
      "than  the  degrees  of  freedom  you  would  have  if  you  were  allowed  to  generate  any\n",
      "image  you  wanted.  These  constraints  tend  to  squeeze  the  dataset  into  a  lower-\n",
      "dimensional manifold.\n",
      "\n",
      "The manifold assumption is often accompanied by another implicit assumption: that\n",
      "the task at hand (e.g., classification or regression) will be simpler if expressed in the\n",
      "lower-dimensional space of the manifold. For example, in the top row of Figure 8-6\n",
      "the  Swiss  roll  is  split  into  two  classes:  in  the  3D  space  (on  the  left),  the  decision\n",
      "\n",
      "220 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Dimensionality Reduction\n",
      "\n",
      "\fboundary  would  be  fairly  complex,  but  in  the  2D  unrolled  manifold  space  (on  the\n",
      "right), the decision boundary is a simple straight line.\n",
      "\n",
      "However, this assumption does not always hold. For example, in the bottom row of\n",
      "Figure 8-6, the decision boundary is located at x1 = 5. This decision boundary looks\n",
      "very simple in the original 3D space (a vertical plane), but it looks more complex in\n",
      "the unrolled manifold (a collection of four independent line segments).\n",
      "\n",
      "In  short,  if  you  reduce  the  dimensionality  of  your  training  set  before  training  a\n",
      "model, it will usually speed up training, but it may not always lead to a better or sim‐\n",
      "pler solution; it all depends on the dataset.\n",
      "\n",
      "Hopefully you now have a good sense of what the curse of dimensionality is and how\n",
      "dimensionality  reduction  algorithms  can  fight  it,  especially  when  the  manifold\n",
      "assumption holds. The rest of this chapter will go through some of the most popular\n",
      "algorithms.\n",
      "\n",
      "Figure 8-6. The decision boundary may not always be simpler with lower dimensions\n",
      "\n",
      "Main Approaches for Dimensionality Reduction \n",
      "\n",
      "| \n",
      "\n",
      "221\n",
      "\n",
      "\fPCA\n",
      "Principal Component Analysis (PCA) is by far the most popular dimensionality reduc‐\n",
      "tion algorithm. First it identifies the hyperplane that lies closest to the data, and then\n",
      "it projects the data onto it, just like in Figure 8-2.\n",
      "\n",
      "Preserving the Variance\n",
      "Before  you  can  project  the  training  set  onto  a  lower-dimensional  hyperplane,  you\n",
      "first need to choose the right hyperplane. For example, a simple 2D dataset is repre‐\n",
      "sented on the left of Figure 8-7, along with three different axes (i.e., one-dimensional\n",
      "hyperplanes). On the right is the result of the projection of the dataset onto each of\n",
      "these axes. As you can see, the projection onto the solid line preserves the maximum\n",
      "variance, while the projection onto the dotted line preserves very little variance, and\n",
      "the projection onto the dashed line preserves an intermediate amount of variance.\n",
      "\n",
      "Figure 8-7. Selecting the subspace onto which to project\n",
      "\n",
      "It  seems  reasonable  to  select  the  axis  that  preserves  the  maximum  amount  of  var‐\n",
      "iance, as it will most likely lose less information than the other projections. Another\n",
      "way to justify this choice is that it is the axis that minimizes the mean squared dis‐\n",
      "tance between the original dataset and its projection onto that axis. This is the rather\n",
      "simple idea behind PCA.4\n",
      "\n",
      "4 “On Lines and Planes of Closest Fit to Systems of Points in Space,” K. Pearson (1901).\n",
      "\n",
      "222 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Dimensionality Reduction\n",
      "\n",
      "\fPrincipal Components\n",
      "PCA identifies the axis that accounts for the largest amount of variance in the train‐\n",
      "ing set. In Figure 8-7, it is the solid line. It also finds a second axis, orthogonal to the\n",
      "first  one,  that  accounts  for  the  largest  amount  of  remaining  variance.  In  this  2D\n",
      "example there is no choice: it is the dotted line. If it were a higher-dimensional data‐\n",
      "set, PCA would also find a third axis, orthogonal to both previous axes, and a fourth,\n",
      "a fifth, and so on—as many axes as the number of dimensions in the dataset.\n",
      "\n",
      "The unit vector that defines the ith axis is called the ith principal component (PC). In\n",
      "Figure  8-7,  the  1st  PC  is  c1  and  the  2nd  PC  is  c2.  In  Figure  8-2  the  first  two  PCs  are\n",
      "represented  by  the  orthogonal  arrows  in  the  plane,  and  the  third  PC  would  be\n",
      "orthogonal to the plane (pointing up or down).\n",
      "\n",
      "The direction of the principal components is not stable: if you per‐\n",
      "turb the training set slightly and run PCA again, some of the new\n",
      "PCs may point in the opposite direction of the original PCs. How‐\n",
      "ever, they will generally still lie on the same axes. In some cases, a\n",
      "pair of PCs may even rotate or swap, but the plane they define will\n",
      "generally remain the same.\n",
      "\n",
      "So  how  can  you  find  the  principal  components  of  a  training  set?  Luckily,  there  is  a\n",
      "standard  matrix  factorization  technique  called  Singular  Value  Decomposition  (SVD)\n",
      "that can decompose the training set matrix X into the matrix multiplication of three\n",
      "matrices U Σ VT, where V contains all the principal components that we are looking\n",
      "for, as shown in Equation 8-1.\n",
      "\n",
      "Equation 8-1. Principal components matrix\n",
      "\n",
      "V =\n",
      "\n",
      "∣\n",
      "∣\n",
      "c1 c2\n",
      "∣\n",
      "∣\n",
      "\n",
      "∣\n",
      "⋯ cn\n",
      "∣\n",
      "\n",
      "The following Python code uses NumPy’s svd() function to obtain all the principal\n",
      "components of the training set, then extracts the first two PCs:\n",
      "\n",
      "X_centered = X - X.mean(axis=0)\n",
      "U, s, Vt = np.linalg.svd(X_centered)\n",
      "c1 = Vt.T[:, 0]\n",
      "c2 = Vt.T[:, 1]\n",
      "\n",
      "PCA \n",
      "\n",
      "| \n",
      "\n",
      "223\n",
      "\n",
      "\fPCA assumes that the dataset is centered around the origin. As we\n",
      "will  see,  Scikit-Learn’s  PCA  classes  take  care  of  centering  the  data\n",
      "for  you.  However,  if  you  implement  PCA  yourself  (as  in  the  pre‐\n",
      "ceding example), or if you use other libraries, don’t forget to center\n",
      "the data first.\n",
      "\n",
      "Projecting Down to d Dimensions\n",
      "Once  you  have  identified  all  the  principal  components,  you  can  reduce  the  dimen‐\n",
      "sionality  of  the  dataset  down  to  d  dimensions  by  projecting  it  onto  the  hyperplane\n",
      "defined by the first d principal components. Selecting this hyperplane ensures that the\n",
      "projection will preserve as much variance as possible. For example, in Figure 8-2 the\n",
      "3D dataset is projected down to the 2D plane defined by the first two principal com‐\n",
      "ponents, preserving a large part of the dataset’s variance. As a result, the 2D projec‐\n",
      "tion looks very much like the original 3D dataset.\n",
      "\n",
      "To project the training set onto the hyperplane, you can simply compute the matrix\n",
      "multiplication  of  the  training  set  matrix  X  by  the  matrix  Wd,  defined  as  the  matrix\n",
      "containing the first d principal components (i.e., the matrix composed of the first d\n",
      "columns of V), as shown in Equation 8-2.\n",
      "\n",
      "Equation 8-2. Projecting the training set down to d dimensions\n",
      "\n",
      "Xd‐proj = XWd\n",
      "\n",
      "The following Python code projects the training set onto the plane defined by the first\n",
      "two principal components:\n",
      "\n",
      "W2 = Vt.T[:, :2]\n",
      "X2D = X_centered.dot(W2)\n",
      "\n",
      "There  you  have  it!  You  now  know  how  to  reduce  the  dimensionality  of  any  dataset\n",
      "down to any number of dimensions, while preserving as much variance as possible.\n",
      "\n",
      "Using Scikit-Learn\n",
      "Scikit-Learn’s  PCA  class  implements  PCA  using  SVD  decomposition  just  like  we  did\n",
      "before.  The  following  code  applies  PCA  to  reduce  the  dimensionality  of  the  dataset\n",
      "down to two dimensions (note that it automatically takes care of centering the data):\n",
      "\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components = 2)\n",
      "X2D = pca.fit_transform(X)\n",
      "\n",
      "After fitting the PCA transformer to the dataset, you can access the principal compo‐\n",
      "nents using the components_ variable (note that it contains the PCs as horizontal vec‐\n",
      "\n",
      "224 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Dimensionality Reduction\n",
      "\n",
      "\ftors, so, for example, the first principal component is equal to pca.components_.T[:,\n",
      "0]).\n",
      "\n",
      "Explained Variance Ratio\n",
      "Another very useful piece of information is the explained variance ratio of each prin‐\n",
      "cipal component, available via the explained_variance_ratio_ variable. It indicates\n",
      "the proportion of the dataset’s variance that lies along the axis of each principal com‐\n",
      "ponent. For example, let’s look at the explained variance ratios of the first two compo‐\n",
      "nents of the 3D dataset represented in Figure 8-2:\n",
      "\n",
      ">>> pca.explained_variance_ratio_\n",
      "array([0.84248607, 0.14631839])\n",
      "\n",
      "This tells you that 84.2% of the dataset’s variance lies along the first axis, and 14.6%\n",
      "lies along the second axis. This leaves less than 1.2% for the third axis, so it is reason‐\n",
      "able to assume that it probably carries little information.\n",
      "\n",
      "Choosing the Right Number of Dimensions\n",
      "Instead  of  arbitrarily  choosing  the  number  of  dimensions  to  reduce  down  to,  it  is\n",
      "generally preferable to choose the number of dimensions that add up to a sufficiently\n",
      "large portion of the variance (e.g., 95%). Unless, of course, you are reducing dimen‐\n",
      "sionality  for  data  visualization—in  that  case  you  will  generally  want  to  reduce  the\n",
      "dimensionality down to 2 or 3.\n",
      "\n",
      "The following code computes PCA without reducing dimensionality, then computes\n",
      "the  minimum  number  of  dimensions  required  to  preserve  95%  of  the  training  set’s\n",
      "variance:\n",
      "\n",
      "pca = PCA()\n",
      "pca.fit(X_train)\n",
      "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
      "d = np.argmax(cumsum >= 0.95) + 1\n",
      "\n",
      "You  could  then  set  n_components=d  and  run  PCA  again.  However,  there  is  a  much\n",
      "better option: instead of specifying the number of principal components you want to\n",
      "preserve, you can set n_components to be a float between 0.0 and 1.0, indicating the\n",
      "ratio of variance you wish to preserve:\n",
      "\n",
      "pca = PCA(n_components=0.95)\n",
      "X_reduced = pca.fit_transform(X_train)\n",
      "\n",
      "Yet  another  option  is  to  plot  the  explained  variance  as  a  function  of  the  number  of\n",
      "dimensions (simply plot cumsum; see Figure 8-8). There will usually be an elbow in the\n",
      "curve, where the explained variance stops growing fast. You can think of this as the\n",
      "intrinsic  dimensionality  of  the  dataset.  In  this  case,  you  can  see  that  reducing  the\n",
      "\n",
      "PCA \n",
      "\n",
      "| \n",
      "\n",
      "225\n",
      "\n",
      "\fdimensionality down to about 100 dimensions wouldn’t lose too much explained var‐\n",
      "iance.\n",
      "\n",
      "Figure 8-8. Explained variance as a function of the number of dimensions\n",
      "\n",
      "PCA for Compression\n",
      "Obviously after dimensionality reduction, the training set takes up much less space.\n",
      "For example, try applying PCA to the MNIST dataset while preserving 95% of its var‐\n",
      "iance. You should find that each instance will have just over 150 features, instead of\n",
      "the  original  784  features.  So  while  most  of  the  variance  is  preserved,  the  dataset  is\n",
      "now less than 20% of its original size! This is a reasonable compression ratio, and you\n",
      "can see how this can speed up a classification algorithm (such as an SVM classifier)\n",
      "tremendously.\n",
      "\n",
      "It  is  also  possible  to  decompress  the  reduced  dataset  back  to  784  dimensions  by\n",
      "applying the inverse transformation of the PCA projection. Of course this won’t give\n",
      "you back the original data, since the projection lost a bit of information (within the\n",
      "5%  variance  that  was  dropped),  but  it  will  likely  be  quite  close  to  the  original  data.\n",
      "The  mean  squared  distance  between  the  original  data  and  the  reconstructed  data\n",
      "(compressed and then decompressed) is called the reconstruction error. For example,\n",
      "the following code compresses the MNIST dataset down to 154 dimensions, then uses\n",
      "the  inverse_transform()  method  to  decompress  it  back  to  784  dimensions.\n",
      "Figure 8-9 shows a few digits from the original training set (on the left), and the cor‐\n",
      "responding digits after compression and decompression. You can see that there is a\n",
      "slight image quality loss, but the digits are still mostly intact.\n",
      "\n",
      "pca = PCA(n_components = 154)\n",
      "X_reduced = pca.fit_transform(X_train)\n",
      "X_recovered = pca.inverse_transform(X_reduced)\n",
      "\n",
      "226 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Dimensionality Reduction\n",
      "\n",
      "\fFigure 8-9. MNIST compression preserving 95% of the variance\n",
      "\n",
      "The equation of the inverse transformation is shown in Equation 8-3.\n",
      "\n",
      "Equation 8-3. PCA inverse transformation, back to the original number of\n",
      "dimensions\n",
      "\n",
      "Xrecovered = Xd‐projWd\n",
      "\n",
      "T\n",
      "\n",
      "Randomized PCA\n",
      "If you set the svd_solver hyperparameter to \"randomized\", Scikit-Learn uses a sto‐\n",
      "chastic algorithm called Randomized PCA that quickly finds an approximation of the\n",
      "first  d  principal  components.  Its  computational  complexity  is  O(m  ×  d2)  +  O(d3),\n",
      "instead  of  O(m  ×  n2)  +  O(n3)  for  the  full  SVD  approach,  so  it  is  dramatically  faster\n",
      "than full SVD when d is much smaller than n:\n",
      "\n",
      "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\")\n",
      "X_reduced = rnd_pca.fit_transform(X_train)\n",
      "\n",
      "By default, svd_solver is actually set to \"auto\": Scikit-Learn automatically uses the\n",
      "randomized PCA algorithm if m or n is greater than 500 and d is less than 80% of m\n",
      "or n, or else it uses the full SVD approach. If you want to force Scikit-Learn to use full\n",
      "SVD, you can set the svd_solver hyperparameter to \"full\".\n",
      "\n",
      "Incremental PCA\n",
      "One  problem  with  the  preceding  implementations  of  PCA  is  that  they  require  the\n",
      "whole  training  set  to  fit  in  memory  in  order  for  the  algorithm  to  run.  Fortunately,\n",
      "Incremental PCA (IPCA) algorithms have been developed: you can split the training\n",
      "set into mini-batches and feed an IPCA algorithm one mini-batch at a time. This is\n",
      "\n",
      "PCA \n",
      "\n",
      "| \n",
      "\n",
      "227\n",
      "\n",
      "\fuseful  for  large  training  sets,  and  also  to  apply  PCA  online  (i.e.,  on  the  fly,  as  new\n",
      "instances arrive).\n",
      "\n",
      "The following code splits the MNIST dataset into 100 mini-batches (using NumPy’s\n",
      "array_split() function) and feeds them to Scikit-Learn’s IncrementalPCA class5 to \n",
      "reduce  the  dimensionality  of  the  MNIST  dataset  down  to  154  dimensions  (just  like\n",
      "before).  Note  that  you  must  call  the  partial_fit()  method  with  each  mini-batch\n",
      "rather than the fit() method with the whole training set:\n",
      "\n",
      "from sklearn.decomposition import IncrementalPCA\n",
      "\n",
      "n_batches = 100\n",
      "inc_pca = IncrementalPCA(n_components=154)\n",
      "for X_batch in np.array_split(X_train, n_batches):\n",
      "    inc_pca.partial_fit(X_batch)\n",
      "\n",
      "X_reduced = inc_pca.transform(X_train)\n",
      "\n",
      "Alternatively,  you  can  use  NumPy’s  memmap  class,  which  allows  you  to  manipulate  a\n",
      "large array stored in a binary file on disk as if it were entirely in memory; the class\n",
      "loads only the data it needs in memory, when it needs it. Since the IncrementalPCA\n",
      "class uses only a small part of the array at any given time, the memory usage remains\n",
      "under control. This makes it possible to call the usual fit() method, as you can see\n",
      "in the following code:\n",
      "\n",
      "X_mm = np.memmap(filename, dtype=\"float32\", mode=\"readonly\", shape=(m, n))\n",
      "\n",
      "batch_size = m // n_batches\n",
      "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
      "inc_pca.fit(X_mm)\n",
      "\n",
      "Kernel PCA\n",
      "In Chapter 5 we discussed the kernel trick, a mathematical technique that implicitly\n",
      "maps instances into a very high-dimensional space (called the feature space), enabling\n",
      "nonlinear  classification  and  regression  with  Support  Vector  Machines.  Recall  that  a\n",
      "linear  decision  boundary  in  the  high-dimensional  feature  space  corresponds  to  a\n",
      "complex nonlinear decision boundary in the original space.\n",
      "\n",
      "It turns out that the same trick can be applied to PCA, making it possible to perform\n",
      "complex  nonlinear  projections  for  dimensionality  reduction.  This  is  called  Kernel\n",
      "\n",
      "5 Scikit-Learn uses the algorithm described in “Incremental Learning for Robust Visual Tracking,” D. Ross et al.\n",
      "\n",
      "(2007).\n",
      "\n",
      "228 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Dimensionality Reduction\n",
      "\n",
      "\fPCA (kPCA).6 It is often good at preserving clusters of instances after projection, or\n",
      "sometimes even unrolling datasets that lie close to a twisted manifold.\n",
      "\n",
      "For example, the following code uses Scikit-Learn’s KernelPCA class to perform kPCA\n",
      "with  an  RBF  kernel  (see  Chapter  5  for  more  details  about  the  RBF  kernel  and  the\n",
      "other kernels):\n",
      "\n",
      "from sklearn.decomposition import KernelPCA\n",
      "\n",
      "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04)\n",
      "X_reduced = rbf_pca.fit_transform(X)\n",
      "\n",
      "Figure  8-10  shows  the  Swiss  roll,  reduced  to  two  dimensions  using  a  linear  kernel\n",
      "(equivalent  to  simply  using  the  PCA  class),  an  RBF  kernel,  and  a  sigmoid  kernel\n",
      "(Logistic).\n",
      "\n",
      "Figure 8-10. Swiss roll reduced to 2D using kPCA with various kernels\n",
      "\n",
      "Selecting a Kernel and Tuning Hyperparameters\n",
      "As  kPCA  is  an  unsupervised  learning  algorithm,  there  is  no  obvious  performance\n",
      "measure  to  help  you  select  the  best  kernel  and  hyperparameter  values.  However,\n",
      "dimensionality  reduction  is  often  a  preparation  step  for  a  supervised  learning  task\n",
      "(e.g., classification), so you can simply use grid search to select the kernel and hyper‐\n",
      "parameters that lead to the best performance on that task. For example, the following\n",
      "code  creates  a  two-step  pipeline,  first  reducing  dimensionality  to  two  dimensions\n",
      "using  kPCA,  then  applying  Logistic  Regression  for  classification.  Then  it  uses  Grid\n",
      "SearchCV to find the best kernel and gamma value for kPCA in order to get the best\n",
      "classification accuracy at the end of the pipeline:\n",
      "\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.pipeline import Pipeline\n",
      "\n",
      "6 “Kernel Principal Component Analysis,” B. Schölkopf, A. Smola, K. Müller (1999).\n",
      "\n",
      "Kernel PCA \n",
      "\n",
      "| \n",
      "\n",
      "229\n",
      "\n",
      "\fclf = Pipeline([\n",
      "        (\"kpca\", KernelPCA(n_components=2)),\n",
      "        (\"log_reg\", LogisticRegression())\n",
      "    ])\n",
      "\n",
      "param_grid = [{\n",
      "        \"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n",
      "        \"kpca__kernel\": [\"rbf\", \"sigmoid\"]\n",
      "    }]\n",
      "\n",
      "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
      "grid_search.fit(X, y)\n",
      "\n",
      "The  best  kernel  and  hyperparameters  are  then  available  through  the  best_params_\n",
      "variable:\n",
      "\n",
      ">>> print(grid_search.best_params_)\n",
      "{'kpca__gamma': 0.043333333333333335, 'kpca__kernel': 'rbf'}\n",
      "\n",
      "Another approach, this time entirely unsupervised, is to select the kernel and hyper‐\n",
      "parameters that yield the lowest reconstruction error. However, reconstruction is not\n",
      "as easy as with linear PCA. Here’s why. Figure 8-11 shows the original Swiss roll 3D\n",
      "dataset  (top  left),  and  the  resulting  2D  dataset  after  kPCA  is  applied  using  an  RBF\n",
      "kernel  (top  right).  Thanks  to  the  kernel  trick,  this  is  mathematically  equivalent  to\n",
      "mapping  the  training  set  to  an  infinite-dimensional  feature  space  (bottom  right)\n",
      "using  the  feature  map  φ,  then  projecting  the  transformed  training  set  down  to  2D\n",
      "using  linear  PCA.  Notice  that  if  we  could  invert  the  linear  PCA  step  for  a  given\n",
      "instance in the reduced space, the reconstructed point would lie in feature space, not\n",
      "in the original space (e.g., like the one represented by an x in the diagram). Since the\n",
      "feature  space  is  infinite-dimensional,  we  cannot  compute  the  reconstructed  point,\n",
      "and therefore we cannot compute the true reconstruction error. Fortunately, it is pos‐\n",
      "sible to find a point in the original space that would map close to the reconstructed\n",
      "point. This is called the reconstruction pre-image. Once you have this pre-image, you\n",
      "can measure its squared distance to the original instance. You can then select the ker‐\n",
      "nel and hyperparameters that minimize this reconstruction pre-image error.\n",
      "\n",
      "230 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Dimensionality Reduction\n",
      "\n",
      "\fFigure 8-11. Kernel PCA and the reconstruction pre-image error\n",
      "\n",
      "You may be wondering how to perform this reconstruction. One solution is to train a\n",
      "supervised regression model, with the projected instances as the training set and the\n",
      "original  instances  as  the  targets.  Scikit-Learn  will  do  this  automatically  if  you  set\n",
      "fit_inverse_transform=True, as shown in the following code:7\n",
      "\n",
      "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433,\n",
      "                    fit_inverse_transform=True)\n",
      "X_reduced = rbf_pca.fit_transform(X)\n",
      "X_preimage = rbf_pca.inverse_transform(X_reduced)\n",
      "\n",
      "By default,  fit_inverse_transform=False and  KernelPCA has no\n",
      "inverse_transform()  method.  This  method  only  gets  created\n",
      "when you set fit_inverse_transform=True.\n",
      "\n",
      "7 Scikit-Learn uses the algorithm based on Kernel Ridge Regression described in Gokhan H. Bakır, Jason\n",
      "\n",
      "Weston, and Bernhard Scholkopf, “Learning to Find Pre-images” (Tubingen, Germany: Max Planck Institute\n",
      "for Biological Cybernetics, 2004).\n",
      "\n",
      "Kernel PCA \n",
      "\n",
      "| \n",
      "\n",
      "231\n",
      "\n",
      "\fYou can then compute the reconstruction pre-image error:\n",
      "\n",
      ">>> from sklearn.metrics import mean_squared_error\n",
      ">>> mean_squared_error(X, X_preimage)\n",
      "32.786308795766132\n",
      "\n",
      "Now you can use grid search with cross-validation to find the kernel and hyperpara‐\n",
      "meters that minimize this pre-image reconstruction error.\n",
      "\n",
      "LLE\n",
      "Locally  Linear  Embedding  (LLE)8  is  another  very  powerful  nonlinear  dimensionality\n",
      "reduction (NLDR) technique. It is a Manifold Learning technique that does not rely\n",
      "on projections like the previous algorithms. In a nutshell, LLE works by first measur‐\n",
      "ing how each training instance linearly relates to its closest neighbors (c.n.), and then\n",
      "looking  for  a  low-dimensional  representation  of  the  training  set  where  these  local\n",
      "relationships  are  best  preserved  (more  details  shortly).  This  makes  it  particularly\n",
      "good at unrolling twisted manifolds, especially when there is not too much noise.\n",
      "\n",
      "For example, the following code uses Scikit-Learn’s LocallyLinearEmbedding class to\n",
      "unroll  the  Swiss  roll.  The  resulting  2D  dataset  is  shown  in  Figure  8-12.  As  you  can\n",
      "see,  the  Swiss  roll  is  completely  unrolled  and  the  distances  between  instances  are\n",
      "locally well preserved. However, distances are not preserved on a larger scale: the left\n",
      "part of the unrolled Swiss roll is stretched, while the right part is squeezed. Neverthe‐\n",
      "less, LLE did a pretty good job at modeling the manifold.\n",
      "\n",
      "from sklearn.manifold import LocallyLinearEmbedding\n",
      "\n",
      "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\n",
      "X_reduced = lle.fit_transform(X)\n",
      "\n",
      "8 “Nonlinear Dimensionality Reduction by Locally Linear Embedding,” S. Roweis, L. Saul (2000).\n",
      "\n",
      "232 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Dimensionality Reduction\n",
      "\n",
      "\fFigure 8-12. Unrolled Swiss roll using LLE\n",
      "\n",
      "Here’s how LLE works: first, for each training instance x(i), the algorithm identifies its\n",
      "k closest neighbors (in the preceding code k = 10), then tries to reconstruct x(i) as a\n",
      "linear function of these neighbors. More specifically, it finds the weights wi,j such that\n",
      "m wi, jx j  is as small as possible, assuming wi,j\n",
      "the squared distance between x(i) and ∑ j = 1\n",
      "= 0 if x(j) is not one of the k closest neighbors of x(i). Thus the first step of LLE is the\n",
      "constrained optimization problem described in Equation 8-4, where W is the weight\n",
      "matrix  containing  all  the  weights  wi,j.  The  second  constraint  simply  normalizes  the\n",
      "weights for each training instance x(i).\n",
      "\n",
      "LLE \n",
      "\n",
      "| \n",
      "\n",
      "233\n",
      "\n",
      "\fEquation 8-4. LLE step 1: linearly modeling local relationships\n",
      "\n",
      "W = argmin\n",
      "\n",
      "W\n",
      "\n",
      "m\n",
      "∑\n",
      "i = 1\n",
      "\n",
      "2\n",
      "\n",
      "wi, jx j\n",
      "\n",
      "m\n",
      "x i − ∑\n",
      "j = 1\n",
      "if x j\n",
      "\n",
      "subject to\n",
      "\n",
      "wi, j = 0\n",
      "m\n",
      "∑\n",
      "j = 1\n",
      "\n",
      "wi, j = 1 for i = 1, 2, ⋯, m\n",
      "\n",
      "is not one of the k c.n. of x i\n",
      "\n",
      "After this step, the weight matrix W (containing the weights wi, j) encodes the local\n",
      "linear relationships between the training instances. Now the second step is to map the\n",
      "training  instances  into  a  d-dimensional  space  (where  d  <  n)  while  preserving  these\n",
      "local relationships as much as possible. If z(i) is the image of x(i) in this d-dimensional\n",
      "m wi, jz j  to be as small\n",
      "space, then we want the squared distance between z(i) and ∑ j = 1\n",
      "as possible. This idea leads to the unconstrained optimization problem described in\n",
      "Equation 8-5. It looks very similar to the first step, but instead of keeping the instan‐\n",
      "ces  fixed  and  finding  the  optimal  weights,  we  are  doing  the  reverse:  keeping  the\n",
      "weights  fixed  and  finding  the  optimal  position  of  the  instances’  images  in  the  low-\n",
      "dimensional space. Note that Z is the matrix containing all z(i).\n",
      "\n",
      "Equation 8-5. LLE step 2: reducing dimensionality while preserving relationships\n",
      "\n",
      "Z = argmin\n",
      "\n",
      "Z\n",
      "\n",
      "m\n",
      "∑\n",
      "i = 1\n",
      "\n",
      "m\n",
      "z i − ∑\n",
      "j = 1\n",
      "\n",
      "wi, jz j\n",
      "\n",
      "2\n",
      "\n",
      "Scikit-Learn’s  LLE  implementation  has  the  following  computational  complexity:\n",
      "O(m log(m)n log(k)) for finding the k nearest neighbors, O(mnk3) for optimizing the\n",
      "weights, and O(dm2) for constructing the low-dimensional representations. Unfortu‐\n",
      "nately, the m2 in the last term makes this algorithm scale poorly to very large datasets.\n",
      "\n",
      "Other Dimensionality Reduction Techniques\n",
      "There  are  many  other  dimensionality  reduction  techniques,  several  of  which  are\n",
      "available in Scikit-Learn. Here are some of the most popular:\n",
      "\n",
      "• Multidimensional Scaling (MDS) reduces dimensionality while trying to preserve\n",
      "\n",
      "the distances between the instances (see Figure 8-13).\n",
      "\n",
      "234 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Dimensionality Reduction\n",
      "\n",
      "\f• Isomap creates a graph by connecting each instance to its nearest neighbors, then\n",
      "reduces  dimensionality  while  trying  to  preserve  the  geodesic  distances9  between\n",
      "the instances.\n",
      "\n",
      "• t-Distributed  Stochastic  Neighbor  Embedding  (t-SNE)  reduces  dimensionality\n",
      "while  trying  to  keep  similar  instances  close  and  dissimilar  instances  apart.  It  is\n",
      "mostly  used  for  visualization,  in  particular  to  visualize  clusters  of  instances  in\n",
      "high-dimensional space (e.g., to visualize the MNIST images in 2D).\n",
      "\n",
      "• Linear Discriminant Analysis (LDA) is actually a classification algorithm, but dur‐\n",
      "ing training it learns the most discriminative axes between the classes, and these\n",
      "axes can then be used to define a hyperplane onto which to project the data. The\n",
      "benefit is that the projection will keep classes as far apart as possible, so LDA is a\n",
      "good  technique  to  reduce  dimensionality  before  running  another  classification\n",
      "algorithm such as an SVM classifier.\n",
      "\n",
      "Figure 8-13. Reducing the Swiss roll to 2D using various techniques\n",
      "\n",
      "Exercises\n",
      "\n",
      "1. What are the main motivations for reducing a dataset’s dimensionality? What are\n",
      "\n",
      "the main drawbacks?\n",
      "\n",
      "2. What is the curse of dimensionality?\n",
      "\n",
      "3. Once  a  dataset’s  dimensionality  has  been  reduced,  is  it  possible  to  reverse  the\n",
      "\n",
      "operation? If so, how? If not, why?\n",
      "\n",
      "4. Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?\n",
      "\n",
      "5. Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained\n",
      "variance ratio to 95%. How many dimensions will the resulting dataset have?\n",
      "\n",
      "9 The geodesic distance between two nodes in a graph is the number of nodes on the shortest path between\n",
      "\n",
      "these nodes.\n",
      "\n",
      "Exercises \n",
      "\n",
      "| \n",
      "\n",
      "235\n",
      "\n",
      "\f6. In what cases would you use vanilla PCA, Incremental PCA, Randomized PCA,\n",
      "\n",
      "or Kernel PCA?\n",
      "\n",
      "7. How can you evaluate the performance of a dimensionality reduction algorithm\n",
      "\n",
      "on your dataset?\n",
      "\n",
      "8. Does  it  make  any  sense  to  chain  two  different  dimensionality  reduction  algo‐\n",
      "\n",
      "rithms?\n",
      "\n",
      "9. Load the MNIST dataset (introduced in Chapter 3) and split it into a training set\n",
      "and  a  test  set  (take  the  first  60,000  instances  for  training,  and  the  remaining\n",
      "10,000 for testing). Train a Random Forest classifier on the dataset and time how\n",
      "long it takes, then evaluate the resulting model on the test set. Next, use PCA to\n",
      "reduce  the  dataset’s  dimensionality,  with  an  explained  variance  ratio  of  95%.\n",
      "Train a new Random Forest classifier on the reduced dataset and see how long it\n",
      "takes. Was training much faster? Next evaluate the classifier on the test set: how\n",
      "does it compare to the previous classifier?\n",
      "\n",
      "10. Use  t-SNE  to  reduce  the  MNIST  dataset  down  to  two  dimensions  and  plot  the\n",
      "result using Matplotlib. You can use a scatterplot using 10 different colors to rep‐\n",
      "resent each image’s target class. Alternatively, you can write colored digits at the\n",
      "location of each instance, or even plot scaled-down versions of the digit images\n",
      "themselves  (if  you  plot  all  digits,  the  visualization  will  be  too  cluttered,  so  you\n",
      "should either draw a random sample or plot an instance only if no other instance\n",
      "has already been plotted at a close distance). You should get a nice visualization\n",
      "with  well-separated  clusters  of  digits.  Try  using  other  dimensionality  reduction\n",
      "algorithms such as PCA, LLE, or MDS and compare the resulting visualizations.\n",
      "\n",
      "Solutions to these exercises are available in ???.\n",
      "\n",
      "236 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 8: Dimensionality Reduction\n",
      "\n",
      "\fCHAPTER 9\n",
      "Unsupervised Learning Techniques\n",
      "\n",
      "With  Early  Release  ebooks,  you  get  books  in  their  earliest  form—\n",
      "the author’s raw and unedited content as he or she writes—so you\n",
      "can  take  advantage  of  these  technologies  long  before  the  official\n",
      "release of these titles. The following will be Chapter 9 in the final\n",
      "release of the book.\n",
      "\n",
      "Although  most  of  the  applications  of  Machine  Learning  today  are  based  on  super‐\n",
      "vised learning (and as a result, this is where most of the investments go to), the vast\n",
      "majority of the available data is actually unlabeled: we have the input features X, but\n",
      "we do not have the labels y. Yann LeCun famously said that “if intelligence was a cake,\n",
      "unsupervised learning would be the cake, supervised learning would be the icing on\n",
      "the  cake,  and  reinforcement  learning  would  be  the  cherry  on  the  cake”.  In  other\n",
      "words,  there  is  a  huge  potential  in  unsupervised  learning  that  we  have  only  barely\n",
      "started to sink our teeth into.\n",
      "\n",
      "For example, say you want to create a system that will take a few pictures of each item\n",
      "on  a  manufacturing  production  line  and  detect  which  items  are  defective.  You  can\n",
      "fairly easily create a system that will take pictures automatically, and this might give\n",
      "you thousands of pictures every day. You can then build a reasonably large dataset in\n",
      "just a few weeks. But wait, there are no labels! If you want to train a regular binary\n",
      "classifier that will predict whether an item is defective or not, you will need to label\n",
      "every  single  picture  as  “defective”  or  “normal”.  This  will  generally  require  human\n",
      "experts  to  sit  down  and  manually  go  through  all  the  pictures.  This  is  a  long,  costly\n",
      "and tedious task, so it will usually only be done on a small subset of the available pic‐\n",
      "tures.  As  a  result,  the  labeled  dataset  will  be  quite  small,  and  the  classifier’s  perfor‐\n",
      "mance will be disappointing. Moreover, every time the company makes any change to\n",
      "its products, the whole process will need to be started over from scratch. Wouldn’t it\n",
      "\n",
      "237\n",
      "\n",
      "\fbe  great  if  the  algorithm  could  just  exploit  the  unlabeled  data  without  needing\n",
      "humans to label every picture? Enter unsupervised learning.\n",
      "\n",
      "In Chapter 8, we looked at the most common unsupervised learning task: dimension‐\n",
      "ality reduction. In this chapter, we will look at a few more unsupervised learning tasks\n",
      "and algorithms:\n",
      "\n",
      "• Clustering: the goal is to group similar instances together into clusters. This is a\n",
      "great  tool  for  data  analysis,  customer  segmentation,  recommender  systems,\n",
      "search  engines,  image  segmentation,  semi-supervised  learning,  dimensionality\n",
      "reduction, and more.\n",
      "\n",
      "• Anomaly  detection:  the  objective  is  to  learn  what  “normal”  data  looks  like,  and\n",
      "use  this  to  detect  abnormal  instances,  such  as  defective  items  on  a  production\n",
      "line or a new trend in a time series.\n",
      "\n",
      "• Density  estimation:  this  is  the  task  of  estimating  the  probability  density  function\n",
      "(PDF) of the random process that generated the dataset. This is commonly used\n",
      "for anomaly detection: instances located in very low-density regions are likely to\n",
      "be anomalies. It is also useful for data analysis and visualization.\n",
      "\n",
      "Ready  for  some  cake?  We  will  start  with  clustering,  using  K-Means  and  DBSCAN,\n",
      "and then we will discuss Gaussian mixture models and see how they can be used for\n",
      "density estimation, clustering, and anomaly detection.\n",
      "\n",
      "Clustering\n",
      "As you enjoy a hike in the mountains, you stumble upon a plant you have never seen\n",
      "before. You look around and you notice a few more. They are not perfectly identical,\n",
      "yet  they  are  sufficiently  similar  for  you  to  know  that  they  most  likely  belong  to  the\n",
      "same species (or at least the same genus). You may need a botanist to tell you what\n",
      "species  that  is,  but  you  certainly  don’t  need  an  expert  to  identify  groups  of  similar-\n",
      "looking objects. This is called clustering: it is the task of identifying similar instances\n",
      "and assigning them to clusters, i.e., groups of similar instances.\n",
      "\n",
      "Just like in classification, each instance gets assigned to a group. However, this is an\n",
      "unsupervised task. Consider Figure 9-1: on the left is the iris dataset (introduced in\n",
      "Chapter 4), where each instance’s species (i.e., its class) is represented with a different\n",
      "marker.  It  is  a  labeled  dataset,  for  which  classification  algorithms  such  as  Logistic\n",
      "Regression,  SVMs  or  Random  Forest  classifiers  are  well  suited.  On  the  right  is  the\n",
      "same dataset, but without the labels, so you cannot use a classification algorithm any‐\n",
      "more. This is where clustering algorithms step in: many of them can easily detect the\n",
      "top left cluster. It is also quite easy to see with our own eyes, but it is not so obvious\n",
      "that  the  lower  right  cluster  is  actually  composed  of  two  distinct  sub-clusters.  That\n",
      "said,  the  dataset  actually  has  two  additional  features  (sepal  length  and  width),  not\n",
      "\n",
      "238 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      "\frepresented here, and clustering algorithms can make good use of all features, so in\n",
      "fact they identify the three clusters fairly well (e.g., using a Gaussian mixture model,\n",
      "only 5 instances out of 150 are assigned to the wrong cluster).\n",
      "\n",
      "Figure 9-1. Classification (left) versus clustering (right)\n",
      "\n",
      "Clustering is used in a wide variety of applications, including:\n",
      "\n",
      "• For customer segmentation: you can cluster your customers based on their pur‐\n",
      "chases, their activity on your website, and so on. This is useful to understand who\n",
      "your  customers  are  and  what  they  need,  so  you  can  adapt  your  products  and\n",
      "marketing campaigns to each segment. For example, this can be useful in recom‐\n",
      "mender systems to suggest content that other users in the same cluster enjoyed.\n",
      "\n",
      "• For data analysis: when analyzing a new dataset, it is often useful to first discover\n",
      "\n",
      "clusters of similar instances, as it is often easier to analyze clusters separately.\n",
      "\n",
      "• As a dimensionality reduction technique: once a dataset has been clustered, it is\n",
      "usually  possible  to  measure  each  instance’s  affinity  with  each  cluster  (affinity  is\n",
      "any  measure  of  how  well  an  instance  fits  into  a  cluster).  Each  instance’s  feature\n",
      "vector x can then be replaced with the vector of its cluster affinities. If there are k\n",
      "clusters,  then  this  vector  is  k  dimensional.  This  is  typically  much  lower  dimen‐\n",
      "sional than the original feature vector, but it can preserve enough information for\n",
      "further processing.\n",
      "\n",
      "• For anomaly detection (also called outlier detection): any instance that has a low\n",
      "affinity to all the clusters is likely to be an anomaly. For example, if you have clus‐\n",
      "tered the users of your website based on their behavior, you can detect users with\n",
      "unusual behavior, such as an unusual number of requests per second, and so on.\n",
      "Anomaly detection is particularly useful in detecting defects in manufacturing, or\n",
      "for fraud detection.\n",
      "\n",
      "• For  semi-supervised  learning:  if  you  only  have  a  few  labels,  you  could  perform\n",
      "clustering and propagate the labels to all the instances in the same cluster. This\n",
      "can  greatly  increase  the  amount  of  labels  available  for  a  subsequent  supervised\n",
      "learning algorithm, and thus improve its performance.\n",
      "\n",
      "Clustering \n",
      "\n",
      "| \n",
      "\n",
      "239\n",
      "\n",
      "\f• For search engines: for example, some search engines let you search for images\n",
      "that  are  similar  to  a  reference  image.  To  build  such  a  system,  you  would  first\n",
      "apply  a  clustering  algorithm  to  all  the  images  in  your  database:  similar  images\n",
      "would end up in the same cluster. Then when a user provides a reference image,\n",
      "all  you  need  to  do  is  to  find  this  image’s  cluster  using  the  trained  clustering\n",
      "model, and you can then simply return all the images from this cluster.\n",
      "\n",
      "• To segment an image: by clustering pixels according to their color, then replacing\n",
      "each  pixel’s  color  with  the  mean  color  of  its  cluster,  it  is  possible  to  reduce  the\n",
      "number of different colors in the image considerably. This technique is used in\n",
      "many  object  detection  and  tracking  systems,  as  it  makes  it  easier  to  detect  the\n",
      "contour of each object.\n",
      "\n",
      "There is no universal definition of what a cluster is: it really depends on the context,\n",
      "and  different  algorithms  will  capture  different  kinds  of  clusters.  For  example,  some\n",
      "algorithms  look  for  instances  centered  around  a  particular  point,  called  a  centroid.\n",
      "Others  look  for  continuous  regions  of  densely  packed  instances:  these  clusters  can\n",
      "take on any shape. Some algorithms are hierarchical, looking for clusters of clusters.\n",
      "And the list goes on.\n",
      "\n",
      "In  this  section,  we  will  look  at  two  popular  clustering  algorithms:  K-Means  and\n",
      "DBSCAN,  and  we  will  show  some  of  their  applications,  such  as  non-linear  dimen‐\n",
      "sionality reduction, semi-supervised learning and anomaly detection.\n",
      "\n",
      "K-Means\n",
      "Consider the unlabeled dataset represented in Figure 9-2: you can clearly see 5 blobs\n",
      "of instances. The K-Means algorithm is a simple algorithm capable of clustering this\n",
      "kind of dataset very quickly and efficiently, often in just a few iterations. It was pro‐\n",
      "posed by Stuart Lloyd at the Bell Labs in 1957 as a technique for pulse-code modula‐\n",
      "tion,  but  it  was  only  published  outside  of  the  company  in  1982,  in  a  paper  titled\n",
      "“Least  square  quantization  in  PCM”.1  By  then,  in  1965,  Edward  W.  Forgy  had  pub‐\n",
      "lished  virtually  the  same  algorithm,  so  K-Means  is  sometimes  referred  to  as  Lloyd-\n",
      "Forgy.\n",
      "\n",
      "1 “Least square quantization in PCM,” Stuart P. Lloyd. (1982).\n",
      "\n",
      "240 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      "\fFigure 9-2. An unlabeled dataset composed of five blobs of instances\n",
      "\n",
      "Let’s train a K-Means clusterer on this dataset. It will try to find each blob’s center and\n",
      "assign each instance to the closest blob:\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "k = 5\n",
      "kmeans = KMeans(n_clusters=k)\n",
      "y_pred = kmeans.fit_predict(X)\n",
      "\n",
      "Note that you have to specify the number of clusters k that the algorithm must find.\n",
      "In this example, it is pretty obvious from looking at the data that k should be set to 5,\n",
      "but in general it is not that easy. We will discuss this shortly.\n",
      "\n",
      "Each instance was assigned to one of the 5 clusters. In the context of clustering, an\n",
      "instance’s  label  is  the  index  of  the  cluster  that  this  instance  gets  assigned  to  by  the\n",
      "algorithm: this is not to be confused with the class labels in classification (remember\n",
      "that  clustering  is  an  unsupervised  learning  task).  The  KMeans  instance  preserves  a\n",
      "copy of the labels of the instances it was trained on, available via the labels_ instance\n",
      "variable:\n",
      "\n",
      ">>> y_pred\n",
      "array([4, 0, 1, ..., 2, 1, 0], dtype=int32)\n",
      ">>> y_pred is kmeans.labels_\n",
      "True\n",
      "\n",
      "We can also take a look at the 5 centroids that the algorithm found:\n",
      "\n",
      ">>> kmeans.cluster_centers_\n",
      "array([[-2.80389616,  1.80117999],\n",
      "       [ 0.20876306,  2.25551336],\n",
      "       [-2.79290307,  2.79641063],\n",
      "       [-1.46679593,  2.28585348],\n",
      "       [-2.80037642,  1.30082566]])\n",
      "\n",
      "Of course, you can easily assign new instances to the cluster whose centroid is closest:\n",
      "\n",
      "Clustering \n",
      "\n",
      "| \n",
      "\n",
      "241\n",
      "\n",
      "\f>>> X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\n",
      ">>> kmeans.predict(X_new)\n",
      "array([1, 1, 2, 2], dtype=int32)\n",
      "\n",
      "If  you  plot  the  cluster’s  decision  boundaries,  you  get  a  Voronoi  tessellation  (see\n",
      "Figure 9-3, where each centroid is represented with an X):\n",
      "\n",
      "Figure 9-3. K-Means decision boundaries (Voronoi tessellation)\n",
      "\n",
      "The vast majority of the instances were clearly assigned to the appropriate cluster, but\n",
      "a few instances were probably mislabeled (especially near the boundary between the\n",
      "top  left  cluster  and  the  central  cluster).  Indeed,  the  K-Means  algorithm  does  not\n",
      "behave very well when the blobs have very different diameters since all it cares about\n",
      "when assigning an instance to a cluster is the distance to the centroid.\n",
      "\n",
      "Instead of assigning each instance to a single cluster, which is called hard clustering, it\n",
      "can be useful to just give each instance a score per cluster: this is called soft clustering.\n",
      "For example, the score can be the distance between the instance and the centroid, or\n",
      "conversely it can be a similarity score (or affinity) such as the Gaussian Radial Basis\n",
      "Function  (introduced  in  Chapter  5).  In  the  KMeans  class,  the  transform()  method\n",
      "measures the distance from each instance to every centroid:\n",
      "\n",
      ">>> kmeans.transform(X_new)\n",
      "array([[2.81093633, 0.32995317, 2.9042344 , 1.49439034, 2.88633901],\n",
      "       [5.80730058, 2.80290755, 5.84739223, 4.4759332 , 5.84236351],\n",
      "       [1.21475352, 3.29399768, 0.29040966, 1.69136631, 1.71086031],\n",
      "       [0.72581411, 3.21806371, 0.36159148, 1.54808703, 1.21567622]])\n",
      "\n",
      "In  this  example,  the  first  instance  in  X_new  is  located  at  a  distance  of  2.81  from  the\n",
      "first centroid, 0.33 from the second centroid, 2.90 from the third centroid, 1.49 from\n",
      "the fourth centroid and 2.87 from the fifth centroid. If you have a high-dimensional\n",
      "dataset and you transform it this way, you end up with a k-dimensional dataset: this\n",
      "can be a very efficient non-linear dimensionality reduction technique.\n",
      "\n",
      "242 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      "\fThe K-Means Algorithm\n",
      "\n",
      "So  how  does  the  algorithm  work?  Well  it  is  really  quite  simple.  Suppose  you  were\n",
      "given the centroids: you could easily label all the instances in the dataset by assigning\n",
      "each of them to the cluster whose centroid is closest. Conversely, if you were given all\n",
      "the instance labels, you could easily locate all the centroids by computing the mean of\n",
      "the instances for each cluster. But you are given neither the labels nor the centroids,\n",
      "so how can you proceed? Well, just start by placing the centroids randomly (e.g., by\n",
      "picking k instances at random and using their locations as centroids). Then label the\n",
      "instances,  update  the  centroids,  label  the  instances,  update  the  centroids,  and  so  on\n",
      "until the centroids stop moving. The algorithm is guaranteed to converge in a finite\n",
      "number  of  steps  (usually  quite  small),  it  will  not  oscillate  forever2.  You  can  see  the\n",
      "algorithm  in  action  in  Figure  9-4:  the  centroids  are  initialized  randomly  (top  left),\n",
      "then the instances are labeled (top right), then the centroids are updated (center left),\n",
      "the instances are relabeled (center right), and so on. As you can see, in just 3 itera‐\n",
      "tions the algorithm has reached a clustering that seems close to optimal.\n",
      "\n",
      "Figure 9-4. The K-Means algorithm\n",
      "\n",
      "2 This can be proven by pointing out that the mean squared distance between the instances and their closest\n",
      "\n",
      "centroid can only go down at each step.\n",
      "\n",
      "Clustering \n",
      "\n",
      "| \n",
      "\n",
      "243\n",
      "\n",
      "\fThe computational complexity of the algorithm is generally linear\n",
      "with regards to the number of instances m, the number of clusters\n",
      "k and the number of dimensions n. However, this is only true when\n",
      "the data has a clustering structure. If it does not, then in the worst\n",
      "case  scenario  the  complexity  can  increase  exponentially  with  the\n",
      "number of instances. In practice, however, this rarely happens, and\n",
      "K-Means is generally one of the fastest clustering algorithms.\n",
      "\n",
      "Unfortunately, although the algorithm is guaranteed to converge, it may not converge\n",
      "to the right solution (i.e., it may converge to a local optimum): this depends on the\n",
      "centroid initialization. For example, Figure 9-5 shows two sub-optimal solutions that\n",
      "the algorithm can converge to if you are not lucky with the random initialization step:\n",
      "\n",
      "Figure 9-5. Sub-optimal solutions due to unlucky centroid initializations\n",
      "\n",
      "Let’s look at a few ways you can mitigate this risk by improving the centroid initializa‐\n",
      "tion.\n",
      "\n",
      "Centroid Initialization Methods\n",
      "\n",
      "If you happen to know approximately where the centroids should be (e.g., if you ran\n",
      "another clustering algorithm earlier), then you can set the init hyperparameter to a\n",
      "NumPy array containing the list of centroids, and set n_init to 1:\n",
      "\n",
      "good_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])\n",
      "kmeans = KMeans(n_clusters=5, init=good_init, n_init=1)\n",
      "\n",
      "Another solution is to run the algorithm multiple times with different random initial‐\n",
      "izations and keep the best solution. This is controlled by the n_init hyperparameter:\n",
      "by default, it is equal to 10, which means that the whole algorithm described earlier\n",
      "actually runs 10 times when you call fit(), and Scikit-Learn keeps the best solution.\n",
      "But how exactly does it know which solution is the best? Well of course it uses a per‐\n",
      "formance  metric!  It  is  called  the  model’s  inertia:  this  is  the  mean  squared  distance\n",
      "between  each  instance  and  its  closest  centroid.  It  is  roughly  equal  to  223.3  for  the\n",
      "model on the left of Figure 9-5, 237.5 for the model on the right of Figure 9-5, and\n",
      "211.6 for the model in Figure 9-3. The KMeans class runs the algorithm n_init times\n",
      "and keeps the model with the lowest inertia: in this example, the model in Figure 9-3\n",
      "will be selected (unless we are very unlucky with n_init consecutive random initiali‐\n",
      "\n",
      "244 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      "\fzations). If you are curious, a model’s inertia is accessible via the inertia_ instance\n",
      "variable:\n",
      "\n",
      ">>> kmeans.inertia_\n",
      "211.59853725816856\n",
      "\n",
      "The score() method returns the negative inertia. Why negative? Well, it is because a\n",
      "predictor’s score() method must always respect the \"great is better\" rule.\n",
      "\n",
      ">>> kmeans.score(X)\n",
      "-211.59853725816856\n",
      "\n",
      "An important improvement to the K-Means algorithm, called K-Means+\\+, was pro‐\n",
      "posed  in  a  2006  paper  by  David  Arthur  and  Sergei  Vassilvitskii:3  they  introduced  a\n",
      "smarter  initialization  step  that  tends  to  select  centroids  that  are  distant  from  one\n",
      "another, and this makes the K-Means algorithm much less likely to converge to a sub-\n",
      "optimal  solution.  They  showed  that  the  additional  computation  required  for  the\n",
      "smarter  initialization  step  is  well  worth  it  since  it  makes  it  possible  to  drastically\n",
      "reduce the number of times the algorithm needs to be run to find the optimal solu‐\n",
      "tion. Here is the K-Means++ initialization algorithm:\n",
      "\n",
      "• Take one centroid c(1), chosen uniformly at random from the dataset.\n",
      "• Take  a  new  centroid  c(i),  choosing  an  instance  x(i)  with  probability:  D  i 2\n",
      "m D  j 2\n",
      " where D(x(i)) is the distance between the instance x(i) and the closest\n",
      "∑ j = 1\n",
      "centroid  that  was  already  chosen.  This  probability  distribution  ensures  that\n",
      "instances  further  away  from  already  chosen  centroids  are  much  more  likely  be\n",
      "selected as centroids.\n",
      "\n",
      "• Repeat the previous step until all k centroids have been chosen.\n",
      "\n",
      "The  KMeans  class  actually  uses  this  initialization  method  by  default.  If  you  want  to\n",
      "force  it  to  use  the  original  method  (i.e.,  picking  k  instances  randomly  to  define  the\n",
      "initial  centroids),  then  you  can  set  the  init  hyperparameter  to  \"random\".  You  will\n",
      "rarely need to do this.\n",
      "\n",
      "Accelerated K-Means and Mini-batch K-Means\n",
      "\n",
      "Another important improvement to the K-Means algorithm was proposed in a 2003\n",
      "paper by Charles Elkan.4 It considerably accelerates the algorithm by avoiding many\n",
      "unnecessary distance calculations: this is achieved by exploiting the triangle inequal‐\n",
      "\n",
      "3 “k-means\\++: The advantages of careful seeding,” David Arthur and Sergei Vassilvitskii (2006).\n",
      "\n",
      "4 “Using the Triangle Inequality to Accelerate k-Means,” Charles Elkan (2003).\n",
      "\n",
      "Clustering \n",
      "\n",
      "| \n",
      "\n",
      "245\n",
      "\n",
      "\fity  (i.e.,  the  straight  line  is  always  the  shortest5)  and  by  keeping  track  of  lower  and\n",
      "upper  bounds  for  distances  between  instances  and  centroids.  This  is  the  algorithm\n",
      "used by default by the KMeans class (but you can force it to use the original algorithm\n",
      "by  setting  the  algorithm  hyperparameter  to  \"full\",  although  you  probably  will\n",
      "never need to).\n",
      "\n",
      "Yet  another  important  variant  of  the  K-Means  algorithm  was  proposed  in  a  2010\n",
      "paper  by  David  Sculley.6  Instead  of  using  the  full  dataset  at  each  iteration,  the  algo‐\n",
      "rithm is capable of using mini-batches, moving the centroids just slightly at each iter‐\n",
      "ation.  This  speeds  up  the  algorithm  typically  by  a  factor  of  3  or  4  and  makes  it\n",
      "possible to cluster huge datasets that do not fit in memory. Scikit-Learn implements\n",
      "this  algorithm  in  the  MiniBatchKMeans  class.  You  can  just  use  this  class  like  the\n",
      "KMeans class:\n",
      "\n",
      "from sklearn.cluster import MiniBatchKMeans\n",
      "\n",
      "minibatch_kmeans = MiniBatchKMeans(n_clusters=5)\n",
      "minibatch_kmeans.fit(X)\n",
      "\n",
      "If the dataset does not fit in memory, the simplest option is to use the memmap class, as\n",
      "we did for incremental PCA in Chapter 8. Alternatively, you can pass one mini-batch\n",
      "at a time to the partial_fit() method, but this will require much more work, since\n",
      "you will need to perform multiple initializations and select the best one yourself (see\n",
      "the notebook for an example).\n",
      "\n",
      "Although  the  Mini-batch  K-Means  algorithm  is  much  faster  than  the  regular  K-\n",
      "Means  algorithm,  its  inertia  is  generally  slightly  worse,  especially  as  the  number  of\n",
      "clusters  increases.  You  can  see  this  in  Figure  9-6:  the  plot  on  the  left  compares  the\n",
      "inertias of Mini-batch K-Means and regular K-Means models trained on the previous\n",
      "dataset  using  various  numbers  of  clusters  k.  The  difference  between  the  two  curves\n",
      "remains fairly constant, but this difference becomes more and more significant as k\n",
      "increases, since the inertia becomes smaller and smaller. However, in the plot on the\n",
      "right, you can see that Mini-batch K-Means is much faster than regular K-Means, and\n",
      "this difference increases with k.\n",
      "\n",
      "5 The triangle inequality is AC ≤ AB + BC where A, B and C are three points, and AB, AC and BC are the\n",
      "\n",
      "distances between these points.\n",
      "\n",
      "6 “Web-Scale K-Means Clustering,” David Sculley (2010).\n",
      "\n",
      "246 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      "\fFigure 9-6. Mini-batch K-Means vs K-Means: worse inertia as k increases (left) but\n",
      "much faster (right)\n",
      "\n",
      "Finding the Optimal Number of Clusters\n",
      "\n",
      "So far, we have set the number of clusters k to 5 because it was obvious by looking at\n",
      "the  data  that  this  is  the  correct  number  of  clusters.  But  in  general,  it  will  not  be  so\n",
      "easy to know how to set k, and the result might be quite bad if you set it to the wrong\n",
      "value. For example, as you can see in Figure 9-7, setting k to 3 or 8 results in fairly\n",
      "bad models:\n",
      "\n",
      "Figure 9-7. Bad choices for the number of clusters\n",
      "\n",
      "You  might  be  thinking  that  we  could  just  pick  the  model  with  the  lowest  inertia,\n",
      "right? Unfortunately, it is not that simple. The inertia for k=3 is 653.2, which is much\n",
      "higher  than  for  k=5  (which  was  211.6),  but  with  k=8,  the  inertia  is  just  119.1.  The\n",
      "inertia is not a good performance metric when trying to choose k since it keeps get‐\n",
      "ting  lower  as  we  increase  k.  Indeed,  the  more  clusters  there  are,  the  closer  each\n",
      "instance will be to its closest centroid, and therefore the lower the inertia will be. Let’s\n",
      "plot the inertia as a function of k (see Figure 9-8):\n",
      "\n",
      "Clustering \n",
      "\n",
      "| \n",
      "\n",
      "247\n",
      "\n",
      "\fFigure 9-8. Selecting the number of clusters k using the “elbow rule”\n",
      "\n",
      "As  you  can  see,  the  inertia  drops  very  quickly  as  we  increase  k  up  to  4,  but  then  it\n",
      "decreases  much  more  slowly  as  we  keep  increasing  k.  This  curve  has  roughly  the\n",
      "shape of an arm, and there is an “elbow” at k=4 so if we did not know better, it would\n",
      "be a good choice: any lower value would be dramatic, while any higher value would\n",
      "not help much, and we might just be splitting perfectly good clusters in half for no\n",
      "good reason.\n",
      "\n",
      "This technique for choosing the best value for the number of clusters is rather coarse.\n",
      "A more precise approach (but also more computationally expensive) is to use the sil‐\n",
      "houette score, which is the mean silhouette coefficient over all the instances. An instan‐\n",
      "ce’s silhouette coefficient is equal to (b – a) / max(a, b) where a is the mean distance\n",
      "to the other instances in the same cluster (it is the mean intra-cluster distance), and b\n",
      "is the mean nearest-cluster distance, that is the mean distance to the instances of the\n",
      "next closest cluster (defined as the one that minimizes b, excluding the instance’s own\n",
      "cluster). The silhouette coefficient can vary between -1 and +1: a coefficient close to\n",
      "+1 means that the instance is well inside its own cluster and far from other clusters,\n",
      "while a coefficient close to 0 means that it is close to a cluster boundary, and finally a\n",
      "coefficient close to -1 means that the instance may have been assigned to the wrong\n",
      "cluster.  To  compute  the  silhouette  score,  you  can  use  Scikit-Learn’s  silhou\n",
      "ette_score() function, giving it all the instances in the dataset, and the labels they\n",
      "were assigned:\n",
      "\n",
      ">>> from sklearn.metrics import silhouette_score\n",
      ">>> silhouette_score(X, kmeans.labels_)\n",
      "0.655517642572828\n",
      "\n",
      "Let’s compare the silhouette scores for different numbers of clusters (see Figure 9-9):\n",
      "\n",
      "248 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      "\fFigure 9-9. Selecting the number of clusters k using the silhouette score\n",
      "\n",
      "As you can see, this visualization is much richer than the previous one: in particular,\n",
      "although  it  confirms  that  k=4  is  a  very  good  choice,  it  also  underlines  the  fact  that\n",
      "k=5 is quite good as well, and much better than k=6 or 7. This was not visible when\n",
      "comparing inertias.\n",
      "\n",
      "An  even  more  informative  visualization  is  obtained  when  you  plot  every  instance’s\n",
      "silhouette coefficient, sorted by the cluster they are assigned to and by the value of the\n",
      "coefficient. This is called a silhouette diagram (see Figure 9-10):\n",
      "\n",
      "Figure 9-10. Silouhette analysis: comparing the silhouette diagrams for various values of\n",
      "k\n",
      "\n",
      "The vertical dashed lines represent the silhouette score for each number of clusters.\n",
      "When most of the instances in a cluster have a lower coefficient than this score (i.e., if\n",
      "many of the instances stop short of the dashed line, ending to the left of it), then the\n",
      "cluster is rather bad since this means its instances are much too close to other clus‐\n",
      "\n",
      "Clustering \n",
      "\n",
      "| \n",
      "\n",
      "249\n",
      "\n",
      "\fters. We can see that when k=3 and when k=6, we get bad clusters. But when k=4 or\n",
      "k=5, the clusters look pretty good – most instances extend beyond the dashed line, to\n",
      "the right and closer to 1.0. When k=4, the cluster at index 1 (the third from the top),\n",
      "is rather big, while when k=5, all clusters have similar sizes, so even though the over‐\n",
      "all silhouette score from k=4 is slightly greater than for k=5, it seems like a good idea\n",
      "to use k=5 to get clusters of similar sizes.\n",
      "\n",
      "Limits of K-Means\n",
      "Despite its many merits, most notably being fast and scalable, K-Means is not perfect.\n",
      "As we saw, it is necessary to run the algorithm several times to avoid sub-optimal sol‐\n",
      "utions, plus you need to specify the number of clusters, which can be quite a hassle.\n",
      "Moreover, K-Means does not behave very well when the clusters have varying sizes,\n",
      "different densities, or non-spherical shapes. For example, Figure 9-11 shows how K-\n",
      "Means clusters a dataset containing three ellipsoidal clusters of different dimensions,\n",
      "densities and orientations:\n",
      "\n",
      "Figure 9-11. K-Means fails to cluster these ellipsoidal blobs properly\n",
      "\n",
      "As  you  can  see,  neither  of  these  solutions  are  any  good.  The  solution  on  the  left  is\n",
      "better, but it still chops off 25% of the middle cluster and assigns it to the cluster on\n",
      "the right. The solution on the right is just terrible, even though its inertia is lower. So\n",
      "depending on the data, different clustering algorithms may perform better. For exam‐\n",
      "ple, on these types of elliptical clusters, Gaussian mixture models work great.\n",
      "\n",
      "It is important to scale the input features before you run K-Means,\n",
      "or  else  the  clusters  may  be  very  stretched,  and  K-Means  will  per‐\n",
      "form  poorly.  Scaling  the  features  does  not  guarantee  that  all  the\n",
      "clusters will be nice and spherical, but it generally improves things.\n",
      "\n",
      "Now let’s look at a few ways we can benefit from clustering. We will use K-Means, but\n",
      "feel free to experiment with other clustering algorithms.\n",
      "\n",
      "250 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      "\fUsing clustering for image segmentation\n",
      "Image  segmentation  is  the  task  of  partitioning  an  image  into  multiple  segments.  In\n",
      "semantic segmentation, all pixels that are part of the same object type get assigned to\n",
      "the same segment. For example, in a self-driving car’s vision system, all pixels that are\n",
      "part  of  a  pedestrian’s  image  might  be  assigned  to  the  “pedestrian”  segment  (there\n",
      "would just be one segment containing all the pedestrians). In instance segmentation,\n",
      "all pixels that are part of the same individual object are assigned to the same segment.\n",
      "In this case there would be a different segment for each pedestrian. The state of the\n",
      "art  in  semantic  or  instance  segmentation  today  is  achieved  using  complex  architec‐\n",
      "tures based on convolutional neural networks (see Chapter 14). Here, we are going to\n",
      "do something much simpler: color segmentation. We will simply assign pixels to the\n",
      "same segment if they have a similar color. In some applications, this may be sufficient,\n",
      "for example if you want to analyze satellite images to measure how much total forest\n",
      "area there is in a region, color segmentation may be just fine.\n",
      "\n",
      "First, let’s load the image (see the upper left image in Figure 9-12) using Matplotlib’s\n",
      "imread() function:\n",
      "\n",
      ">>> from matplotlib.image import imread  # you could also use `imageio.imread()`\n",
      ">>> image = imread(os.path.join(\"images\",\"clustering\",\"ladybug.png\"))\n",
      ">>> image.shape\n",
      "(533, 800, 3)\n",
      "\n",
      "The  image  is  represented  as  a  3D  array:  the  first  dimension’s  size  is  the  height,  the\n",
      "second is the width, and the third is the number of color channels, in this case red,\n",
      "green and blue (RGB). In other words, for each pixel there is a 3D vector containing\n",
      "the intensities of red, green and blue, each between 0.0 and 1.0 (or between 0 and 255\n",
      "if  you  use  imageio.imread()).  Some  images  may  have  less  channels,  such  as  gray‐\n",
      "scale  images  (one  channel),  or  more  channels,  such  as  images  with  an  additional\n",
      "alpha channel for transparency, or satellite images which often contain channels for\n",
      "many light frequencies (e.g., infrared). The following code reshapes the array to get a\n",
      "long list of RGB colors, then it clusters these colors using K-Means. For example, it\n",
      "may  identify  a  color  cluster  for  all  shades  of  green.  Next,  for  each  color  (e.g.,  dark\n",
      "green), it looks for the mean color of the pixel’s color cluster. For example, all shades\n",
      "of green may be replaced with the same light green color (assuming the mean color of\n",
      "the green cluster is light green). Finally it reshapes this long list of colors to get the\n",
      "same shape as the original image. And we’re done!\n",
      "\n",
      "X = image.reshape(-1, 3)\n",
      "kmeans = KMeans(n_clusters=8).fit(X)\n",
      "segmented_img = kmeans.cluster_centers_[kmeans.labels_]\n",
      "segmented_img = segmented_img.reshape(image.shape)\n",
      "\n",
      "This outputs the image shown in the upper right of Figure 9-12. You can experiment\n",
      "with various numbers of clusters, as shown in the figure. When you use less than 8\n",
      "clusters,  notice  that  the  ladybug’s  flashy  red  color  fails  to  get  a  cluster  of  its  own:  it\n",
      "\n",
      "Clustering \n",
      "\n",
      "| \n",
      "\n",
      "251\n",
      "\n",
      "\fgets merged with colors from the environment. This is due to the fact that the lady‐\n",
      "bug is quite small, much smaller than the rest of the image, so even though its color is\n",
      "flashy, K-Means fails to dedicate a cluster to it: as mentioned earlier, K-Means prefers\n",
      "clusters of similar sizes.\n",
      "\n",
      "Figure 9-12. Image segmentation using K-Means with various numbers of color clusters\n",
      "\n",
      "That was not too hard, was it? Now let’s look at another application of clustering: pre‐\n",
      "processing.\n",
      "\n",
      "Using Clustering for Preprocessing\n",
      "Clustering can be an efficient approach to dimensionality reduction, in particular as a\n",
      "preprocessing  step  before  a  supervised  learning  algorithm.  For  example,  let’s  tackle\n",
      "the digits dataset which is a simple MNIST-like dataset containing 1,797 grayscale 8×8\n",
      "images representing digits 0 to 9. First, let’s load the dataset:\n",
      "\n",
      "from sklearn.datasets import load_digits\n",
      "\n",
      "X_digits, y_digits = load_digits(return_X_y=True)\n",
      "\n",
      "Now, let’s split it into a training set and a test set:\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits)\n",
      "\n",
      "Next, let’s fit a Logistic Regression model:\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "log_reg = LogisticRegression(random_state=42)\n",
      "log_reg.fit(X_train, y_train)\n",
      "\n",
      "Let’s evaluate its accuracy on the test set:\n",
      "\n",
      ">>> log_reg.score(X_test, y_test)\n",
      "0.9666666666666667\n",
      "\n",
      "252 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      "\fOkay,  that’s  our  baseline:  96.7%  accuracy.  Let’s  see  if  we  can  do  better  by  using  K-\n",
      "Means  as  a  preprocessing  step.  We  will  create  a  pipeline  that  will  first  cluster  the\n",
      "training  set  into  50  clusters  and  replace  the  images  with  their  distances  to  these  50\n",
      "clusters, then apply a logistic regression model.\n",
      "\n",
      "Although  it  is  tempting  to  define  the  number  of  clusters  to  10,\n",
      "since  there  are  10  different  digits,  it  is  unlikely  to  perform  well,\n",
      "because there are several different ways to write each digit.\n",
      "\n",
      "from sklearn.pipeline import Pipeline\n",
      "\n",
      "pipeline = Pipeline([\n",
      "    (\"kmeans\", KMeans(n_clusters=50)),\n",
      "    (\"log_reg\", LogisticRegression()),\n",
      "])\n",
      "pipeline.fit(X_train, y_train)\n",
      "\n",
      "Now let’s evaluate this classification pipeline:\n",
      "\n",
      ">>> pipeline.score(X_test, y_test)\n",
      "0.9822222222222222\n",
      "\n",
      "How about that? We almost divided the error rate by a factor of 2!\n",
      "\n",
      "But we chose the number of clusters k completely arbitrarily, we can surely do better.\n",
      "Since K-Means is just a preprocessing step in a classification pipeline, finding a good\n",
      "value for k is much simpler than earlier: there’s no need to perform silhouette analysis\n",
      "or minimize the inertia, the best value of k is simply the one that results in the best\n",
      "classification performance during cross-validation. Let’s use GridSearchCV to find the\n",
      "optimal number of clusters:\n",
      "\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "param_grid = dict(kmeans__n_clusters=range(2, 100))\n",
      "grid_clf = GridSearchCV(pipeline, param_grid, cv=3, verbose=2)\n",
      "grid_clf.fit(X_train, y_train)\n",
      "\n",
      "Let’s look at best value for k, and the performance of the resulting pipeline:\n",
      "\n",
      ">>> grid_clf.best_params_\n",
      "{'kmeans__n_clusters': 90}\n",
      ">>> grid_clf.score(X_test, y_test)\n",
      "0.9844444444444445\n",
      "\n",
      "With  k=90  clusters,  we  get  a  small  accuracy  boost,  reaching  98.4%  accuracy  on  the\n",
      "test set. Cool!\n",
      "\n",
      "Clustering \n",
      "\n",
      "| \n",
      "\n",
      "253\n",
      "\n",
      "\fUsing Clustering for Semi-Supervised Learning\n",
      "Another use case for clustering is in semi-supervised learning, when we have plenty\n",
      "of unlabeled instances and very few labeled instances. Let’s train a logistic regression\n",
      "model on a sample of 50 labeled instances from the digits dataset:\n",
      "\n",
      "n_labeled = 50\n",
      "log_reg = LogisticRegression()\n",
      "log_reg.fit(X_train[:n_labeled], y_train[:n_labeled])\n",
      "\n",
      "What is the performance of this model on the test set?\n",
      "\n",
      ">>> log_reg.score(X_test, y_test)\n",
      "0.8266666666666667\n",
      "\n",
      "The accuracy is just 82.7%: it should come as no surprise that this is much lower than\n",
      "earlier, when we trained the model on the full training set. Let’s see how we can do\n",
      "better. First, let’s cluster the training set into 50 clusters, then for each cluster let’s find\n",
      "the image closest to the centroid. We will call these images the representative images:\n",
      "\n",
      "k = 50\n",
      "kmeans = KMeans(n_clusters=k)\n",
      "X_digits_dist = kmeans.fit_transform(X_train)\n",
      "representative_digit_idx = np.argmin(X_digits_dist, axis=0)\n",
      "X_representative_digits = X_train[representative_digit_idx]\n",
      "\n",
      "Figure 9-13 shows these 50 representative images:\n",
      "\n",
      "Figure 9-13. Fifty representative digit images (one per cluster)\n",
      "\n",
      "Now let’s look at each image and manually label it:\n",
      "\n",
      "y_representative_digits = np.array([4, 8, 0, 6, 8, 3, ..., 7, 6, 2, 3, 1, 1])\n",
      "\n",
      "Now we have a dataset with just 50 labeled instances, but instead of being completely\n",
      "random instances, each of them is a representative image of its cluster. Let’s see if the\n",
      "performance is any better:\n",
      "\n",
      ">>> log_reg = LogisticRegression()\n",
      ">>> log_reg.fit(X_representative_digits, y_representative_digits)\n",
      ">>> log_reg.score(X_test, y_test)\n",
      "0.9244444444444444\n",
      "\n",
      "Wow! We jumped from 82.7% accuracy to 92.4%, although we are still only training\n",
      "the model on 50 instances. Since it is often costly and painful to label instances, espe‐\n",
      "\n",
      "254 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      "\fcially when it has to be done manually by experts, it is a good idea to label representa‐\n",
      "tive instances rather than just random instances.\n",
      "\n",
      "But  perhaps  we  can  go  one  step  further:  what  if  we  propagated  the  labels  to  all  the\n",
      "other instances in the same cluster? This is called label propagation:\n",
      "\n",
      "y_train_propagated = np.empty(len(X_train), dtype=np.int32)\n",
      "for i in range(k):\n",
      "    y_train_propagated[kmeans.labels_==i] = y_representative_digits[i]\n",
      "\n",
      "Now let’s train the model again and look at its performance:\n",
      "\n",
      ">>> log_reg = LogisticRegression()\n",
      ">>> log_reg.fit(X_train, y_train_propagated)\n",
      ">>> log_reg.score(X_test, y_test)\n",
      "0.9288888888888889\n",
      "\n",
      "We  got  a  tiny  little  accuracy  boost.  Better  than  nothing,  but  not  astounding.  The\n",
      "problem is that we propagated each representative instance’s label to all the instances\n",
      "in  the  same  cluster,  including  the  instances  located  close  to  the  cluster  boundaries,\n",
      "which are more likely to be mislabeled. Let’s see what happens if we only propagate\n",
      "the labels to the 20% of the instances that are closest to the centroids:\n",
      "\n",
      "percentile_closest = 20\n",
      "\n",
      "X_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]\n",
      "for i in range(k):\n",
      "    in_cluster = (kmeans.labels_ == i)\n",
      "    cluster_dist = X_cluster_dist[in_cluster]\n",
      "    cutoff_distance = np.percentile(cluster_dist, percentile_closest)\n",
      "    above_cutoff = (X_cluster_dist > cutoff_distance)\n",
      "    X_cluster_dist[in_cluster & above_cutoff] = -1\n",
      "\n",
      "partially_propagated = (X_cluster_dist != -1)\n",
      "X_train_partially_propagated = X_train[partially_propagated]\n",
      "y_train_partially_propagated = y_train_propagated[partially_propagated]\n",
      "\n",
      "Now let’s train the model again on this partially propagated dataset:\n",
      "\n",
      ">>> log_reg = LogisticRegression()\n",
      ">>> log_reg.fit(X_train_partially_propagated, y_train_partially_propagated)\n",
      ">>> log_reg.score(X_test, y_test)\n",
      "0.9422222222222222\n",
      "\n",
      "Nice! With just 50 labeled instances (only 5 examples per class on average!), we got\n",
      "94.2% performance, which is pretty close to the performance of logistic regression on\n",
      "the  fully  labeled  digits  dataset  (which  was  96.7%).  This  is  because  the  propagated\n",
      "labels are actually pretty good, their accuracy is very close to 99%:\n",
      "\n",
      "Clustering \n",
      "\n",
      "| \n",
      "\n",
      "255\n",
      "\n",
      "\f>>> np.mean(y_train_partially_propagated == y_train[partially_propagated])\n",
      "0.9896907216494846\n",
      "\n",
      "Active Learning\n",
      "To continue improving your model and your training set, the next step could be to do\n",
      "a few rounds of active learning: this is when a human expert interacts with the learn‐\n",
      "ing algorithm, providing labels when the algorithm needs them. There are many dif‐\n",
      "ferent  strategies  for  active  learning,  but  one  of  the  most  common  ones  is  called\n",
      "uncertainty sampling:\n",
      "\n",
      "• The model is trained on the labeled instances gathered so far, and this model is\n",
      "\n",
      "used to make predictions on all the unlabeled instances.\n",
      "\n",
      "• The  instances  for  which  the  model  is  most  uncertain  (i.e.,  when  its  estimated\n",
      "\n",
      "probability is lowest) must be labeled by the expert.\n",
      "\n",
      "• Then  you  just  iterate  this  process  again  and  again,  until  the  performance\n",
      "\n",
      "improvement stops being worth the labeling effort.\n",
      "\n",
      "Other strategies include labeling the instances that would result in the largest model\n",
      "change, or the largest drop in the model’s validation error, or the instances that differ‐\n",
      "ent models disagree on (e.g., an SVM, a Random Forest, and so on).\n",
      "\n",
      "Before  we  move  on  to  Gaussian  mixture  models,  let’s  take  a  look  at  DBSCAN,\n",
      "another popular clustering algorithm that illustrates a very different approach based\n",
      "on local density estimation. This approach allows the algorithm to identify clusters of\n",
      "arbitrary shapes.\n",
      "\n",
      "DBSCAN\n",
      "This  algorithm  defines  clusters  as  continuous  regions  of  high  density.  It  is  actually\n",
      "quite simple:\n",
      "\n",
      "• For each instance, the algorithm counts how many instances are located within a\n",
      "small  distance  ε  (epsilon)  from  it.  This  region  is  called  the  instance’s  ε-\n",
      "neighborhood.\n",
      "\n",
      "• If an instance has at least min_samples instances in its ε-neighborhood (includ‐\n",
      "ing itself), then it is considered a core instance. In other words, core instances are\n",
      "those that are located in dense regions.\n",
      "\n",
      "• All instances in the neighborhood of a core instance belong to the same cluster.\n",
      "This may include other core instances, therefore a long sequence of neighboring\n",
      "core instances forms a single cluster.\n",
      "\n",
      "256 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      "\f• Any instance that is not a core instance and does not have one in its neighbor‐\n",
      "\n",
      "hood is considered an anomaly.\n",
      "\n",
      "This algorithm works well if all the clusters are dense enough, and they are well sepa‐\n",
      "rated by low-density regions. The DBSCAN class in Scikit-Learn is as simple to use as\n",
      "you might expect. Let’s test it on the moons dataset, introduced in Chapter 5:\n",
      "\n",
      "from sklearn.cluster import DBSCAN\n",
      "from sklearn.datasets import make_moons\n",
      "\n",
      "X, y = make_moons(n_samples=1000, noise=0.05)\n",
      "dbscan = DBSCAN(eps=0.05, min_samples=5)\n",
      "dbscan.fit(X)\n",
      "\n",
      "The labels of all the instances are now available in the labels_ instance variable:\n",
      "\n",
      ">>> dbscan.labels_\n",
      "array([ 0,  2, -1, -1,  1,  0,  0,  0, ...,  3,  2,  3,  3,  4,  2,  6,  3])\n",
      "\n",
      "Notice that some instances have a cluster index equal to -1: this means that they are\n",
      "considered as anomalies by the algorithm. The indices of the core instances are avail‐\n",
      "able  in  the  core_sample_indices_  instance  variable,  and  the  core  instances  them‐\n",
      "selves are available in the components_ instance variable:\n",
      "\n",
      ">>> len(dbscan.core_sample_indices_)\n",
      "808\n",
      ">>> dbscan.core_sample_indices_\n",
      "array([ 0,  4,  5,  6,  7,  8, 10, 11, ..., 992, 993, 995, 997, 998, 999])\n",
      ">>> dbscan.components_\n",
      "array([[-0.02137124,  0.40618608],\n",
      "       [-0.84192557,  0.53058695],\n",
      "                  ...\n",
      "       [-0.94355873,  0.3278936 ],\n",
      "       [ 0.79419406,  0.60777171]])\n",
      "\n",
      "This clustering is represented in the left plot of Figure 9-14. As you can see, it identi‐\n",
      "fied quite a lot of anomalies, plus 7 different clusters. How disappointing! Fortunately,\n",
      "if we widen each instance’s neighborhood by increasing eps to 0.2, we get the cluster‐\n",
      "ing on the right, which looks perfect. Let’s continue with this model.\n",
      "\n",
      "Figure 9-14. DBSCAN clustering using two different neighborhood radiuses\n",
      "\n",
      "Clustering \n",
      "\n",
      "| \n",
      "\n",
      "257\n",
      "\n",
      "\fSomewhat  surprisingly,  the  DBSCAN  class  does  not  have  a  predict()  method,\n",
      "although  it  has  a  fit_predict()  method.  In  other  words,  it  cannot  predict  which\n",
      "cluster a new instance belongs to. The rationale for this decision is that several classi‐\n",
      "fication  algorithms  could  make  sense  here,  and  it  is  easy  enough  to  train  one,  for\n",
      "example a KNeighborsClassifier:\n",
      "\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "knn = KNeighborsClassifier(n_neighbors=50)\n",
      "knn.fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_])\n",
      "\n",
      "Now, given a few new instances, we can predict which cluster they most likely belong\n",
      "to, and even estimate a probability for each cluster. Note that we only trained them on\n",
      "the core instances, but we could also have chosen to train them on all the instances,\n",
      "or all but the anomalies: this choice depends on the final task.\n",
      "\n",
      ">>> X_new = np.array([[-0.5, 0], [0, 0.5], [1, -0.1], [2, 1]])\n",
      ">>> knn.predict(X_new)\n",
      "array([1, 0, 1, 0])\n",
      ">>> knn.predict_proba(X_new)\n",
      "array([[0.18, 0.82],\n",
      "       [1.  , 0.  ],\n",
      "       [0.12, 0.88],\n",
      "       [1.  , 0.  ]])\n",
      "\n",
      "The  decision  boundary  is  represented  on  Figure  9-15  (the  crosses  represent  the  4\n",
      "instances in X_new). Notice that since there is no anomaly in the KNN’s training set,\n",
      "the classifier always chooses a cluster, even when that cluster is far away. However, it\n",
      "is  fairly  straightforward  to  introduce  a  maximum  distance,  in  which  case  the  two\n",
      "instances that are far away from both clusters are classified as anomalies. To do this,\n",
      "we  can  use  the  kneighbors()  method  of  the  KNeighborsClassifier:  given  a  set  of\n",
      "instances,  it  returns  the  distances  and  the  indices  of  the  k  nearest  neighbors  in  the\n",
      "training set (two matrices, each with k columns):\n",
      "\n",
      ">>> y_dist, y_pred_idx = knn.kneighbors(X_new, n_neighbors=1)\n",
      ">>> y_pred = dbscan.labels_[dbscan.core_sample_indices_][y_pred_idx]\n",
      ">>> y_pred[y_dist > 0.2] = -1\n",
      ">>> y_pred.ravel()\n",
      "array([-1,  0,  1, -1])\n",
      "\n",
      "258 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      "\fFigure 9-15. cluster_classification_diagram\n",
      "\n",
      "In short, DBSCAN is a very simple yet powerful algorithm, capable of identifying any\n",
      "number  of  clusters,  of  any  shape,  it  is  robust  to  outliers,  and  it  has  just  two  hyper‐\n",
      "parameters (eps and min_samples). However, if the density varies significantly across\n",
      "the clusters, it can be impossible for it to capture all the clusters properly. Moreover,\n",
      "its computational complexity is roughly O(m log m), making it pretty close to linear\n",
      "with regards to the number of instances. However, Scikit-Learn’s implementation can\n",
      "require up to O(m2) memory if eps is large.\n",
      "\n",
      "Other Clustering Algorithms\n",
      "Scikit-Learn  implements  several  more  clustering  algorithms  that  you  should  take  a\n",
      "look at. We cannot cover them all in detail here, but here is a brief overview:\n",
      "\n",
      "• Agglomerative  clustering:  a  hierarchy  of  clusters  is  built  from  the  bottom  up.\n",
      "Think  of  many  tiny  bubbles  floating  on  water  and  gradually  attaching  to  each\n",
      "other  until  there’s  just  one  big  group  of  bubbles.  Similarly,  at  each  iteration\n",
      "agglomerative clustering connects the nearest pair of clusters (starting with indi‐\n",
      "vidual instances). If you draw a tree with a branch for every pair of clusters that\n",
      "merged,  you  get  a  binary  tree  of  clusters,  where  the  leaves  are  the  individual\n",
      "instances. This approach scales very well to large numbers of instances or clus‐\n",
      "ters, it can capture clusters of various shapes, it produces a flexible and informa‐\n",
      "tive cluster tree instead of forcing you to choose a particular cluster scale, and it\n",
      "can  be  used  with  any  pairwise  distance.  It  can  scale  nicely  to  large  numbers  of\n",
      "instances  if  you  provide  a  connectivity  matrix.  This  is  a  sparse  m  by  m  matrix\n",
      "that  indicates  which  pairs  of  instances  are  neighbors  (e.g.,  returned  by\n",
      "sklearn.neighbors.kneighbors_graph()).  Without  a  connectivity  matrix,  the\n",
      "algorithm does not scale well to large datasets.\n",
      "\n",
      "• Birch: this algorithm was designed specifically for very large datasets, and it can\n",
      "be faster than batch K-Means, with similar results, as long as the number of fea‐\n",
      "tures is not too large (<20). It builds a tree structure during training containing\n",
      "\n",
      "Clustering \n",
      "\n",
      "| \n",
      "\n",
      "259\n",
      "\n",
      "\fjust enough information to quickly assign each new instance to a cluster, without\n",
      "having to store all the instances in the tree: this allows it to use limited memory,\n",
      "while handle huge datasets.\n",
      "\n",
      "• Mean-shift:  this  algorithm  starts  by  placing  a  circle  centered  on  each  instance,\n",
      "then  for  each  circle  it  computes  the  mean  of  all  the  instances  located  within  it,\n",
      "and  it  shifts  the  circle  so  that  it  is  centered  on  the  mean.  Next,  it  iterates  this\n",
      "mean-shift step until all the circles stop moving (i.e., until each of them is cen‐\n",
      "tered on the mean of the instances it contains). This algorithm shifts the circles\n",
      "in  the  direction  of  higher  density,  until  each  of  them  has  found  a  local  density\n",
      "maximum. Finally, all the instances whose circles have settled in the same place\n",
      "(or close enough) are assigned to the same cluster. This has some of the same fea‐\n",
      "tures as DBSCAN, in particular it can find any number of clusters of any shape, it\n",
      "has just one hyperparameter (the radius of the circles, called the bandwidth) and\n",
      "it relies on local density estimation. However, it tends to chop clusters into pieces\n",
      "when  they  have  internal  density  variations.  Unfortunately,  its  computational\n",
      "complexity is O(m2), so it is not suited for large datasets.\n",
      "\n",
      "• Affinity propagation: this algorithm uses a voting system, where instances vote for\n",
      "similar  instances  to  be  their  representatives,  and  once  the  algorithm  converges,\n",
      "each  representative  and  its  voters  form  a  cluster.  This  algorithm  can  detect  any\n",
      "number of clusters of different sizes. Unfortunately, this algorithm has a compu‐\n",
      "tational complexity of O(m2), so it is not suited for large datasets.\n",
      "\n",
      "• Spectral clustering: this algorithm takes a similarity matrix between the instances\n",
      "and creates a low-dimensional embedding from it (i.e., it reduces its dimension‐\n",
      "ality),  then  it  uses  another  clustering  algorithm  in  this  low-dimensional  space\n",
      "(Scikit-Learn’s  implementation  uses  K-Means).  Spectral  clustering  can  capture\n",
      "complex cluster structures, and it can also be used to cut graphs (e.g., to identify\n",
      "clusters  of  friends  on  a  social  network),  however  it  does  not  scale  well  to  large\n",
      "number of instances, and it does not behave well when the clusters have very dif‐\n",
      "ferent sizes.\n",
      "\n",
      "Now let’s dive into Gaussian mixture models, which can be used for density estima‐\n",
      "tion, clustering and anomaly detection.\n",
      "\n",
      "Gaussian Mixtures\n",
      "A  Gaussian  mixture  model  (GMM)  is  a  probabilistic  model  that  assumes  that  the\n",
      "instances  were  generated  from  a  mixture  of  several  Gaussian  distributions  whose\n",
      "parameters are unknown. All the instances generated from a single Gaussian distri‐\n",
      "bution form a cluster that typically looks like an ellipsoid. Each cluster can have a dif‐\n",
      "ferent ellipsoidal shape, size, density and orientation, just like in Figure 9-11. When\n",
      "you observe an instance, you know it was generated from one of the Gaussian distri‐\n",
      "\n",
      "260 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      "\fbutions, but you are not told which one, and you do not know what the parameters of\n",
      "these distributions are.\n",
      "\n",
      "There  are  several  GMM  variants:  in  the  simplest  variant,  implemented  in  the  Gaus\n",
      "sianMixture  class,  you  must  know  in  advance  the  number  k  of  Gaussian  distribu‐\n",
      "tions.  The  dataset  X  is  assumed  to  have  been  generated  through  the  following\n",
      "probabilistic process:\n",
      "\n",
      "• For each instance, a cluster is picked randomly among k clusters. The probability\n",
      "of choosing the jth cluster is defined by the cluster’s weight ϕ(j).7 The index of the\n",
      "cluster chosen for the ith instance is noted z(i).\n",
      "\n",
      "• If z(i)=j, meaning the ith instance has been assigned to the jth cluster, the location\n",
      "x(i)  of  this  instance  is  sampled  randomly  from  the  Gaussian  distribution  with\n",
      "mean μ(j) and covariance matrix Σ(j). This is noted  i ∼  μ j , Σ j\n",
      "\n",
      ".\n",
      "\n",
      "This  generative  process  can  be  represented  as  a  graphical  model  (see  Figure  9-16).\n",
      "This  is  a  graph  which  represents  the  structure  of  the  conditional  dependencies\n",
      "between random variables.\n",
      "\n",
      "Figure 9-16. Gaussian mixture model\n",
      "\n",
      "Here is how to interpret it:8\n",
      "\n",
      "• The circles represent random variables.\n",
      "\n",
      "• The squares represent fixed values (i.e., parameters of the model).\n",
      "\n",
      "7 Phi (ϕ or φ) is the 21st letter of the Greek alphabet.\n",
      "\n",
      "8 Most of these notations are standard, but a few additional notations were taken from the Wikipedia article on\n",
      "\n",
      "plate notation.\n",
      "\n",
      "Gaussian Mixtures \n",
      "\n",
      "| \n",
      "\n",
      "261\n",
      "\n",
      "\f• The large rectangles are called plates: they indicate that their content is repeated\n",
      "\n",
      "several times.\n",
      "\n",
      "• The number indicated at the bottom right hand side of each plate indicates how\n",
      "many times its content is repeated, so there are m random variables z(i) (from z(1)\n",
      "to z(m)) and m random variables x(i), and k means μ(j) and k covariance matrices\n",
      "Σ(j), but just one weight vector ϕ (containing all the weights ϕ(1) to ϕ(k)).\n",
      "\n",
      "• Each variable z(i) is drawn from the categorical distribution with weights ϕ. Each\n",
      "variable x(i) is drawn from the normal distribution with the mean and covariance\n",
      "matrix defined by its cluster z(i).\n",
      "\n",
      "• The solid arrows represent conditional dependencies. For example, the probabil‐\n",
      "ity  distribution  for  each  random  variable  z(i)  depends  on  the  weight  vector  ϕ.\n",
      "Note that when an arrow crosses a plate boundary, it means that it applies to all\n",
      "the  repetitions  of  that  plate,  so  for  example  the  weight  vector  ϕ  conditions  the\n",
      "probability distributions of all the random variables x(1) to x(m).\n",
      "\n",
      "• The squiggly arrow from z(i) to x(i) represents a switch: depending on the value of\n",
      "z(i),  the  instance  x(i)  will  be  sampled  from  a  different  Gaussian  distribution.  For\n",
      "example, if z(i)=j, then  i ∼  μ j , Σ j\n",
      ".\n",
      "\n",
      "• Shaded nodes indicate that the value is known, so in this case only the random\n",
      "variables x(i) have known values: they are called observed variables. The unknown\n",
      "random variables z(i) are called latent variables.\n",
      "\n",
      "So what can you do with such a model? Well, given the dataset X, you typically want\n",
      "to start by estimating the weights ϕ and all the distribution parameters μ(1) to μ(k) and\n",
      "Σ(1) to Σ(k). Scikit-Learn’s GaussianMixture class makes this trivial:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "\n",
      "gm = GaussianMixture(n_components=3, n_init=10)\n",
      "gm.fit(X)\n",
      "\n",
      "Let’s look at the parameters that the algorithm estimated:\n",
      "\n",
      ">>> gm.weights_\n",
      "array([0.20965228, 0.4000662 , 0.39028152])\n",
      ">>> gm.means_\n",
      "array([[ 3.39909717,  1.05933727],\n",
      "       [-1.40763984,  1.42710194],\n",
      "       [ 0.05135313,  0.07524095]])\n",
      ">>> gm.covariances_\n",
      "array([[[ 1.14807234, -0.03270354],\n",
      "        [-0.03270354,  0.95496237]],\n",
      "\n",
      "       [[ 0.63478101,  0.72969804],\n",
      "        [ 0.72969804,  1.1609872 ]],\n",
      "\n",
      "262 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      "\f       [[ 0.68809572,  0.79608475],\n",
      "        [ 0.79608475,  1.21234145]]])\n",
      "\n",
      "Great,  it  worked  fine!  Indeed,  the  weights  that  were  used  to  generate  the  data  were\n",
      "0.2, 0.4 and 0.4, and similarly, the means and covariance matrices were very close to\n",
      "those  found  by  the  algorithm.  But  how?  This  class  relies  on  the  Expectation-\n",
      "Maximization (EM) algorithm, which has many similarities with the K-Means algo‐\n",
      "rithm:  it  also  initializes  the  cluster  parameters  randomly,  then  it  repeats  two  steps\n",
      "until  convergence,  first  assigning  instances  to  clusters  (this  is  called  the  expectation\n",
      "step) then updating the clusters (this is called the maximization step). Sounds famil‐\n",
      "iar? Indeed, in the context of clustering you can think of EM as a generalization of K-\n",
      "Means which not only finds the cluster centers (μ(1) to μ(k)), but also their size, shape\n",
      "and orientation (Σ(1) to Σ(k)), as well as their relative weights (ϕ(1) to ϕ(k)). Unlike K-\n",
      "Means,  EM  uses  soft  cluster  assignments  rather  than  hard  assignments:  for  each\n",
      "instance  during  the  expectation  step,  the  algorithm  estimates  the  probability  that  it\n",
      "belongs  to  each  cluster  (based  on  the  current  cluster  parameters).  Then,  during  the\n",
      "maximization step, each cluster is updated using all the instances in the dataset, with\n",
      "each  instance  weighted  by  the  estimated  probability  that  it  belongs  to  that  cluster.\n",
      "These probabilities are called the responsibilities of the clusters for the instances. Dur‐\n",
      "ing  the  maximization  step,  each  cluster’s  update  will  mostly  be  impacted  by  the\n",
      "instances it is most responsible for.\n",
      "\n",
      "Unfortunately,  just  like  K-Means,  EM  can  end  up  converging  to\n",
      "poor solutions, so it needs to be run several times, keeping only the\n",
      "best solution. This is why we set n_init to 10. Be careful: by default\n",
      "n_init is only set to 1.\n",
      "\n",
      "You  can  check  whether  or  not  the  algorithm  converged  and  how  many  iterations  it\n",
      "took:\n",
      "\n",
      ">>> gm.converged_\n",
      "True\n",
      ">>> gm.n_iter_\n",
      "3\n",
      "\n",
      "Okay, now that you have an estimate of the location, size, shape, orientation and rela‐\n",
      "tive weight of each cluster, the model can easily assign each instance to the most likely\n",
      "cluster  (hard  clustering)  or  estimate  the  probability  that  it  belongs  to  a  particular\n",
      "cluster (soft clustering). For this, just use the predict() method for hard clustering,\n",
      "or the predict_proba() method for soft clustering:\n",
      "\n",
      ">>> gm.predict(X)\n",
      "array([2, 2, 1, ..., 0, 0, 0])\n",
      ">>> gm.predict_proba(X)\n",
      "array([[2.32389467e-02, 6.77397850e-07, 9.76760376e-01],\n",
      "       [1.64685609e-02, 6.75361303e-04, 9.82856078e-01],\n",
      "\n",
      "Gaussian Mixtures \n",
      "\n",
      "| \n",
      "\n",
      "263\n",
      "\n",
      "\f       [2.01535333e-06, 9.99923053e-01, 7.49319577e-05],\n",
      "       ...,\n",
      "       [9.99999571e-01, 2.13946075e-26, 4.28788333e-07],\n",
      "       [1.00000000e+00, 1.46454409e-41, 5.12459171e-16],\n",
      "       [1.00000000e+00, 8.02006365e-41, 2.27626238e-15]])\n",
      "\n",
      "It is a generative model, meaning you can actually sample new instances from it (note\n",
      "that they are ordered by cluster index):\n",
      "\n",
      ">>> X_new, y_new = gm.sample(6)\n",
      ">>> X_new\n",
      "array([[ 2.95400315,  2.63680992],\n",
      "       [-1.16654575,  1.62792705],\n",
      "       [-1.39477712, -1.48511338],\n",
      "       [ 0.27221525,  0.690366  ],\n",
      "       [ 0.54095936,  0.48591934],\n",
      "       [ 0.38064009, -0.56240465]])\n",
      "\n",
      ">>> y_new\n",
      "array([0, 1, 2, 2, 2, 2])\n",
      "\n",
      "It is also possible to estimate the density of the model at any given location. This is\n",
      "achieved  using  the  score_samples()  method:  for  each  instance  it  is  given,  this\n",
      "method  estimates  the  log  of  the  probability  density  function  (PDF)  at  that  location.\n",
      "The greater the score, the higher the density:\n",
      "\n",
      ">>> gm.score_samples(X)\n",
      "array([-2.60782346, -3.57106041, -3.33003479, ..., -3.51352783,\n",
      "       -4.39802535, -3.80743859])\n",
      "\n",
      "If you compute the exponential of these scores, you get the value of the PDF at the\n",
      "location of the given instances. These are not probabilities, but probability densities:\n",
      "they can take on any positive value, not just between 0 and 1. To estimate the proba‐\n",
      "bility that an instance will fall within a particular region, you would have to integrate\n",
      "the PDF over that region (if you do so over the entire space of possible instance loca‐\n",
      "tions, the result will be 1).\n",
      "\n",
      "Figure 9-17 shows the cluster means, the decision boundaries (dashed lines), and the\n",
      "density contours of this model:\n",
      "\n",
      "264 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      "\fFigure 9-17. Cluster means, decision boundaries and density contours of a trained Gaus‐\n",
      "sian mixture model\n",
      "\n",
      "Nice! The algorithm clearly found an excellent solution. Of course, we made its task\n",
      "easy by actually generating the data using a set of 2D Gaussian distributions (unfortu‐\n",
      "nately, real life data is not always so Gaussian and low-dimensional), and we also gave\n",
      "the  algorithm  the  correct  number  of  clusters.  When  there  are  many  dimensions,  or\n",
      "many clusters, or few instances, EM can struggle to converge to the optimal solution.\n",
      "You might need to reduce the difficulty of the task by limiting the number of parame‐\n",
      "ters that the algorithm has to learn: one way to do this is to limit the range of shapes\n",
      "and  orientations  that  the  clusters  can  have.  This  can  be  achieved  by  imposing  con‐\n",
      "straints on the covariance matrices. To do this, just set the covariance_type hyper‐\n",
      "parameter to one of the following values:\n",
      "\n",
      "• \"spherical\": all clusters must be spherical, but they can have different diameters\n",
      "\n",
      "(i.e., different variances).\n",
      "\n",
      "• \"diag\": clusters can take on any ellipsoidal shape of any size, but the ellipsoid’s\n",
      "axes must be parallel to the coordinate axes (i.e., the covariance matrices must be\n",
      "diagonal).\n",
      "\n",
      "• \"tied\":  all  clusters  must  have  the  same  ellipsoidal  shape,  size  and  orientation\n",
      "\n",
      "(i.e., all clusters share the same covariance matrix).\n",
      "\n",
      "By  default,  covariance_type  is  equal  to  \"full\",  which  means  that  each  cluster  can\n",
      "take  on  any  shape,  size  and  orientation  (it  has  its  own  unconstrained  covariance\n",
      "matrix).  Figure  9-18  plots  the  solutions  found  by  the  EM  algorithm  when  cova\n",
      "riance_type is set to \"tied\" or \"spherical“.\n",
      "\n",
      "Gaussian Mixtures \n",
      "\n",
      "| \n",
      "\n",
      "265\n",
      "\n",
      "\fFigure 9-18. covariance_type_diagram\n",
      "\n",
      "The  computational  complexity  of  training  a  GaussianMixture\n",
      "model  depends  on  the  number  of  instances  m,  the  number  of\n",
      "dimensions n, the number of clusters k, and the constraints on the\n",
      "covariance matrices. If covariance_type is \"spherical or \"diag\",\n",
      "it is O(kmn), assuming the data has a clustering structure. If cova\n",
      "riance_type is \"tied\" or \"full\", it is O(kmn2 + kn3), so it will not\n",
      "scale to large numbers of features.\n",
      "\n",
      "Gaussian mixture models can also be used for anomaly detection. Let’s see how.\n",
      "\n",
      "Anomaly Detection using Gaussian Mixtures\n",
      "Anomaly detection (also called outlier detection) is the task of detecting instances that\n",
      "deviate  strongly  from  the  norm.  These  instances  are  of  course  called  anomalies  or\n",
      "outliers, while the normal instances are called inliers. Anomaly detection is very use‐\n",
      "ful in a wide variety of applications, for example in fraud detection, or for detecting\n",
      "defective  products  in  manufacturing,  or  to  remove  outliers  from  a  dataset  before\n",
      "training  another  model,  which  can  significantly  improve  the  performance  of  the\n",
      "resulting model.\n",
      "\n",
      "Using a Gaussian mixture model for anomaly detection is quite simple: any instance\n",
      "located in a low-density region can be considered an anomaly. You must define what\n",
      "density  threshold  you  want  to  use.  For  example,  in  a  manufacturing  company  that\n",
      "tries  to  detect  defective  products,  the  ratio  of  defective  products  is  usually  well-\n",
      "known. Say it is equal to 4%, then you can set the density threshold to be the value\n",
      "that results in having 4% of the instances located in areas below that threshold den‐\n",
      "sity. If you notice that you get too many false positives (i.e., perfectly good products\n",
      "that are flagged as defective), you can lower the threshold. Conversely, if you have too\n",
      "many  false  negatives  (i.e.,  defective  products  that  the  system  does  not  flag  as  defec‐\n",
      "tive),  you  can  increase  the  threshold.  This  is  the  usual  precision/recall  tradeoff  (see\n",
      "Chapter 3). Here is how you would identify the outliers using the 4th percentile low‐\n",
      "\n",
      "266 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      "\fest density as the threshold (i.e., approximately 4% of the instances will be flagged as\n",
      "anomalies):\n",
      "\n",
      "densities = gm.score_samples(X)\n",
      "density_threshold = np.percentile(densities, 4)\n",
      "anomalies = X[densities < density_threshold]\n",
      "\n",
      "These anomalies are represented as stars on Figure 9-19:\n",
      "\n",
      "Figure 9-19. Anomaly detection using a Gaussian mixture model\n",
      "\n",
      "A closely related task is novelty detection: it differs from anomaly detection in that the\n",
      "algorithm is assumed to be trained on a “clean” dataset, uncontaminated by outliers,\n",
      "whereas anomaly detection does not make this assumption. Indeed, outlier detection\n",
      "is often precisely used to clean up a dataset.\n",
      "\n",
      "Gaussian mixture models try to fit all the data, including the outli‐\n",
      "ers, so if you have too many of them, this will bias the model’s view\n",
      "of  “normality”:  some  outliers  may  wrongly  be  considered  as  nor‐\n",
      "mal.  If  this  happens,  you  can  try  to  fit  the  model  once,  use  it  to\n",
      "detect  and  remove  the  most  extreme  outliers,  then  fit  the  model\n",
      "again on the cleaned up dataset. Another approach is to use robust\n",
      "covariance estimation methods (see the EllipticEnvelope class).\n",
      "\n",
      "Just like K-Means, the GaussianMixture algorithm requires you to specify the num‐\n",
      "ber of clusters. So how can you find it?\n",
      "\n",
      "Selecting the Number of Clusters\n",
      "With K-Means, you could use the inertia or the silhouette score to select the appro‐\n",
      "priate number of clusters, but with Gaussian mixtures, it is not possible to use these\n",
      "metrics because they are not reliable when the clusters are not spherical or have dif‐\n",
      "ferent sizes. Instead, you can try to find the model that minimizes a theoretical infor‐\n",
      "\n",
      "Gaussian Mixtures \n",
      "\n",
      "| \n",
      "\n",
      "267\n",
      "\n",
      "\fmation  criterion  such  as  the  Bayesian  information  criterion  (BIC)  or  the  Akaike\n",
      "information criterion (AIC), defined in Equation 9-1.\n",
      "\n",
      "Equation 9-1. Bayesian information criterion (BIC) and Akaike information\n",
      "criterion (AIC)\n",
      "\n",
      "BIC =\n",
      "\n",
      "log m p − 2 log L\n",
      "\n",
      "AIC = 2p − 2 log L\n",
      "\n",
      "• m is the number of instances, as always.\n",
      "\n",
      "• p is the number of parameters learned by the model.\n",
      "\n",
      "• L is the maximized value of the likelihood function of the model.\n",
      "\n",
      "Both the BIC and the AIC penalize models that have more parameters to learn (e.g.,\n",
      "more clusters), and reward models that fit the data well. They often end up selecting\n",
      "the same model, but when they differ, the model selected by the BIC tends to be sim‐\n",
      "pler (fewer parameters) than the one selected by the AIC, but it does not fit the data\n",
      "quite as well (this is especially true for larger datasets).\n",
      "\n",
      "Likelihood function\n",
      "The  terms  “probability”  and  “likelihood”  are  often  used  interchangeably  in  the\n",
      "English language, but they have very different meanings in statistics: given a statistical\n",
      "model with some parameters θ, the word “probability” is used to describe how plausi‐\n",
      "ble a future outcome x is (knowing the parameter values θ), while the word “likeli‐\n",
      "hood”  is  used  to  describe  how  plausible  a  particular  set  of  parameter  values  θ  are,\n",
      "after the outcome x is known.\n",
      "\n",
      "Consider a one-dimensional mixture model of two Gaussian distributions centered at\n",
      "-4  and  +1.  For  simplicity,  this  toy  model  has  a  single  parameter  θ  that  controls  the\n",
      "standard  deviations  of  both  distributions.  The  top  left  contour  plot  in  Figure  9-20\n",
      "shows the entire model f(x; θ) as a function of both x and θ. To estimate the probabil‐\n",
      "ity  distribution  of  a  future  outcome  x,  you  need  to  set  the  model  parameter  θ.  For\n",
      "example,  if  you  set  it  to  θ=1.3  (the  horizontal  line),  you  get  the  probability  density\n",
      "function f(x; θ=1.3) shown in the lower left plot. Say you want to estimate the proba‐\n",
      "bility that x will fall between -2 and +2, you must calculate the integral of the PDF on\n",
      "this  range  (i.e.,  the  surface  of  the  shaded  region).  On  the  other  hand,  if  you  have\n",
      "observed a single instance x=2.5 (the vertical line in the upper left plot), you get the\n",
      "likelihood function noted ℒ (θ|x=2.5)=f(x=2.5; θ) represented in the upper right plot.\n",
      "\n",
      "In short, the PDF is a function of x (with θ fixed) while the likelihood function is a\n",
      "function of θ (with x fixed). It is important to understand that the likelihood function\n",
      "is  not  a  probability  distribution:  if  you  integrate  a  probability  distribution  over  all\n",
      "\n",
      "268 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      "\fpossible values of x, you always get 1, but if you integrate the likelihood function over\n",
      "all possible values of θ, the result can be any positive value.\n",
      "\n",
      "Figure 9-20. A model’s parametric function (top left), and some derived functions: a PDF\n",
      "(lower left), a likelihood function (top right) and a log likelihood function (lower right)\n",
      "\n",
      "Given a dataset X, a common task is to try to estimate the most likely values for the\n",
      "model parameters. To do this, you must find the values that maximize the likelihood\n",
      "function, given X. In this example, if you have observed a single instance x=2.5, the\n",
      "maximum likelihood estimate (MLE) of θ is θ=1.5. If a prior probability distribution g\n",
      "over  θ  exists,  it  is  possible  to  take  it  into  account  by  maximizing  ℒ (θ|x)g(θ)  rather\n",
      "than just maximizing ℒ (θ|x). This is called maximum a-posteriori (MAP) estimation.\n",
      "Since MAP constrains the parameter values, you can think of it as a regularized ver‐\n",
      "sion of MLE.\n",
      "\n",
      "Notice  that  it  is  equivalent  to  maximize  the  likelihood  function  or  to  maximize  its\n",
      "logarithm (represented in the lower right hand side of Figure 9-20): indeed, the loga‐\n",
      "rithm  is  a  strictly  increasing  function,  so  if  θ  maximizes  the  log  likelihood,  it  also\n",
      "maximizes the likelihood. It turns out that it is generally easier to maximize the log\n",
      "likelihood. For example, if you observed several independent instances x(1) to x(m), you\n",
      "would need to find the value of θ that maximizes the product of the individual likeli‐\n",
      "hood functions. But it is equivalent, and much simpler, to maximize the sum (not the\n",
      "product) of the log likelihood functions, thanks to the magic of the logarithm which\n",
      "converts products into sums: log(ab)=log(a)+log(b).\n",
      "\n",
      "Once  you  have  estimated  θ,  the  value  of  θ  that  maximizes  the  likelihood  function,\n",
      "then you are ready to compute L = ℒ θ,  . This is the value which is used to com‐\n",
      "pute the AIC and BIC: you can think of it as a measure of how well the model fits the\n",
      "data.\n",
      "\n",
      "To compute the BIC and AIC, just call the bic() or aic() methods:\n",
      "\n",
      "Gaussian Mixtures \n",
      "\n",
      "| \n",
      "\n",
      "269\n",
      "\n",
      "\f>>> gm.bic(X)\n",
      "8189.74345832983\n",
      ">>> gm.aic(X)\n",
      "8102.518178214792\n",
      "\n",
      "Figure 9-21 shows the BIC for different numbers of clusters k. As you can see, both\n",
      "the BIC and the AIC are lowest when k=3, so it is most likely the best choice. Note\n",
      "that we could also search for the best value for the covariance_type hyperparameter.\n",
      "For example, if it is \"spherical\" rather than \"full\", then the model has much fewer\n",
      "parameters to learn, but it does not fit the data as well.\n",
      "\n",
      "Figure 9-21. AIC and BIC for different numbers of clusters k\n",
      "\n",
      "Bayesian Gaussian Mixture Models\n",
      "Rather than manually searching for the optimal number of clusters, it is possible to\n",
      "use  instead  the  BayesianGaussianMixture  class  which  is  capable  of  giving  weights\n",
      "equal (or close) to zero to unnecessary clusters. Just set the number of clusters n_com\n",
      "ponents to a value that you have good reason to believe is greater than the optimal\n",
      "number  of  clusters  (this  assumes  some  minimal  knowledge  about  the  problem  at\n",
      "hand),  and  the  algorithm  will  eliminate  the  unnecessary  clusters  automatically.  For\n",
      "example, let’s set the number of clusters to 10 and see what happens:\n",
      "\n",
      ">>> from sklearn.mixture import BayesianGaussianMixture\n",
      ">>> bgm = BayesianGaussianMixture(n_components=10, n_init=10, random_state=42)\n",
      ">>> bgm.fit(X)\n",
      ">>> np.round(bgm.weights_, 2)\n",
      "array([0.4 , 0.21, 0.4 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ])\n",
      "\n",
      "Perfect: the algorithm automatically detected that only 3 clusters are needed, and the\n",
      "resulting clusters are almost identical to the ones in Figure 9-17.\n",
      "\n",
      "In  this  model,  the  cluster  parameters  (including  the  weights,  means  and  covariance\n",
      "matrices)  are  not  treated  as  fixed  model  parameters  anymore,  but  as  latent  random\n",
      "variables, like the cluster assignments (see Figure 9-22). So z now includes both the\n",
      "cluster parameters and the cluster assignments.\n",
      "\n",
      "270 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      "\fFigure 9-22. Bayesian Gaussian mixture model\n",
      "\n",
      "Prior knowledge about the latent variables z can be encoded in a probability distribu‐\n",
      "tion p(z) called the prior. For example, we may have a prior belief that the clusters are\n",
      "likely  to  be  few  (low  concentration),  or  conversely,  that  they  are  more  likely  to  be\n",
      "plentiful  (high  concentration).  This  can  be  adjusted  using  the  weight_concentra\n",
      "tion_prior hyperparameter. Setting it to 0.01 or 1000 gives very different clusterings\n",
      "(see Figure 9-23). However, the more data we have, the less the priors matter. In fact,\n",
      "to plot diagrams with such large differences, you must use very strong priors and lit‐\n",
      "tle data.\n",
      "\n",
      "Figure 9-23. Using different concentration priors\n",
      "\n",
      "The fact that you see only 3 regions in the right plot although there\n",
      "are  4  centroids  is  not  a  bug:  the  weight  of  the  top-right  cluster  is\n",
      "much larger than the weight of the lower-right cluster, so the prob‐\n",
      "ability that any given point in this region belongs to the top-right\n",
      "cluster is greater than the probability that it belongs to the lower-\n",
      "right cluster, even near the lower-right cluster.\n",
      "\n",
      "Gaussian Mixtures \n",
      "\n",
      "| \n",
      "\n",
      "271\n",
      "\n",
      "\fBayes’ theorem (Equation 9-2) tells us how to update the probability distribution over\n",
      "the latent variables after we observe some data X. It computes the posterior distribu‐\n",
      "tion p(z|X), which is the conditional probability of z given X.\n",
      "\n",
      "Equation 9-2. Bayes’ theorem\n",
      "\n",
      "p z X = Posterior =\n",
      "\n",
      "Likelihood×Prior\n",
      "Evidence\n",
      "\n",
      "=\n",
      "\n",
      "p X z p z\n",
      "p X\n",
      "\n",
      "Unfortunately, in a Gaussian mixture model (and many other problems), the denomi‐\n",
      "nator  p(x)  is  intractable,  as  it  requires  integrating  over  all  the  possible  values  of  z\n",
      "(Equation 9-3). This means considering all possible combinations of cluster parame‐\n",
      "ters and cluster assignments.\n",
      "\n",
      "Equation 9-3. The evidence p(X) is often intractable\n",
      "p X = ∫ p X z p z dz\n",
      "\n",
      "This  is  one  of  the  central  problems  in  Bayesian  statistics,  and  there  are  several\n",
      "approaches to solving it. One of them is variational inference, which picks a family of\n",
      "distributions q(z; λ) with its own variational parameters λ (lambda), then it optimizes\n",
      "these parameters to make q(z) a good approximation of p(z|X). This is achieved by\n",
      "finding the value of λ that minimizes the KL divergence from q(z) to p(z|X), noted\n",
      "DKL(q‖p). The KL divergence equation is shown in (see Equation 9-4), and it can be\n",
      "rewritten  as  the  log  of  the  evidence  (log  p(X))  minus  the  evidence  lower  bound\n",
      "(ELBO). Since the log of the evidence does not depend on q, it is a constant term, so\n",
      "minimizing the KL divergence just requires maximizing the ELBO.\n",
      "\n",
      "Equation 9-4. KL divergence from q(z) to p(z|X)\n",
      "\n",
      "DKL q ∥ p = q log\n",
      "\n",
      "q z\n",
      "p z X\n",
      "= q log q z − log p z X\n",
      "p z, X\n",
      "p X\n",
      "\n",
      "= q log q z − log\n",
      "\n",
      "= q log q z − log p z, X + log p X\n",
      "= q log q z − q log p z, X + q log p X\n",
      "= q log p X − q log p z, X − q log q z\n",
      "= log p X − ELBO\n",
      "\n",
      "where ELBO = q log p z, X − q log q z\n",
      "\n",
      "272 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      "\fIn practice, there are different techniques to maximize the ELBO. In mean field varia‐\n",
      "tional inference, it is necessary to pick the family of distributions q(z; λ) and the prior\n",
      "p(z) very carefully to ensure that the equation for the ELBO simplifies to a form that\n",
      "can  actually  be  computed.  Unfortunately,  there  is  no  general  way  to  do  this,  it\n",
      "depends on the task and requires some mathematical skills. For example, the distribu‐\n",
      "tions  and  lower  bound  equations  used  in  Scikit-Learn’s  BayesianGaussianMixture\n",
      "class are presented in the documentation. From these equations it is possible to derive\n",
      "update equations for the cluster parameters and assignment variables: these are then\n",
      "used very much like in the Expectation-Maximization algorithm. In fact, the compu‐\n",
      "tational  complexity  of  the  BayesianGaussianMixture  class  is  similar  to  that  of  the\n",
      "GaussianMixture  class  (but  generally  significantly  slower).  A  simpler  approach  to\n",
      "maximizing the ELBO is called black box stochastic variational inference (BBSVI): at\n",
      "each iteration, a few samples are drawn from q and they are used to estimate the gra‐\n",
      "dients of the ELBO with regards to the variational parameters λ, which are then used\n",
      "in a gradient ascent step. This approach makes it possible to use Bayesian inference\n",
      "with any kind of model (provided it is differentiable), even deep neural networks: this\n",
      "is called Bayesian deep learning.\n",
      "\n",
      "If  you  want  to  dive  deeper  into  Bayesian  statistics,  check  out  the\n",
      "Bayesian Data Analysis book by Andrew Gelman, John Carlin, Hal\n",
      "Stern, David Dunson, Aki Vehtari, and Donald Rubin.\n",
      "\n",
      "Gaussian mixture models work great on clusters with ellipsoidal shapes, but if you try\n",
      "to fit a dataset with different shapes, you may have bad surprises. For example, let’s\n",
      "see what happens if we use a Bayesian Gaussian mixture model to cluster the moons\n",
      "dataset (see Figure 9-24):\n",
      "\n",
      "Figure 9-24. moons_vs_bgm_diagram\n",
      "\n",
      "Oops, the algorithm desperately searched for ellipsoids, so it found 8 different clus‐\n",
      "ters instead of 2. The density estimation is not too bad, so this model could perhaps\n",
      "be used for anomaly detection, but it failed to identify the two moons. Let’s now look\n",
      "at a few clustering algorithms capable of dealing with arbitrarily shaped clusters.\n",
      "\n",
      "Gaussian Mixtures \n",
      "\n",
      "| \n",
      "\n",
      "273\n",
      "\n",
      "\fOther Anomaly Detection and Novelty Detection Algorithms\n",
      "Scikit-Learn  also  implements  a  few  algorithms  dedicated  to  anomaly  detection  or\n",
      "novelty detection:\n",
      "\n",
      "• Fast-MCD (minimum covariance determinant), implemented by the EllipticEn\n",
      "velope  class:  this  algorithm  is  useful  for  outlier  detection,  in  particular  to\n",
      "cleanup  a  dataset.  It  assumes  that  the  normal  instances  (inliers)  are  generated\n",
      "from a single Gaussian distribution (not a mixture), but it also assumes that the\n",
      "dataset is contaminated with outliers that were not generated from this Gaussian\n",
      "distribution. When it estimates the parameters of the Gaussian distribution (i.e.,\n",
      "the  shape  of  the  elliptic  envelope  around  the  inliers),  it  is  careful  to  ignore  the\n",
      "instances that are most likely outliers. This gives a better estimation of the elliptic\n",
      "envelope, and thus makes it better at identifying the outliers.\n",
      "\n",
      "• Isolation  forest:  this  is  an  efficient  algorithm  for  outlier  detection,  especially  in\n",
      "high-dimensional datasets. The algorithm builds a Random Forest in which each\n",
      "Decision Tree is grown randomly: at each node, it picks a feature randomly, then\n",
      "it picks a random threshold value (between the min and max value) to split the\n",
      "dataset in two. The dataset gradually gets chopped into pieces this way, until all\n",
      "instances  end  up  isolated  from  the  other  instances.  An  anomaly  is  usually  far\n",
      "from other instances, so on average (across all the Decision Trees) it tends to get\n",
      "isolated in less steps than normal instances.\n",
      "\n",
      "• Local  outlier  factor  (LOF):  this  algorithm  is  also  good  for  outlier  detection.  It\n",
      "compares the density of instances around a given instance to the density around\n",
      "its neighbors. An anomaly is often more isolated than its k nearest neighbors.\n",
      "\n",
      "• One-class SVM: this algorithm is better suited for novelty detection. Recall that a\n",
      "kernelized  SVM  classifier  separates  two  classes  by  first  (implicitly)  mapping  all\n",
      "the instances to a high-dimensional space, then separating the two classes using a\n",
      "linear  SVM  classifier  within  this  high-dimensional  space  (see  Chapter  5).  Since\n",
      "we just have one class of instances, the one-class SVM algorithm instead tries to\n",
      "separate the instances in high-dimensional space from the origin. In the original\n",
      "space,  this  will  correspond  to  finding  a  small  region  that  encompasses  all  the\n",
      "instances.  If  a  new  instance  does  not  fall  within  this  region,  it  is  an  anomaly.\n",
      "There are a few hyperparameters to tweak: the usual ones for a kernelized SVM,\n",
      "plus  a  margin  hyperparameter  that  corresponds  to  the  probability  of  a  new\n",
      "instance being mistakenly considered as novel, when it is in fact normal. It works\n",
      "great,  especially  with  high-dimensional  datasets,  but  just  like  all  SVMs,  it  does\n",
      "not scale to large datasets.\n",
      "\n",
      "274 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 9: Unsupervised Learning Techniques\n",
      "\n",
      "\fPART II\n",
      "Neural Networks and Deep Learning\n",
      "\n",
      "\f\fCHAPTER 10\n",
      "Introduction to Artificial Neural Networks\n",
      "with Keras\n",
      "\n",
      "With  Early  Release  ebooks,  you  get  books  in  their  earliest  form—\n",
      "the author’s raw and unedited content as he or she writes—so you\n",
      "can  take  advantage  of  these  technologies  long  before  the  official\n",
      "release of these titles. The following will be Chapter 10 in the final\n",
      "release of the book.\n",
      "\n",
      "Birds  inspired  us  to  fly,  burdock  plants  inspired  velcro,  and  countless  more  inven‐\n",
      "tions were inspired by nature. It seems only logical, then, to look at the brain’s archi‐\n",
      "tecture  for  inspiration  on  how  to  build  an  intelligent  machine.  This  is  the  key  idea\n",
      "that  sparked  artificial  neural  networks  (ANNs).  However,  although  planes  were\n",
      "inspired by birds, they don’t have to flap their wings. Similarly, ANNs have gradually\n",
      "become  quite  different  from  their  biological  cousins.  Some  researchers  even  argue\n",
      "that  we  should  drop  the  biological  analogy  altogether  (e.g.,  by  saying  “units”  rather\n",
      "than “neurons”), lest we restrict our creativity to biologically plausible systems.1\n",
      "\n",
      "ANNs are at the very core of Deep Learning. They are versatile, powerful, and scala‐\n",
      "ble,  making  them  ideal  to  tackle  large  and  highly  complex  Machine  Learning  tasks,\n",
      "such as classifying billions of images (e.g., Google Images), powering speech recogni‐\n",
      "tion services (e.g., Apple’s Siri), recommending the best videos to watch to hundreds\n",
      "of millions of users every day (e.g., YouTube), or learning to beat the world champion\n",
      "at  the  game  of  Go  by  playing  millions  of  games  against  itself  (DeepMind’s  Alpha‐\n",
      "Zero).\n",
      "\n",
      "1 You can get the best of both worlds by being open to biological inspirations without being afraid to create\n",
      "\n",
      "biologically unrealistic models, as long as they work well.\n",
      "\n",
      "277\n",
      "\n",
      "\fIn the first part of this chapter, we will introduce artificial neural networks, starting\n",
      "with a quick tour of the very first ANN architectures, leading up to Multi-Layer Per‐\n",
      "ceptrons (MLPs) which are heavily used today (other architectures will be explored in\n",
      "the next chapters). In the second part, we will look at how to implement neural net‐\n",
      "works using the popular Keras API. This is a beautifully designed and simple high-\n",
      "level API for building, training, evaluating and running neural networks. But don’t be\n",
      "fooled  by  its  simplicity:  it  is  expressive  and  flexible  enough  to  let  you  build  a  wide\n",
      "variety of neural network architectures. In fact, it will probably be sufficient for most\n",
      "of  your  use  cases.  Moreover,  should  you  ever  need  extra  flexibility,  you  can  always\n",
      "write  custom  Keras  components  using  its  lower-level  API,  as  we  will  see  in  Chap‐\n",
      "ter 12.\n",
      "\n",
      "But first, let’s go back in time to see how artificial neural networks came to be!\n",
      "\n",
      "From Biological to Artificial Neurons\n",
      "Surprisingly,  ANNs  have  been  around  for  quite  a  while:  they  were  first  introduced\n",
      "back  in  1943  by  the  neurophysiologist  Warren  McCulloch  and  the  mathematician\n",
      "Walter  Pitts.  In  their  landmark  paper,2  “A  Logical  Calculus  of  Ideas  Immanent  in\n",
      "Nervous Activity,” McCulloch and Pitts presented a simplified computational model\n",
      "of how biological neurons might work together in animal brains to perform complex\n",
      "computations  using  propositional  logic.  This  was  the  first  artificial  neural  network\n",
      "architecture. Since then many other architectures have been invented, as we will see.\n",
      "\n",
      "The  early  successes  of  ANNs  until  the  1960s  led  to  the  widespread  belief  that  we\n",
      "would soon be conversing with truly intelligent machines. When it became clear that\n",
      "this promise would go unfulfilled (at least for quite a while), funding flew elsewhere\n",
      "and ANNs entered a long winter. In the early 1980s there was a revival of interest in \n",
      "connectionism (the study of neural networks), as new architectures were invented and\n",
      "better training techniques were developed. But progress was slow, and by the 1990s\n",
      "other powerful Machine Learning techniques were invented, such as Support Vector\n",
      "Machines (see Chapter 5). These techniques seemed to offer better results and stron‐\n",
      "ger theoretical foundations than ANNs, so once again the study of neural networks\n",
      "entered a long winter.\n",
      "\n",
      "Finally, we are now witnessing yet another wave of interest in ANNs. Will this wave\n",
      "die out like the previous ones did? Well, there are a few good reasons to believe that\n",
      "this wave is different and that it will have a much more profound impact on our lives:\n",
      "\n",
      "2 “A Logical Calculus of Ideas Immanent in Nervous Activity,” W. McCulloch and W. Pitts (1943).\n",
      "\n",
      "278 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
      "\n",
      "\f• There  is  now  a  huge  quantity  of  data  available  to  train  neural  networks,  and\n",
      "ANNs  frequently  outperform  other  ML  techniques  on  very  large  and  complex\n",
      "problems.\n",
      "\n",
      "• The tremendous increase in computing power since the 1990s now makes it pos‐\n",
      "sible  to  train  large  neural  networks  in  a  reasonable  amount  of  time.  This  is  in\n",
      "part due to Moore’s Law, but also thanks to the gaming industry, which has pro‐\n",
      "duced powerful GPU cards by the millions.\n",
      "\n",
      "• The training algorithms have been improved. To be fair they are only slightly dif‐\n",
      "ferent from the ones used in the 1990s, but these relatively small tweaks have a\n",
      "huge positive impact.\n",
      "\n",
      "• Some theoretical limitations of ANNs have turned out to be benign in practice.\n",
      "For example, many people thought that ANN training algorithms were doomed\n",
      "because they were likely to get stuck in local optima, but it turns out that this is\n",
      "rather rare in practice (or when it is the case, they are usually fairly close to the\n",
      "global optimum).\n",
      "\n",
      "• ANNs seem to have entered a virtuous circle of funding and progress. Amazing\n",
      "products  based  on  ANNs  regularly  make  the  headline  news,  which  pulls  more\n",
      "and more attention and funding toward them, resulting in more and more pro‐\n",
      "gress, and even more amazing products.\n",
      "\n",
      "Biological Neurons\n",
      "Before we discuss artificial neurons, let’s take a quick look at a biological neuron (rep‐\n",
      "resented in Figure 10-1). It is an unusual-looking cell mostly found in animal cerebral\n",
      "cortexes (e.g., your brain), composed of a cell body containing the nucleus and most\n",
      "of  the  cell’s  complex  components,  and  many  branching  extensions  called  dendrites,\n",
      "plus  one  very  long  extension  called  the  axon.  The  axon’s  length  may  be  just  a  few\n",
      "times longer than the cell body, or up to tens of thousands of times longer. Near its\n",
      "extremity the axon splits off into many branches called telodendria, and at the tip of\n",
      "these  branches  are  minuscule  structures  called  synaptic  terminals  (or  simply  synap‐\n",
      "ses), which are connected to the dendrites (or directly to the cell body) of other neu‐\n",
      "rons.  Biological  neurons  receive  short  electrical  impulses  called  signals  from  other\n",
      "neurons  via  these  synapses.  When  a  neuron  receives  a  sufficient  number  of  signals\n",
      "from other neurons within a few milliseconds, it fires its own signals.\n",
      "\n",
      "From Biological to Artificial Neurons \n",
      "\n",
      "| \n",
      "\n",
      "279\n",
      "\n",
      "\fFigure 10-1. Biological neuron3\n",
      "\n",
      "Thus, individual biological neurons seem to behave in a rather simple way, but they\n",
      "are organized in a vast network of billions of neurons, each neuron typically connec‐\n",
      "ted to thousands of other neurons. Highly complex computations can be performed\n",
      "by a vast network of fairly simple neurons, much like a complex anthill can emerge\n",
      "from the combined efforts of simple ants. The architecture of biological neural net‐\n",
      "works (BNN)4 is still the subject of active research, but some parts of the brain have\n",
      "been mapped, and it seems that neurons are often organized in consecutive layers, as \n",
      "shown in Figure 10-2.\n",
      "\n",
      "Figure 10-2. Multiple layers in a biological neural network (human cortex)5\n",
      "\n",
      "3 Image by Bruce Blaus (Creative Commons 3.0). Reproduced from https://en.wikipedia.org/wiki/Neuron.\n",
      "\n",
      "4 In the context of Machine Learning, the phrase “neural networks” generally refers to ANNs, not BNNs.\n",
      "\n",
      "5 Drawing of a cortical lamination by S. Ramon y Cajal (public domain). Reproduced from https://en.wikipe\n",
      "\n",
      "dia.org/wiki/Cerebral_cortex.\n",
      "\n",
      "280 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
      "\n",
      "\fLogical Computations with Neurons\n",
      "Warren McCulloch and Walter Pitts proposed a very simple model of the biological\n",
      "neuron, which later became known as an artificial neuron: it has one or more binary\n",
      "(on/off) inputs and one binary output. The artificial neuron simply activates its out‐\n",
      "put when more than a certain number of its inputs are active. McCulloch and Pitts\n",
      "showed  that  even  with  such  a  simplified  model  it  is  possible  to  build  a  network  of\n",
      "artificial neurons that computes any logical proposition you want. For example, let’s\n",
      "build  a  few  ANNs  that  perform  various  logical  computations  (see  Figure  10-3),\n",
      "assuming that a neuron is activated when at least two of its inputs are active.\n",
      "\n",
      "Figure 10-3. ANNs performing simple logical computations\n",
      "\n",
      "• The first network on the left is simply the identity function: if neuron A is activa‐\n",
      "ted, then neuron C gets activated as well (since it receives two input signals from\n",
      "neuron A), but if neuron A is off, then neuron C is off as well.\n",
      "\n",
      "• The  second  network  performs  a  logical  AND:  neuron  C  is  activated  only  when\n",
      "both neurons A and B are activated (a single input signal is not enough to acti‐\n",
      "vate neuron C).\n",
      "\n",
      "• The third network performs a logical OR: neuron C gets activated if either neu‐\n",
      "\n",
      "ron A or neuron B is activated (or both).\n",
      "\n",
      "• Finally, if we suppose that an input connection can inhibit the neuron’s activity\n",
      "(which is the case with biological neurons), then the fourth network computes a\n",
      "slightly more complex logical proposition: neuron C is activated only if neuron A\n",
      "is active and if neuron B is off. If neuron A is active all the time, then you get a\n",
      "logical NOT: neuron C is active when neuron B is off, and vice versa.\n",
      "\n",
      "You  can  easily  imagine  how  these  networks  can  be  combined  to  compute  complex\n",
      "logical expressions (see the exercises at the end of the chapter).\n",
      "\n",
      "The Perceptron\n",
      "The Perceptron is one of the simplest ANN architectures, invented in 1957 by Frank\n",
      "Rosenblatt. It is based on a slightly different artificial neuron (see Figure 10-4) called \n",
      "\n",
      "From Biological to Artificial Neurons \n",
      "\n",
      "| \n",
      "\n",
      "281\n",
      "\n",
      "\fa  threshold  logic  unit  (TLU),  or  sometimes  a  linear  threshold  unit  (LTU):  the  inputs\n",
      "and output are now numbers (instead of binary on/off values) and each input con‐\n",
      "nection is associated with a weight. The TLU computes a weighted sum of its inputs\n",
      "(z = w1 x1 + w2 x2 + ⋯ + wn xn = xT w), then applies a step function to that sum and\n",
      "outputs the result: hw(x) = step(z), where z = xT w.\n",
      "\n",
      "Figure 10-4. Threshold logic unit\n",
      "\n",
      "The  most  common  step  function  used  in  Perceptrons  is  the  Heaviside  step  function\n",
      "(see Equation 10-1). Sometimes the sign function is used instead.\n",
      "\n",
      "Equation 10-1. Common step functions used in Perceptrons\n",
      "\n",
      "heaviside z =\n",
      "\n",
      "0 if z < 0\n",
      "1 if z ≥ 0\n",
      "\n",
      "sgn z =\n",
      "\n",
      "−1 if z < 0\n",
      "if z = 0\n",
      "0\n",
      "+1 if z > 0\n",
      "\n",
      "A single TLU can be used for simple linear binary classification. It computes a linear\n",
      "combination of the inputs and if the result exceeds a threshold, it outputs the positive\n",
      "class  or  else  outputs  the  negative  class  (just  like  a  Logistic  Regression  classifier  or  a\n",
      "linear SVM). For example, you could use a single TLU to classify iris flowers based on\n",
      "the petal length and width (also adding an extra bias feature x0 = 1, just like we did in\n",
      "previous chapters). Training a TLU in this case means finding the right values for w0,\n",
      "w1, and w2 (the training algorithm is discussed shortly).\n",
      "\n",
      "A Perceptron is simply composed of a single layer of TLUs,6 with each TLU connected\n",
      "to all the inputs. When all the neurons in a layer are connected to every neuron in the\n",
      "previous  layer  (i.e.,  its  input  neurons),  it  is  called  a  fully  connected  layer  or  a  dense\n",
      "layer. To represent the fact that each input is sent to every TLU, it is common to draw\n",
      "special  passthrough  neurons  called  input  neurons:  they  just  output  whatever  input\n",
      "they are fed. All the input neurons form the input layer. Moreover, an extra bias fea‐\n",
      "\n",
      "6 The name Perceptron is sometimes used to mean a tiny network with a single TLU.\n",
      "\n",
      "282 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
      "\n",
      "\fture is generally added (x0 = 1): it is typically represented using a special type of neu‐\n",
      "ron  called  a  bias  neuron,  which  just  outputs  1  all  the  time.  A  Perceptron  with  two\n",
      "inputs and three outputs is represented in Figure 10-5. This Perceptron can classify\n",
      "instances simultaneously into three different binary classes, which makes it a multi‐\n",
      "output classifier.\n",
      "\n",
      "Figure 10-5. Perceptron diagram\n",
      "\n",
      "Thanks to the magic of linear algebra, it is possible to efficiently compute the outputs\n",
      "of a layer of artificial neurons for several instances at once, by using Equation 10-2:\n",
      "\n",
      "Equation 10-2. Computing the outputs of a fully connected layer\n",
      "\n",
      "hW, b X = ϕ XW + b\n",
      "\n",
      "• As always, X represents the matrix of input features. It has one row per instance,\n",
      "\n",
      "one column per feature.\n",
      "\n",
      "• The  weight  matrix  W  contains  all  the  connection  weights  except  for  the  ones\n",
      "from the bias neuron. It has one row per input neuron and one column per artifi‐\n",
      "cial neuron in the layer.\n",
      "\n",
      "• The  bias  vector  b  contains  all  the  connection  weights  between  the  bias  neuron\n",
      "\n",
      "and the artificial neurons. It has one bias term per artificial neuron.\n",
      "\n",
      "• The  function  ϕ  is  called  the  activation  function:  when  the  artificial  neurons  are\n",
      "TLUs, it is a step function (but we will discuss other activation functions shortly).\n",
      "\n",
      "So  how  is  a  Perceptron  trained?  The  Perceptron  training  algorithm  proposed  by\n",
      "Frank Rosenblatt was largely inspired by Hebb’s rule. In his book The Organization of\n",
      "Behavior, published in 1949, Donald Hebb suggested that when a biological neuron\n",
      "often  triggers  another  neuron,  the  connection  between  these  two  neurons  grows\n",
      "stronger.  This  idea  was  later  summarized  by  Siegrid  Löwel  in  this  catchy  phrase:\n",
      "“Cells that fire together, wire together.” This rule later became known as Hebb’s rule \n",
      "\n",
      "From Biological to Artificial Neurons \n",
      "\n",
      "| \n",
      "\n",
      "283\n",
      "\n",
      "\f(or  Hebbian  learning);  that  is,  the  connection  weight  between  two  neurons  is\n",
      "increased whenever they have the same output. Perceptrons are trained using a var‐\n",
      "iant of this rule that takes into account the error made by the network; it reinforces\n",
      "connections  that  help  reduce  the  error.  More  specifically,  the  Perceptron  is  fed  one\n",
      "training instance at a time, and for each instance it makes its predictions. For every\n",
      "output  neuron  that  produced  a  wrong  prediction,  it  reinforces  the  connection\n",
      "weights  from  the  inputs  that  would  have  contributed  to  the  correct  prediction.  The\n",
      "rule is shown in Equation 10-3.\n",
      "\n",
      "Equation 10-3. Perceptron learning rule (weight update)\n",
      "\n",
      "wi, j\n",
      "\n",
      "next step = wi, j + η y j − y j xi\n",
      "\n",
      "• wi, j is the connection weight between the ith input neuron and the jth output neu‐\n",
      "\n",
      "ron.\n",
      "\n",
      "• xi is the ith input value of the current training instance.\n",
      "• y j is the output of the jth output neuron for the current training instance.\n",
      "• yj is the target output of the jth output neuron for the current training instance.\n",
      "• η is the learning rate.\n",
      "\n",
      "The decision boundary of each output neuron is linear, so Perceptrons are incapable\n",
      "of learning complex patterns (just like Logistic Regression classifiers). However, if the\n",
      "training instances are linearly separable, Rosenblatt demonstrated that this algorithm\n",
      "would converge to a solution.7 This is called the Perceptron convergence theorem.\n",
      "\n",
      "Scikit-Learn  provides  a  Perceptron  class  that  implements  a  single  TLU  network.  It\n",
      "can be used pretty much as you would expect—for example, on the iris dataset (intro‐\n",
      "duced in Chapter 4):\n",
      "\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_iris\n",
      "from sklearn.linear_model import Perceptron\n",
      "\n",
      "iris = load_iris()\n",
      "X = iris.data[:, (2, 3)]  # petal length, petal width\n",
      "y = (iris.target == 0).astype(np.int)  # Iris Setosa?\n",
      "\n",
      "per_clf = Perceptron()\n",
      "per_clf.fit(X, y)\n",
      "\n",
      "7 Note that this solution is generally not unique: in general when the data are linearly separable, there is an\n",
      "\n",
      "infinity of hyperplanes that can separate them.\n",
      "\n",
      "284 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
      "\n",
      "\fy_pred = per_clf.predict([[2, 0.5]])\n",
      "\n",
      "You may have noticed the fact that the Perceptron learning algorithm strongly resem‐\n",
      "bles Stochastic Gradient Descent. In fact, Scikit-Learn’s Perceptron class is equivalent\n",
      "to using an SGDClassifier with the following hyperparameters: loss=\"perceptron\",\n",
      "learning_rate=\"constant\", eta0=1 (the learning rate), and penalty=None (no regu‐\n",
      "larization).\n",
      "\n",
      "Note that contrary to Logistic Regression classifiers, Perceptrons do not output a class\n",
      "probability; rather, they just make predictions based on a hard threshold. This is one\n",
      "of the good reasons to prefer Logistic Regression over Perceptrons.\n",
      "\n",
      "In  their  1969  monograph  titled  Perceptrons,  Marvin  Minsky  and  Seymour  Papert\n",
      "highlighted a number of serious weaknesses of Perceptrons, in particular the fact that\n",
      "they  are  incapable  of  solving  some  trivial  problems  (e.g.,  the  Exclusive  OR  (XOR)\n",
      "classification problem; see the left side of Figure 10-6). Of course this is true of any\n",
      "other linear classification model as well (such as Logistic Regression classifiers), but\n",
      "researchers  had  expected  much  more  from  Perceptrons,  and  their  disappointment\n",
      "was  great,  and  many  researchers  dropped  neural  networks  altogether  in  favor  of\n",
      "higher-level problems such as logic, problem solving, and search.\n",
      "\n",
      "However, it turns out that some of the limitations of Perceptrons can be eliminated by\n",
      "stacking multiple Perceptrons. The resulting ANN is called a Multi-Layer Perceptron\n",
      "(MLP). In particular, an MLP can solve the XOR problem, as you can verify by com‐\n",
      "puting the output of the MLP represented on the right of Figure 10-6: with inputs (0,\n",
      "0)  or  (1,  1)  the  network  outputs  0,  and  with  inputs  (0,  1)  or  (1,  0)  it  outputs  1.  All\n",
      "connections have a weight equal to 1, except the four connections where the weight is\n",
      "shown. Try verifying that this network indeed solves the XOR problem!\n",
      "\n",
      "Figure 10-6. XOR classification problem and an MLP that solves it\n",
      "\n",
      "From Biological to Artificial Neurons \n",
      "\n",
      "| \n",
      "\n",
      "285\n",
      "\n",
      "\fMulti-Layer Perceptron and Backpropagation\n",
      "An MLP is composed of one (passthrough) input layer, one or more layers of TLUs,\n",
      "called  hidden  layers,  and  one  final  layer  of  TLUs  called  the  output  layer  (see\n",
      "Figure  10-7).  The  layers  close  to  the  input  layer  are  usually  called  the  lower  layers,\n",
      "and  the  ones  close  to  the  outputs  are  usually  called  the  upper  layers.  Every  layer\n",
      "except the output layer includes a bias neuron and is fully connected to the next layer.\n",
      "\n",
      "Figure 10-7. Multi-Layer Perceptron\n",
      "\n",
      "The signal flows only in one direction (from the inputs to the out‐\n",
      "puts), so this architecture is an example of a feedforward neural net‐\n",
      "work (FNN).\n",
      "\n",
      "When an ANN contains a deep stack of hidden layers8, it is called a deep neural net‐\n",
      "work (DNN). The field of Deep Learning studies DNNs, and more generally models\n",
      "containing  deep  stacks  of  computations.  However,  many  people  talk  about  Deep\n",
      "Learning whenever neural networks are involved (even shallow ones).\n",
      "\n",
      "For  many  years  researchers  struggled  to  find  a  way  to  train  MLPs,  without  success.\n",
      "But  in  1986,  David  Rumelhart,  Geoffrey  Hinton  and  Ronald  Williams  published  a\n",
      "groundbreaking paper9 introducing the backpropagation training algorithm, which is\n",
      "still  used  today.  In  short,  it  is  simply  Gradient  Descent  (introduced  in  Chapter  4)\n",
      "\n",
      "8 In the 1990s, an ANN with more than two hidden layers was considered deep. Nowadays, it is common to see\n",
      "\n",
      "ANNs with dozens of layers, or even hundreds, so the definition of “deep” is quite fuzzy.\n",
      "\n",
      "9 “Learning Internal Representations by Error Propagation,” D. Rumelhart, G. Hinton, R. Williams (1986).\n",
      "\n",
      "286 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
      "\n",
      "\fusing an efficient technique for computing the gradients automatically10: in just two\n",
      "passes through the network (one forward, one backward), the backpropagation algo‐\n",
      "rithm is able to compute the gradient of the network’s error with regards to every sin‐\n",
      "gle model parameter. In other words, it can find out how each connection weight and\n",
      "each bias term should be tweaked in order to reduce the error. Once it has these gra‐\n",
      "dients,  it  just  performs  a  regular  Gradient  Descent  step,  and  the  whole  process  is\n",
      "repeated until the network converges to the solution.\n",
      "\n",
      "Automatically  computing  gradients  is  called  automatic  differentia‐\n",
      "tion, or autodiff. There are various autodiff techniques, with differ‐\n",
      "ent  pros  and  cons.  The  one  used  by  backpropagation  is  called\n",
      "reverse-mode autodiff. It is fast and precise, and is well suited when\n",
      "the  function  to  differentiate  has  many  variables  (e.g.,  connection\n",
      "weights) and few outputs (e.g., one loss). If you want to learn more\n",
      "about autodiff, check out ???.\n",
      "\n",
      "Let’s run through this algorithm in a bit more detail:\n",
      "\n",
      "• It handles one mini-batch at a time (for example containing 32 instances each),\n",
      "and  it  goes  through  the  full  training  set  multiple  times.  Each  pass  is  called  an\n",
      "epoch, as we saw in Chapter 4.\n",
      "\n",
      "• Each mini-batch is passed to the network’s input layer, which just sends it to the\n",
      "first hidden layer. The algorithm then computes the output of all the neurons in\n",
      "this  layer  (for  every  instance  in  the  mini-batch).  The  result  is  passed  on  to  the\n",
      "next layer, its output is computed and passed to the next layer, and so on until we\n",
      "get  the  output  of  the  last  layer,  the  output  layer.  This  is  the  forward  pass:  it  is\n",
      "exactly  like  making  predictions,  except  all  intermediate  results  are  preserved\n",
      "since they are needed for the backward pass.\n",
      "\n",
      "• Next, the algorithm measures the network’s output error (i.e., it uses a loss func‐\n",
      "tion that compares the desired output and the actual output of the network, and\n",
      "returns some measure of the error).\n",
      "\n",
      "• Then  it  computes  how  much  each  output  connection  contributed  to  the  error.\n",
      "This is done analytically by simply applying the chain rule (perhaps the most fun‐\n",
      "damental rule in calculus), which makes this step fast and precise.\n",
      "\n",
      "• The algorithm then measures how much of these error contributions came from\n",
      "each connection in the layer below, again using the chain rule—and so on until\n",
      "the  algorithm  reaches  the  input  layer.  As  we  explained  earlier,  this  reverse  pass\n",
      "efficiently  measures  the  error  gradient  across  all  the  connection  weights  in  the\n",
      "\n",
      "10 This technique was actually independently invented several times by various researchers in different fields,\n",
      "\n",
      "starting with P. Werbos in 1974.\n",
      "\n",
      "From Biological to Artificial Neurons \n",
      "\n",
      "| \n",
      "\n",
      "287\n",
      "\n",
      "\fnetwork by propagating the error gradient backward through the network (hence\n",
      "the name of the algorithm).\n",
      "\n",
      "• Finally, the algorithm performs a Gradient Descent step to tweak all the connec‐\n",
      "\n",
      "tion weights in the network, using the error gradients it just computed.\n",
      "\n",
      "This  algorithm  is  so  important,  it’s  worth  summarizing  it  again:  for  each  training\n",
      "instance  the  backpropagation  algorithm  first  makes  a  prediction  (forward  pass),\n",
      "measures the error, then goes through each layer in reverse to measure the error con‐\n",
      "tribution from each connection (reverse pass), and finally slightly tweaks the connec‐\n",
      "tion weights to reduce the error (Gradient Descent step).\n",
      "\n",
      "It is important to initialize all the hidden layers’ connection weights\n",
      "randomly, or else training will fail. For example, if you initialize all\n",
      "weights and biases to zero, then all neurons in a given layer will be\n",
      "perfectly  identical,  and  thus  backpropagation  will  affect  them  in\n",
      "exactly the same way, so they will remain identical. In other words,\n",
      "despite having hundreds of neurons per layer, your model will act\n",
      "as  if  it  had  only  one  neuron  per  layer:  it  won’t  be  too  smart.  If\n",
      "instead you randomly initialize the weights, you break the symme‐\n",
      "try and allow backpropagation to train a diverse team of neurons.\n",
      "\n",
      "In order for this algorithm to work properly, the authors made a key change to the\n",
      "MLP’s architecture: they replaced the step function with the logistic function, σ(z) =\n",
      "1 / (1 + exp(–z)). This was essential because the step function contains only flat seg‐\n",
      "ments, so there is no gradient to work with (Gradient Descent cannot move on a flat\n",
      "surface),  while  the  logistic  function  has  a  well-defined  nonzero  derivative  every‐\n",
      "where,  allowing  Gradient  Descent  to  make  some  progress  at  every  step.  In  fact,  the\n",
      "backpropagation algorithm works well with many other activation functions, not just\n",
      "the logistic function. Two other popular activation functions are:\n",
      "\n",
      "The hyperbolic tangent function tanh(z) = 2σ(2z) – 1\n",
      "\n",
      "Just like the logistic function it is S-shaped, continuous, and differentiable, but its\n",
      "output value ranges from –1 to 1 (instead of 0 to 1 in the case of the logistic func‐\n",
      "tion), which tends to make each layer’s output more or less centered around 0 at\n",
      "the beginning of training. This often helps speed up convergence.\n",
      "\n",
      "The Rectified Linear Unit function: ReLU(z) = max(0, z)\n",
      "\n",
      "It is continuous but unfortunately not differentiable at z = 0 (the slope changes\n",
      "abruptly, which can make Gradient Descent bounce around), and its derivative is\n",
      "0 for z < 0. However, in practice it works very well and has the advantage of being\n",
      "\n",
      "288 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
      "\n",
      "\ffast  to  compute11.  Most  importantly,  the  fact  that  it  does  not  have  a  maximum\n",
      "output  value  also  helps  reduce  some  issues  during  Gradient  Descent  (we  will\n",
      "come back to this in Chapter 11).\n",
      "\n",
      "These  popular  activation  functions  and  their  derivatives  are  represented  in\n",
      "Figure 10-8. But wait! Why do we need activation functions in the first place? Well, if\n",
      "you  chain  several  linear  transformations,  all  you  get  is  a  linear  transformation.  For\n",
      "example, say f(x) = 2 x + 3 and g(x) = 5 x - 1, then chaining these two linear functions\n",
      "gives you another linear function: f(g(x)) = 2(5 x - 1) + 3 = 10 x + 1. So if you don’t\n",
      "have some non-linearity between layers, then even a deep stack of layers is equivalent\n",
      "to a single layer: you cannot solve very complex problems with that.\n",
      "\n",
      "Figure 10-8. Activation functions and their derivatives\n",
      "\n",
      "Okay! So now you know where neural nets came from, what their architecture is and\n",
      "how to compute their outputs, and you also learned about the backpropagation algo‐\n",
      "rithm. But what exactly can you do with them?\n",
      "\n",
      "Regression MLPs\n",
      "First, MLPs can be used for regression tasks. If you want to predict a single value (e.g.,\n",
      "the  price  of  a  house  given  many  of  its  features),  then  you  just  need  a  single  output\n",
      "neuron: its output is the predicted value. For multivariate regression (i.e., to predict\n",
      "multiple  values  at  once),  you  need  one  output  neuron  per  output  dimension.  For\n",
      "example, to locate the center of an object on an image, you need to predict 2D coordi‐\n",
      "nates,  so  you  need  two  output  neurons.  If  you  also  want  to  place  a  bounding  box\n",
      "around the object, then you need two more numbers: the width and the height of the\n",
      "object. So you end up with 4 output neurons.\n",
      "\n",
      "11 Biological neurons seem to implement a roughly sigmoid (S-shaped) activation function, so researchers stuck\n",
      "to sigmoid functions for a very long time. But it turns out that ReLU generally works better in ANNs. This is\n",
      "one of the cases where the biological analogy was misleading.\n",
      "\n",
      "From Biological to Artificial Neurons \n",
      "\n",
      "| \n",
      "\n",
      "289\n",
      "\n",
      "\fIn general, when building an MLP for regression, you do not want to use any activa‐\n",
      "tion function for the output neurons, so they are free to output any range of values.\n",
      "However, if you want to guarantee that the output will always be positive, then you\n",
      "can use the ReLU activation function, or the softplus activation function in the output\n",
      "layer.  Finally,  if  you  want  to  guarantee  that  the  predictions  will  fall  within  a  given\n",
      "range of values, then you can use the logistic function or the hyperbolic tangent, and\n",
      "scale the labels to the appropriate range: 0 to 1 for the logistic function, or –1 to 1 for\n",
      "the hyperbolic tangent.\n",
      "\n",
      "The loss function to use during training is typically the mean squared error, but if you\n",
      "have  a  lot  of  outliers  in  the  training  set,  you  may  prefer  to  use  the  mean  absolute\n",
      "error  instead.  Alternatively,  you  can  use  the  Huber  loss,  which  is  a  combination  of\n",
      "both.\n",
      "\n",
      "The Huber loss is quadratic when the error is smaller than a thres‐\n",
      "hold δ (typically 1), but linear when the error is larger than δ. This\n",
      "makes it less sensitive to outliers than the mean squared error, and\n",
      "it is often more precise and converges faster than the mean abso‐\n",
      "lute error.\n",
      "\n",
      "Table 10-1 summarizes the typical architecture of a regression MLP.\n",
      "\n",
      "Table 10-1. Typical Regression MLP Architecture\n",
      "\n",
      "Hyperparameter\n",
      "# input neurons\n",
      "\n",
      "# hidden layers\n",
      "\n",
      "Typical Value\n",
      "One per input feature (e.g., 28 x 28 = 784 for MNIST)\n",
      "\n",
      "Depends on the problem. Typically 1 to 5.\n",
      "\n",
      "# neurons per hidden layer Depends on the problem. Typically 10 to 100.\n",
      "\n",
      "# output neurons\n",
      "\n",
      "Hidden activation\n",
      "\n",
      "Output activation\n",
      "\n",
      "Loss function\n",
      "\n",
      "1 per prediction dimension\n",
      "\n",
      "ReLU (or SELU, see Chapter 11)\n",
      "\n",
      "None or ReLU/Softplus (if positive outputs) or Logistic/Tanh (if bounded outputs)\n",
      "\n",
      "MSE or MAE/Huber (if outliers)\n",
      "\n",
      "Classification MLPs\n",
      "MLPs  can  also  be  used  for  classification  tasks.  For  a  binary  classification  problem,\n",
      "you just need a single output neuron using the logistic activation function: the output\n",
      "will be a number between 0 and 1, which you can interpret as the estimated probabil‐\n",
      "ity  of  the  positive  class.  Obviously,  the  estimated  probability  of  the  negative  class  is\n",
      "equal to one minus that number.\n",
      "\n",
      "MLPs can also easily handle multilabel binary classification tasks (see Chapter 3). For\n",
      "example,  you  could  have  an  email  classification  system  that  predicts  whether  each\n",
      "incoming email is ham or spam, and simultaneously predicts whether it is an urgent\n",
      "\n",
      "290 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
      "\n",
      "\for  non-urgent  email.  In  this  case,  you  would  need  two  output  neurons,  both  using\n",
      "the logistic activation function: the first would output the probability that the email is\n",
      "spam and the second would output the probability that it is urgent. More generally,\n",
      "you would dedicate one output neuron for each positive class. Note that the output\n",
      "probabilities do not necessarily add up to one. This lets the model output any combi‐\n",
      "nation of labels: you can have non-urgent ham, urgent ham, non-urgent spam, and\n",
      "perhaps even urgent spam (although that would probably be an error).\n",
      "\n",
      "If  each  instance  can  belong  only  to  a  single  class,  out  of  3  or  more  possible  classes\n",
      "(e.g.,  classes  0  through  9  for  digit  image  classification),  then  you  need  to  have  one\n",
      "output neuron per class, and you should use the softmax activation function for the\n",
      "whole output layer (see Figure 10-9). The softmax function (introduced in Chapter 4)\n",
      "will ensure that all the estimated probabilities are between 0 and 1 and that they add\n",
      "up to one (which is required if the classes are exclusive). This is called multiclass clas‐\n",
      "sification.\n",
      "\n",
      "Figure 10-9. A modern MLP (including ReLU and softmax) for classification\n",
      "\n",
      "Regarding  the  loss  function,  since  we  are  predicting  probability  distributions,  the\n",
      "cross-entropy (also called the log loss, see Chapter 4) is generally a good choice.\n",
      "\n",
      "Table 10-2 summarizes the typical architecture of a classification MLP.\n",
      "\n",
      "Table 10-2. Typical Classification MLP Architecture\n",
      "\n",
      "Hyperparameter\n",
      "Input and hidden layers\n",
      "\n",
      "Binary classification Multilabel binary classification Multiclass classification\n",
      "Same as regression\n",
      "\n",
      "Same as regression\n",
      "\n",
      "Same as regression\n",
      "\n",
      "# output neurons\n",
      "\n",
      "1\n",
      "\n",
      "Output layer activation\n",
      "\n",
      "Logistic\n",
      "\n",
      "1 per label\n",
      "\n",
      "Logistic\n",
      "\n",
      "1 per class\n",
      "\n",
      "Softmax\n",
      "\n",
      "From Biological to Artificial Neurons \n",
      "\n",
      "| \n",
      "\n",
      "291\n",
      "\n",
      "\fHyperparameter\n",
      "Loss function\n",
      "\n",
      "Binary classification Multilabel binary classification Multiclass classification\n",
      "Cross-Entropy\n",
      "\n",
      "Cross-Entropy\n",
      "\n",
      "Cross-Entropy\n",
      "\n",
      "Before  we  go  on,  I  recommend  you  go  through  exercise  1,  at  the\n",
      "end  of  this  chapter.  You  will  play  with  various  neural  network\n",
      "architectures and visualize their outputs using the TensorFlow Play‐\n",
      "ground.  This  will  be  very  useful  to  better  understand  MLPs,  for\n",
      "example  the  effects  of  all  the  hyperparameters  (number  of  layers\n",
      "and neurons, activation functions, and more).\n",
      "\n",
      "Now you have all the concepts you need to start implementing MLPs with Keras!\n",
      "\n",
      "Implementing MLPs with Keras\n",
      "Keras is a high-level Deep Learning API that allows you to easily build, train, evaluate\n",
      "and execute all sorts of neural networks. Its documentation (or specification) is avail‐\n",
      "able at https://keras.io. The reference implementation is simply called Keras as well, so\n",
      "to  avoid  any  confusion  we  will  call  it  keras-team  (since  it  is  available  at  https://\n",
      "github.com/keras-team/keras).  It  was  developed  by  François  Chollet  as  part  of  a\n",
      "research  project12  and  released  as  an  open  source  project  in  March  2015.  It  quickly\n",
      "gained  popularity  owing  to  its  ease-of-use,  flexibility  and  beautiful  design.  To  per‐\n",
      "form  the  heavy  computations  required  by  neural  networks,  keras-team  relies  on  a\n",
      "computation  backend.  At  the  present,  you  can  choose  from  three  popular  open\n",
      "source  deep  learning  libraries:  TensorFlow,  Microsoft  Cognitive  Toolkit  (CNTK)  or\n",
      "Theano.\n",
      "\n",
      "Moreover,  since  late  2016,  other  implementations  have  been  released.  You  can  now\n",
      "run Keras on Apache MXNet, Apple’s Core ML, Javascript or Typescript (to run Keras\n",
      "code in a web browser), or PlaidML (which can run on all sorts of GPU devices, not\n",
      "just  Nvidia).  Moreover,  TensorFlow  itself  now  comes  bundled  with  its  own  Keras\n",
      "implementation called tf.keras. It only supports TensorFlow as the backend, but it has\n",
      "the  advantage  of  offering  some  very  useful  extra  features  (see  Figure  10-10):  for\n",
      "example,  it  supports  TensorFlow’s  Data  API  which  makes  it  quite  easy  to  load  and\n",
      "preprocess data efficiently. For this reason, we will use tf.keras in this book. However,\n",
      "in  this  chapter  we  will  not  use  any  of  the  TensorFlow-specific  features,  so  the  code\n",
      "should run fine on other Keras implementations as well (at least in Python), with only\n",
      "minor modifications, such as changing the imports.\n",
      "\n",
      "12 Project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System).\n",
      "\n",
      "292 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
      "\n",
      "\fFigure 10-10. Two Keras implementations: keras-team (left) and tf.keras (right)\n",
      "\n",
      "As tf.keras is bundled with TensorFlow, let’s install TensorFlow!\n",
      "\n",
      "Installing TensorFlow 2\n",
      "Assuming you installed Jupyter and Scikit-Learn by following the installation instruc‐\n",
      "tions  in  Chapter  2,  you  can  simply  use  pip  to  install  TensorFlow.  If  you  created  an\n",
      "isolated environment using virtualenv, you first need to activate it:\n",
      "\n",
      "$ cd $ML_PATH              # Your ML working directory (e.g., $HOME/ml)\n",
      "$ source env/bin/activate  # on Linux or MacOSX\n",
      "$ .\\env\\Scripts\\activate   # on Windows\n",
      "\n",
      "Next, install TensorFlow 2 (if you are not using a virtualenv, you will need adminis‐\n",
      "trator rights, or to add the --user option):\n",
      "\n",
      "$ python3 -m pip install --upgrade tensorflow\n",
      "\n",
      "For  GPU  support,  you  need  to  install  tensorflow-gpu  instead  of\n",
      "tensorflow,  and  there  are  other  libraries  to  install.  See  https://\n",
      "tensorflow.org/install/gpu for more details.\n",
      "\n",
      "To test your installation, open a Python shell or a Jupyter notebook, then import Ten‐\n",
      "sorFlow and tf.keras, and print their versions:\n",
      "\n",
      ">>> import tensorflow as tf\n",
      ">>> from tensorflow import keras\n",
      ">>> tf.__version__\n",
      "'2.0.0'\n",
      ">>> keras.__version__\n",
      "'2.2.4-tf'\n",
      "\n",
      "Implementing MLPs with Keras \n",
      "\n",
      "| \n",
      "\n",
      "293\n",
      "\n",
      "\fThe second version is the version of the Keras API implemented by tf.keras. Note that\n",
      "it  ends  with  -tf,  highlighting  the  fact  that  tf.keras  implements  the  Keras  API,  plus\n",
      "some extra TensorFlow-specific features.\n",
      "\n",
      "Now let’s use tf.keras! Let’s start by building a simple image classifier.\n",
      "\n",
      "Building an Image Classifier Using the Sequential API\n",
      "First,  we  need  to  load  a  dataset.  We  will  tackle  Fashion  MNIST,  which  is  a  drop-in\n",
      "replacement  of  MNIST  (introduced  in  Chapter  3).  It  has  the  exact  same  format  as\n",
      "MNIST  (70,000  grayscale  images  of  28×28  pixels  each,  with  10  classes),  but  the\n",
      "images represent fashion items rather than handwritten digits, so each class is more\n",
      "diverse and the problem turns out to be significantly more challenging than MNIST.\n",
      "For example, a simple linear model reaches about 92% accuracy on MNIST, but only\n",
      "about 83% on Fashion MNIST.\n",
      "\n",
      "Using Keras to Load the Dataset\n",
      "\n",
      "Keras provides some utility functions to fetch and load common datasets, including\n",
      "MNIST, Fashion MNIST, the original California housing dataset, and more. Let’s load\n",
      "Fashion MNIST:\n",
      "\n",
      "fashion_mnist = keras.datasets.fashion_mnist\n",
      "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
      "\n",
      "When loading MNIST or Fashion MNIST using Keras rather than Scikit-Learn, one\n",
      "important difference is that every image is represented as a 28×28 array rather than a\n",
      "1D array of size 784. Moreover, the pixel intensities are represented as integers (from\n",
      "0 to 255) rather than floats (from 0.0 to 255.0). Here is the shape and data type of the\n",
      "training set:\n",
      "\n",
      ">>> X_train_full.shape\n",
      "(60000, 28, 28)\n",
      ">>> X_train_full.dtype\n",
      "dtype('uint8')\n",
      "\n",
      "Note that the dataset is already split into a training set and a test set, but there is no\n",
      "validation set, so let’s create one. Moreover, since we are going to train the neural net‐\n",
      "work using Gradient Descent, we must scale the input features. For simplicity, we just\n",
      "scale the pixel intensities down to the 0-1 range by dividing them by 255.0 (this also\n",
      "converts them to floats):\n",
      "\n",
      "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
      "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
      "\n",
      "With  MNIST,  when  the  label  is  equal  to  5,  it  means  that  the  image  represents  the\n",
      "handwritten  digit  5.  Easy.  However,  for  Fashion  MNIST,  we  need  the  list  of  class\n",
      "names to know what we are dealing with:\n",
      "\n",
      "294 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
      "\n",
      "\fclass_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
      "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
      "\n",
      "For example, the first image in the training set represents a coat:\n",
      "\n",
      ">>> class_names[y_train[0]]\n",
      "'Coat'\n",
      "\n",
      "Figure 10-11 shows a few samples from the Fashion MNIST dataset:\n",
      "\n",
      "Figure 10-11. Samples from Fashion MNIST\n",
      "\n",
      "Creating the Model Using the Sequential API\n",
      "\n",
      "Now let’s build the neural network! Here is a classification MLP with two hidden lay‐\n",
      "ers:\n",
      "\n",
      "model = keras.models.Sequential()\n",
      "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
      "model.add(keras.layers.Dense(300, activation=\"relu\"))\n",
      "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
      "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
      "\n",
      "Let’s go through this code line by line:\n",
      "\n",
      "• The  first  line  creates  a  Sequential  model.  This  is  the  simplest  kind  of  Keras\n",
      "model, for neural networks that are just composed of a single stack of layers, con‐\n",
      "nected sequentially. This is called the sequential API.\n",
      "\n",
      "• Next, we build the first layer and add it to the model. It is a Flatten layer whose\n",
      "role is simply to convert each input image into a 1D array: if it receives input data\n",
      "X, it computes X.reshape(-1, 1). This layer does not have any parameters, it is\n",
      "just there to do some simple preprocessing. Since it is the first layer in the model,\n",
      "you should specify the input_shape: this does not include the batch size, only the\n",
      "shape of the instances. Alternatively, you could add a keras.layers.InputLayer\n",
      "as the first layer, setting shape=[28,28].\n",
      "\n",
      "• Next we add a Dense hidden layer with 300 neurons. It will use the ReLU activa‐\n",
      "tion function. Each Dense layer manages its own weight matrix, containing all the\n",
      "connection weights between the neurons and their inputs. It also manages a vec‐\n",
      "\n",
      "Implementing MLPs with Keras \n",
      "\n",
      "| \n",
      "\n",
      "295\n",
      "\n",
      "\ftor of bias terms (one per neuron). When it receives some input data, it computes\n",
      "Equation 10-2.\n",
      "\n",
      "• Next we add a second Dense hidden layer with 100 neurons, also using the ReLU\n",
      "\n",
      "activation function.\n",
      "\n",
      "• Finally, we add a  Dense output layer with 10 neurons (one per class), using the\n",
      "\n",
      "softmax activation function (because the classes are exclusive).\n",
      "\n",
      "Specifying  activation=\"relu\" \n",
      "tion=keras.activations.relu.  Other  activation  functions  are\n",
      "available  in  the  keras.activations  package,  we  will  use  many  of\n",
      "them in this book. See https://keras.io/activations/ for the full list.\n",
      "\n",
      "to  activa\n",
      "\n",
      "equivalent \n",
      "\n",
      "is \n",
      "\n",
      "Instead  of  adding  the  layers  one  by  one  as  we  just  did,  you  can  pass  a  list  of  layers\n",
      "when creating the Sequential model:\n",
      "\n",
      "model = keras.models.Sequential([\n",
      "    keras.layers.Flatten(input_shape=[28, 28]),\n",
      "    keras.layers.Dense(300, activation=\"relu\"),\n",
      "    keras.layers.Dense(100, activation=\"relu\"),\n",
      "    keras.layers.Dense(10, activation=\"softmax\")\n",
      "])\n",
      "\n",
      "Using Code Examples From keras.io\n",
      "Code examples documented on keras.io will work fine with tf.keras, but you need to\n",
      "change the imports. For example, consider this keras.io code:\n",
      "\n",
      "from keras.layers import Dense\n",
      "output_layer = Dense(10)\n",
      "\n",
      "You must change the imports like this:\n",
      "\n",
      "from tensorflow.keras.layers import Dense\n",
      "output_layer = Dense(10)\n",
      "\n",
      "Or simply use full paths, if you prefer:\n",
      "\n",
      "from tensorflow import keras\n",
      "output_layer = keras.layers.Dense(10)\n",
      "\n",
      "This is more verbose, but I use this approach in this book so you can easily see which\n",
      "packages to use, and to avoid confusion between standard classes and custom classes.\n",
      "In production code, I use the previous approach, as do most people.\n",
      "\n",
      "296 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
      "\n",
      "\fThe model’s summary() method displays all the model’s layers13, including each layer’s\n",
      "name (which is automatically generated unless you set it when creating the layer), its\n",
      "output shape (None means the batch size can be anything), and its number of parame‐\n",
      "ters. The summary ends with the total number of parameters, including trainable and\n",
      "non-trainable parameters. Here we only have trainable parameters (we will see exam‐\n",
      "ples of non-trainable parameters in Chapter 11):\n",
      "\n",
      ">>> model.summary()\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #\n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0\n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               235500\n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               30100\n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1010\n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "\n",
      "Note that  Dense layers often have a lot of parameters. For example, the first hidden\n",
      "layer  has  784  ×  300  connection  weights,  plus  300  bias  terms,  which  adds  up  to\n",
      "235,500 parameters! This gives the model quite a lot of flexibility to fit the training\n",
      "data, but it also means that the model runs the risk of overfitting, especially when you\n",
      "do not have a lot of training data. We will come back to this later.\n",
      "\n",
      "You can easily get a model’s list of layers, to fetch a layer by its index, or you can fetch\n",
      "it by name:\n",
      "\n",
      ">>> model.layers\n",
      "[<tensorflow.python.keras.layers.core.Flatten at 0x132414e48>,\n",
      " <tensorflow.python.keras.layers.core.Dense at 0x1324149b0>,\n",
      " <tensorflow.python.keras.layers.core.Dense at 0x1356ba8d0>,\n",
      " <tensorflow.python.keras.layers.core.Dense at 0x13240d240>]\n",
      ">>> model.layers[1].name\n",
      "'dense_3'\n",
      ">>> model.get_layer('dense_3').name\n",
      "'dense_3'\n",
      "\n",
      "All  the  parameters  of  a  layer  can  be  accessed  using  its  get_weights()  and\n",
      "set_weights() method. For a Dense layer, this includes both the connection weights\n",
      "and the bias terms:\n",
      "\n",
      "13 You can also generate an image of your model using keras.utils.plot_model().\n",
      "\n",
      "Implementing MLPs with Keras \n",
      "\n",
      "| \n",
      "\n",
      "297\n",
      "\n",
      "\f>>> weights, biases = hidden1.get_weights()\n",
      ">>> weights\n",
      "array([[ 0.03854964, -0.04054524,  0.00599282, ...,  0.02566582,\n",
      "         0.01032123,  0.06914985],\n",
      "       ...,\n",
      "       [ 0.02632413, -0.05105981, -0.00332005, ...,  0.04175945,\n",
      "         0.0443138 , -0.05558084]], dtype=float32)\n",
      ">>> weights.shape\n",
      "(784, 300)\n",
      ">>> biases\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., ...,  0., 0., 0.], dtype=float32)\n",
      ">>> biases.shape\n",
      "(300,)\n",
      "\n",
      "Notice  that  the  Dense  layer  initialized  the  connection  weights  randomly  (which  is\n",
      "needed to break symmetry, as we discussed earlier), and the biases were just initial‐\n",
      "ized to zeros, which is fine. If you ever want to use a different initialization method,\n",
      "you can set kernel_initializer (kernel is another name for the matrix of connec‐\n",
      "tion weights) or bias_initializer when creating the layer. We will discuss initializ‐\n",
      "ers further in Chapter 11, but if you want the full list, see https://keras.io/initializers/.\n",
      "\n",
      "The shape of the weight matrix depends on the number of inputs.\n",
      "This  is  why  it  is  recommended  to  specify  the  input_shape  when\n",
      "creating the first layer in a Sequential model. However, if you do\n",
      "not specify the input shape, it’s okay: Keras will simply wait until it\n",
      "knows the input shape before it actually builds the model. This will\n",
      "happen either when you feed it actual data (e.g., during training),\n",
      "or  when  you  call  its  build()  method.  Until  the  model  is  really\n",
      "built, the layers will not have any weights, and you will not be able\n",
      "to do certain things (such as print the model summary or save the\n",
      "model), so if you know the input shape when creating the model, it\n",
      "is best to specify it.\n",
      "\n",
      "Compiling the Model\n",
      "\n",
      "After a model is created, you must call its compile() method to specify the loss func‐\n",
      "tion and the optimizer to use. Optionally, you can also specify a list of extra metrics to\n",
      "compute during training and evaluation:\n",
      "\n",
      "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
      "              optimizer=\"sgd\",\n",
      "              metrics=[\"accuracy\"])\n",
      "\n",
      "298 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
      "\n",
      "\fUsing loss=\"sparse_categorical_crossentropy\" is equivalent to\n",
      "loss=keras.losses.sparse_categorical_crossentropy. \n",
      "Simi‐\n",
      "larly, optimizer=\"sgd\" is equivalent to optimizer=keras.optimiz\n",
      "metrics=[\"accuracy\"] \n",
      "ers.SGD() \n",
      "to\n",
      "metrics=[keras.metrics.sparse_categorical_accuracy] (when\n",
      "using this loss). We will use many other losses, optimizers and met‐\n",
      "rics  in  this  book,  but  for  the  full  lists  see  https://keras.io/losses/,\n",
      "https://keras.io/optimizers/ and https://keras.io/metrics/.\n",
      "\n",
      "equivalent \n",
      "\n",
      "and \n",
      "\n",
      "is \n",
      "\n",
      "This  requires  some  explanation.  First,  we  use  the  \"sparse_categorical_crossen\n",
      "tropy\" loss because we have sparse labels (i.e., for each instance there is just a target\n",
      "class index, from 0 to 9 in this case), and the classes are exclusive. If instead we had\n",
      "one target probability per class for each instance (such as one-hot vectors, e.g. [0.,\n",
      "0., 0., 1., 0., 0., 0., 0., 0., 0.] to represent class 3), then we would need\n",
      "to use the \"categorical_crossentropy\" loss instead. If we were doing binary classi‐\n",
      "fication  (with  one  or  more  binary  labels),  then  we  would  use  the  \"sigmoid\"  (i.e.,\n",
      "logistic)  activation  function  in  the  output  layer  instead  of  the  \"softmax\"  activation\n",
      "function, and we would use the \"binary_crossentropy\" loss.\n",
      "\n",
      "If you want to convert sparse labels (i.e., class indices) to one-hot\n",
      "vector  labels,  you  can  use  the  keras.utils.to_categorical()\n",
      "function. To go the other way round, you can just use the np.arg\n",
      "max() function with axis=1.\n",
      "\n",
      "Secondly, regarding the optimizer, \"sgd\" simply means that we will train the model\n",
      "using  simple  Stochastic  Gradient  Descent.  In  other  words,  Keras  will  perform  the\n",
      "backpropagation  algorithm  described  earlier  (i.e.,  reverse-mode  autodiff  +  Gradient\n",
      "Descent). We will discuss more efficient optimizers in Chapter 11 (they improve the\n",
      "Gradient Descent part, not the autodiff).\n",
      "\n",
      "Finally, since this is a classifier, it’s useful to measure its \"accuracy\" during training\n",
      "and evaluation.\n",
      "\n",
      "Training and Evaluating the Model\n",
      "\n",
      "Now  the  model  is  ready  to  be  trained.  For  this  we  simply  need  to  call  its  fit()\n",
      "method. We pass it the input features (X_train) and the target classes (y_train), as\n",
      "well as the number of epochs to train (or else it would default to just 1, which would\n",
      "definitely not be enough to converge to a good solution). We also pass a validation set\n",
      "(this is optional): Keras will measure the loss and the extra metrics on this set at the\n",
      "end of each epoch, which is very useful to see how well the model really performs: if\n",
      "the  performance  on  the  training  set  is  much  better  than  on  the  validation  set,  your\n",
      "\n",
      "Implementing MLPs with Keras \n",
      "\n",
      "| \n",
      "\n",
      "299\n",
      "\n",
      "\fmodel is probably overfitting the training set (or there is a bug, such as a data mis‐\n",
      "match between the training set and the validation set):\n",
      "\n",
      ">>> history = model.fit(X_train, y_train, epochs=30,\n",
      "...                     validation_data=(X_valid, y_valid))\n",
      "...\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/30\n",
      "55000/55000 [==========] - 3s 55us/sample - loss: 1.4948     - acc: 0.5757\n",
      "                                          - val_loss: 1.0042 - val_acc: 0.7166\n",
      "Epoch 2/30\n",
      "55000/55000 [==========] - 3s 55us/sample - loss: 0.8690     - acc: 0.7318\n",
      "                                          - val_loss: 0.7549 - val_acc: 0.7616\n",
      "[...]\n",
      "Epoch 50/50\n",
      "55000/55000 [==========] - 4s 72us/sample - loss: 0.3607     - acc: 0.8752\n",
      "                                          - val_loss: 0.3706 - val_acc: 0.8728\n",
      "\n",
      "And that’s it! The neural network is trained. At each epoch during training, Keras dis‐\n",
      "plays the number of instances processed so far (along with a progress bar), the mean\n",
      "training time per sample, the loss and accuracy (or any other extra metrics you asked\n",
      "for), both on the training set and the validation set. You can see that the training loss\n",
      "went down, which is a good sign, and the validation accuracy reached 87.28% after 50\n",
      "epochs,  not  too  far  from  the  training  accuracy,  so  there  does  not  seem  to  be  much\n",
      "overfitting going on.\n",
      "\n",
      "Instead  of  passing  a  validation  set  using  the  validation_data\n",
      "argument, you could instead set validation_split to the ratio of\n",
      "the training set that you want Keras to use for validation (e.g., 0.1).\n",
      "\n",
      "If the training set was very skewed, with some classes being overrepresented and oth‐\n",
      "ers  underrepresented,  it  would  be  useful  to  set  the  class_weight  argument  when\n",
      "calling  the  fit()  method,  giving  a  larger  weight  to  underrepresented  classes,  and  a\n",
      "lower weight to overrepresented classes. These weights would be used by Keras when\n",
      "computing  the  loss.  If  you  need  per-instance  weights  instead,  you  can  set  the  sam\n",
      "ple_weight argument (it supersedes class_weight). This could be useful for exam‐\n",
      "ple  if  some  instances  were  labeled  by  experts  while  others  were  labeled  using  a\n",
      "crowdsourcing platform: you might want to give more weight to the former. You can\n",
      "also provide sample weights (but not class weights) for the validation set by adding\n",
      "them as a third item in the validation_data tuple.\n",
      "\n",
      "The fit() method returns a History object containing the training parameters (his\n",
      "tory.params), the list of epochs it went through (history.epoch), and most impor‐\n",
      "tantly  a  dictionary  (history.history)  containing  the  loss  and  extra  metrics  it\n",
      "measured  at  the  end  of  each  epoch  on  the  training  set  and  on  the  validation  set  (if\n",
      "\n",
      "300 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
      "\n",
      "\fany).  If  you  create  a  Pandas  DataFrame  using  this  dictionary  and  call  its  plot()\n",
      "method, you get the learning curves shown in Figure 10-12:\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
      "plt.grid(True)\n",
      "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
      "plt.show()\n",
      "\n",
      "Figure 10-12. Learning Curves\n",
      "\n",
      "You  can  see  that  both  the  training  and  validation  accuracy  steadily  increase  during\n",
      "training, while the training and validation loss decrease. Good! Moreover, the valida‐\n",
      "tion curves are quite close to the training curves, which means that there is not too\n",
      "much overfitting. In this particular case, the model performed better on the valida‐\n",
      "tion set than on the training set at the beginning of training: this sometimes happens\n",
      "by chance (especially when the validation set is fairly small). However, the training set\n",
      "performance  ends  up  beating  the  validation  performance,  as  is  generally  the  case\n",
      "when you train for long enough. You can tell that the model has not quite converged\n",
      "yet, as the validation loss is still going down, so you should probably continue train‐\n",
      "ing. It’s as simple as calling the fit() method again, since Keras just continues train‐\n",
      "ing where it left off (you should be able to reach close to 89% validation accuracy).\n",
      "\n",
      "If you are not satisfied with the performance of your model, you should go back and\n",
      "tune the model’s hyperparameters, for example the number of layers, the number of\n",
      "neurons per layer, the types of activation functions we use for each hidden layer, the\n",
      "\n",
      "Implementing MLPs with Keras \n",
      "\n",
      "| \n",
      "\n",
      "301\n",
      "\n",
      "\fnumber of training epochs, the batch size (it can be set in the fit() method using the\n",
      "batch_size  argument,  which  defaults  to  32).  We  will  get  back  to  hyperparameter\n",
      "tuning at the end of this chapter. Once you are satisfied with your model’s validation\n",
      "accuracy,  you  should  evaluate  it  on  the  test  set  to  estimate  the  generalization  error\n",
      "before  you  deploy  the  model  to  production.  You  can  easily  do  this  using  the  evalu\n",
      "ate() method (it also supports several other arguments, such as batch_size or sam\n",
      "ple_weight, please check the documentation for more details):\n",
      "\n",
      ">>> model.evaluate(X_test, y_test)\n",
      "8832/10000 [==========================] - ETA: 0s - loss: 0.4074 - acc: 0.8540\n",
      "[0.40738476498126985, 0.854]\n",
      "\n",
      "As we saw in Chapter 2, it is common to get slightly lower performance on the test set\n",
      "than on the validation set, because the hyperparameters are tuned on the validation\n",
      "set, not the test set (however, in this example, we did not do any hyperparameter tun‐\n",
      "ing,  so  the  lower  accuracy  is  just  bad  luck).  Remember  to  resist  the  temptation  to\n",
      "tweak the hyperparameters on the test set, or else your estimate of the generalization\n",
      "error will be too optimistic.\n",
      "\n",
      "Using the Model to Make Predictions\n",
      "\n",
      "Next, we can use the model’s predict() method to make predictions on new instan‐\n",
      "ces. Since we don’t have actual new instances, we will just use the first 3 instances of\n",
      "the test set:\n",
      "\n",
      ">>> X_new = X_test[:3]\n",
      ">>> y_proba = model.predict(X_new)\n",
      ">>> y_proba.round(2)\n",
      "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.12, 0.  , 0.79],\n",
      "       [0.  , 0.  , 0.94, 0.  , 0.02, 0.  , 0.04, 0.  , 0.  , 0.  ],\n",
      "       [0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n",
      "      dtype=float32)\n",
      "\n",
      "As you can see, for each instance the model estimates one probability per class, from\n",
      "class 0 to class 9. For example, for the first image it estimates that the probability of\n",
      "class 9 (ankle boot) is 79%, the probability of class 7 (sneaker) is 12%, the probability\n",
      "of  class  5  (sandal)  is  9%,  and  the  other  classes  are  negligible.  In  other  words,  it\n",
      "“believes”  it’s  footwear,  probably  ankle  boots,  but  it’s  not  entirely  sure,  it  might  be\n",
      "sneakers or sandals instead. If you only care about the class with the highest estima‐\n",
      "ted  probability  (even  if  that  probability  is  quite  low)  then  you  can  use  the  pre\n",
      "dict_classes() method instead:\n",
      "\n",
      ">>> y_pred = model.predict_classes(X_new)\n",
      ">>> y_pred\n",
      "array([9, 2, 1])\n",
      ">>> np.array(class_names)[y_pred]\n",
      "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')\n",
      "\n",
      "And the classifier actually classified all three images correctly:\n",
      "\n",
      "302 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
      "\n",
      "\f>>> y_new = y_test[:3]\n",
      ">>> y_new\n",
      "array([9, 2, 1])\n",
      "\n",
      "Now you know how to build, train, evaluate and use a classification MLP using the\n",
      "Sequential API. But what about regression?\n",
      "\n",
      "Building a Regression MLP Using the Sequential API\n",
      "Let’s switch to the California housing problem and tackle it using a regression neural\n",
      "network.  For  simplicity,  we  will  use  Scikit-Learn’s  fetch_california_housing()\n",
      "function to load the data: this dataset is simpler than the one we used in Chapter 2,\n",
      "since it contains only numerical features (there is no ocean_proximity feature), and\n",
      "there is no missing value. After loading the data, we split it into a training set, a vali‐\n",
      "dation set and a test set, and we scale all the features:\n",
      "\n",
      "from sklearn.datasets import fetch_california_housing\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "housing = fetch_california_housing()\n",
      "\n",
      "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
      "    housing.data, housing.target)\n",
      "X_train, X_valid, y_train, y_valid = train_test_split(\n",
      "    X_train_full, y_train_full)\n",
      "\n",
      "scaler = StandardScaler()\n",
      "X_train_scaled = scaler.fit_transform(X_train)\n",
      "X_valid_scaled = scaler.transform(X_valid)\n",
      "X_test_scaled = scaler.transform(X_test)\n",
      "\n",
      "Building, training, evaluating and using a regression MLP using the Sequential API to\n",
      "make predictions is quite similar to what we did for classification. The main differ‐\n",
      "ences  are  the  fact  that  the  output  layer  has  a  single  neuron  (since  we  only  want  to\n",
      "predict  a  single  value)  and  uses  no  activation  function,  and  the  loss  function  is  the\n",
      "mean squared error. Since the dataset is quite noisy, we just use a single hidden layer\n",
      "with fewer neurons than before, to avoid overfitting:\n",
      "\n",
      "model = keras.models.Sequential([\n",
      "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
      "    keras.layers.Dense(1)\n",
      "])\n",
      "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
      "history = model.fit(X_train, y_train, epochs=20,\n",
      "                    validation_data=(X_valid, y_valid))\n",
      "mse_test = model.evaluate(X_test, y_test)\n",
      "X_new = X_test[:3] # pretend these are new instances\n",
      "y_pred = model.predict(X_new)\n",
      "\n",
      "Implementing MLPs with Keras \n",
      "\n",
      "| \n",
      "\n",
      "303\n",
      "\n",
      "\fAs you can see, the Sequential API is quite easy to use. However, although sequential\n",
      "models are extremely common, it is sometimes useful to build neural networks with\n",
      "more complex topologies, or with multiple inputs or outputs. For this purpose, Keras\n",
      "offers the Functional API.\n",
      "\n",
      "Building Complex Models Using the Functional API\n",
      "One example of a non-sequential neural network is a Wide & Deep neural network.\n",
      "This neural network architecture was introduced in a 2016 paper by Heng-Tze Cheng\n",
      "et al.14. It connects all or part of the inputs directly to the output layer, as shown in\n",
      "Figure 10-13. This architecture makes it possible for the neural network to learn both\n",
      "deep  patterns  (using  the  deep  path)  and  simple  rules  (through  the  short  path).  In\n",
      "contrast, a regular MLP forces all the data to flow through the full stack of layers, thus\n",
      "simple patterns in the data may end up being distorted by this sequence of transfor‐\n",
      "mations.\n",
      "\n",
      "14 “Wide & Deep Learning for Recommender Systems,” Heng-Tze Cheng et al. (2016).\n",
      "\n",
      "304 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
      "\n",
      "\fFigure 10-13. Wide and Deep Neural Network\n",
      "\n",
      "Let’s build such a neural network to tackle the California housing problem:\n",
      "\n",
      "input = keras.layers.Input(shape=X_train.shape[1:])\n",
      "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input)\n",
      "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
      "concat = keras.layers.Concatenate()[input, hidden2])\n",
      "output = keras.layers.Dense(1)(concat)\n",
      "model = keras.models.Model(inputs=[input], outputs=[output])\n",
      "\n",
      "Let’s go through each line of this code:\n",
      "\n",
      "• First,  we  need  to  create  an  Input  object.  This  is  needed  because  we  may  have\n",
      "\n",
      "multiple inputs, as we will see later.\n",
      "\n",
      "• Next,  we  create  a  Dense  layer  with  30  neurons  and  using  the  ReLU  activation\n",
      "function. As soon as it is created, notice that we call it like a function, passing it\n",
      "the input. This is why this is called the Functional API. Note that we are just tell‐\n",
      "\n",
      "Implementing MLPs with Keras \n",
      "\n",
      "| \n",
      "\n",
      "305\n",
      "\n",
      "\fing Keras how it should connect the layers together, no actual data is being pro‐\n",
      "cessed yet.\n",
      "\n",
      "• We  then  create  a  second  hidden  layer,  and  again  we  use  it  as  a  function.  Note\n",
      "\n",
      "however that we pass it the output of the first hidden layer.\n",
      "\n",
      "• Next, we create a Concatenate() layer, and once again we immediately use it like\n",
      "a  function,  to  concatenate  the  input  and  the  output  of  the  second  hidden  layer\n",
      "(you may prefer the keras.layers.concatenate() function, which creates a Con\n",
      "catenate layer and immediately calls it with the given inputs).\n",
      "\n",
      "• Then we create the output layer, with a single neuron and no activation function,\n",
      "\n",
      "and we call it like a function, passing it the result of the concatenation.\n",
      "• Lastly, we create a Keras Model, specifying which inputs and outputs to use.\n",
      "\n",
      "Once you have built the Keras model, everything is exactly like earlier, so no need to\n",
      "repeat  it  here:  you  must  compile  the  model,  train  it,  evaluate  it  and  use  it  to  make\n",
      "predictions.\n",
      "\n",
      "But  what  if  you  want  to  send  a  subset  of  the  features  through  the  wide  path,  and  a\n",
      "different subset (possibly overlapping) through the deep path (see Figure 10-14)? In\n",
      "this  case,  one  solution  is  to  use  multiple  inputs.  For  example,  suppose  we  want  to\n",
      "send  5  features  through  the  deep  path  (features  0  to  4),  and  6  features  through  the\n",
      "wide path (features 2 to 7):\n",
      "\n",
      "input_A = keras.layers.Input(shape=[5])\n",
      "input_B = keras.layers.Input(shape=[6])\n",
      "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
      "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
      "concat = keras.layers.concatenate([input_A, hidden2])\n",
      "output = keras.layers.Dense(1)(concat)\n",
      "model = keras.models.Model(inputs=[input_A, input_B], outputs=[output])\n",
      "\n",
      "306 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
      "\n",
      "\fFigure 10-14. Handling Multiple Inputs\n",
      "\n",
      "The  code  is  self-explanatory.  Note  that  we  specified  inputs=[input_A,  input_B]\n",
      "when creating the model. Now we can compile the model as usual, but when we call\n",
      "the fit() method, instead of passing a single input matrix X_train, we must pass a\n",
      "pair  of  matrices  (X_train_A,  X_train_B):  one  per  input.  The  same  is  true  for\n",
      "X_valid, and also for X_test and X_new when you call evaluate() or predict():\n",
      "\n",
      "model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
      "\n",
      "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
      "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
      "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
      "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
      "\n",
      "history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
      "                    validation_data=((X_valid_A, X_valid_B), y_valid))\n",
      "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
      "y_pred = model.predict((X_new_A, X_new_B))\n",
      "\n",
      "Implementing MLPs with Keras \n",
      "\n",
      "| \n",
      "\n",
      "307\n",
      "\n",
      "\fThere are also many use cases in which you may want to have multiple outputs:\n",
      "\n",
      "• The  task  may  demand  it,  for  example  you  may  want  to  locate  and  classify  the\n",
      "main object in a picture. This is both a regression task (finding the coordinates of\n",
      "the object’s center, as well as its width and height) and a classification task.\n",
      "\n",
      "• Similarly,  you  may  have  multiple  independent  tasks  to  perform  based  on  the\n",
      "same data. Sure, you could train one neural network per task, but in many cases\n",
      "you  will  get  better  results  on  all  tasks  by  training  a  single  neural  network  with\n",
      "one output per task. This is because the neural network can learn features in the\n",
      "data that are useful across tasks.\n",
      "\n",
      "• Another use case is as a regularization technique (i.e., a training constraint whose\n",
      "objective is to reduce overfitting and thus improve the model’s ability to general‐\n",
      "ize). For example, you may want to add some auxiliary outputs in a neural net‐\n",
      "work  architecture  (see  Figure  10-15)  to  ensure  that  the  underlying  part  of  the\n",
      "network  learns  something  useful  on  its  own,  without  relying  on  the  rest  of  the\n",
      "network.\n",
      "\n",
      "Figure 10-15. Handling Multiple Outputs – Auxiliary Output for Regularization\n",
      "\n",
      "Adding  extra  outputs  is  quite  easy:  just  connect  them  to  the  appropriate  layers  and\n",
      "add them to your model’s list of outputs. For example, the following code builds the\n",
      "network represented in Figure 10-15:\n",
      "\n",
      "308 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
      "\n",
      "\f[...] # Same as above, up to the main output layer\n",
      "output = keras.layers.Dense(1)(concat)\n",
      "aux_output = keras.layers.Dense(1)(hidden2)\n",
      "model = keras.models.Model(inputs=[input_A, input_B],\n",
      "                           outputs=[output, aux_output])\n",
      "\n",
      "Each  output  will  need  its  own  loss  function,  so  when  we  compile  the  model  we\n",
      "should pass a list of losses (if we pass a single loss, Keras will assume that the same\n",
      "loss must be used for all outputs). By default, Keras will compute all these losses and\n",
      "simply  add  them  up  to  get  the  final  loss  used  for  training.  However,  we  care  much\n",
      "more about the main output than about the auxiliary output (as it is just used for reg‐\n",
      "ularization), so we want to give the main output’s loss a much greater weight. Fortu‐\n",
      "nately, it is possible to set all the loss weights when compiling the model:\n",
      "\n",
      "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=\"sgd\")\n",
      "\n",
      "Now  when  we  train  the  model,  we  need  to  provide  some  labels  for  each  output.  In\n",
      "this example, the main output and the auxiliary output should try to predict the same\n",
      "thing, so they should use the same labels. So instead of passing y_train, we just need\n",
      "to pass (y_train, y_train) (and the same goes for y_valid and y_test):\n",
      "\n",
      "history = model.fit(\n",
      "    [X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
      "    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))\n",
      "\n",
      "When we evaluate the model, Keras will return the total loss, as well as all the individ‐\n",
      "ual losses:\n",
      "\n",
      "total_loss, main_loss, aux_loss = model.evaluate(\n",
      "    [X_test_A, X_test_B], [y_test, y_test])\n",
      "\n",
      "Similarly, the predict() method will return predictions for each output:\n",
      "\n",
      "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])\n",
      "\n",
      "As you can see, you can build any sort of architecture you want quite easily with the\n",
      "Functional API. Let’s look at one last way you can build Keras models.\n",
      "\n",
      "Building Dynamic Models Using the Subclassing API\n",
      "Both the Sequential API and the Functional API are declarative: you start by declar‐\n",
      "ing which layers you want to use and how they should be connected, and only then\n",
      "can you start feeding the model some data for training or inference. This has many\n",
      "advantages:  the  model  can  easily  be  saved,  cloned,  shared,  its  structure  can  be  dis‐\n",
      "played and analyzed, the framework can infer shapes and check types, so errors can\n",
      "be caught early (i.e., before any data ever goes through the model). It’s also fairly easy\n",
      "to debug, since the whole model is just a static graph of layers. But the flip side is just\n",
      "that:  it’s  static.  Some  models  involve  loops,  varying  shapes,  conditional  branching,\n",
      "and other dynamic behaviors. For such cases, or simply if you prefer a more impera‐\n",
      "tive programming style, the Subclassing API is for you.\n",
      "\n",
      "Implementing MLPs with Keras \n",
      "\n",
      "| \n",
      "\n",
      "309\n",
      "\n",
      "\fSimply subclass the Model class, create the layers you need in the constructor, and use\n",
      "them to perform the computations you want in the call() method. For example, cre‐\n",
      "ating  an  instance  of  the  following  WideAndDeepModel  class  gives  us  an  equivalent\n",
      "model to the one we just built with the Functional API. You can then compile it, eval‐\n",
      "uate it and use it to make predictions, exactly like we just did.\n",
      "\n",
      "class WideAndDeepModel(keras.models.Model):\n",
      "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
      "        super().__init__(**kwargs) # handles standard args (e.g., name)\n",
      "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
      "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
      "        self.main_output = keras.layers.Dense(1)\n",
      "        self.aux_output = keras.layers.Dense(1)\n",
      "\n",
      "    def call(self, inputs):\n",
      "        input_A, input_B = inputs\n",
      "        hidden1 = self.hidden1(input_B)\n",
      "        hidden2 = self.hidden2(hidden1)\n",
      "        concat = keras.layers.concatenate([input_A, hidden2])\n",
      "        main_output = self.main_output(concat)\n",
      "        aux_output = self.aux_output(hidden2)\n",
      "        return main_output, aux_output\n",
      "\n",
      "model = WideAndDeepModel()\n",
      "\n",
      "This example looks very much like the Functional API, except we do not need to cre‐\n",
      "ate the inputs, we just use the input argument to the call() method, and we separate\n",
      "the creation of the layers15 in the constructor from their usage in the call() method.\n",
      "However, the big difference is that you can do pretty much anything you want in the\n",
      "call()  method:  for  loops,  if  statements,  low-level  TensorFlow  operations,  your\n",
      "imagination  is  the  limit  (see  Chapter  12)!  This  makes  it  a  great  API  for  researchers\n",
      "experimenting with new ideas.\n",
      "\n",
      "However,  this  extra  flexibility  comes  at  a  cost:  your  model’s  architecture  is  hidden\n",
      "within the call() method, so Keras cannot easily inspect it, it cannot save or clone it,\n",
      "and when you call the  summary() method, you only get a list of layers, without any\n",
      "information on how they are connected to each other. Moreover, Keras cannot check\n",
      "types and shapes ahead of time, and it is easier to make mistakes. So unless you really\n",
      "need  that  extra  flexibility,  you  should  probably  stick  to  the  Sequential  API  or  the\n",
      "Functional API.\n",
      "\n",
      "15 Keras models have an output attribute, so we cannot use that name for the main output layer, which is why\n",
      "\n",
      "we renamed it to main_output.\n",
      "\n",
      "310 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
      "\n",
      "\fKeras models can be used just like regular layers, so you can easily\n",
      "compose them to build complex architectures.\n",
      "\n",
      "Now that you know how to build and train neural nets using Keras, you will want to\n",
      "save them!\n",
      "\n",
      "Saving and Restoring a Model\n",
      "Saving a trained Keras model is as simple as it gets:\n",
      "\n",
      "model.save(\"my_keras_model.h5\")\n",
      "\n",
      "Keras  will  save  both  the  model’s  architecture  (including  every  layer’s  hyperparame‐\n",
      "ters)  and  the  value  of  all  the  model  parameters  for  every  layer  (e.g.,  connection\n",
      "weights and biases), using the HDF5 format. It also saves the optimizer (including its\n",
      "hyperparameters and any state it may have).\n",
      "\n",
      "You  will  typically  have  a  script  that  trains  a  model  and  saves  it,  and  one  or  more\n",
      "scripts (or web services) that load the model and use it to make predictions. Loading\n",
      "the model is just as easy:\n",
      "\n",
      "model = keras.models.load_model(\"my_keras_model.h5\")\n",
      "\n",
      "This  will  work  when  using  the  Sequential  API  or  the  Functional\n",
      "API,  but  unfortunately  not  when  using  Model  subclassing.  How‐\n",
      "ever, you can use save_weights() and load_weights() to at least\n",
      "save  and  restore  the  model  parameters  (but  you  will  need  to  save\n",
      "and restore everything else yourself).\n",
      "\n",
      "But what if training lasts several hours? This is quite common, especially when train‐\n",
      "ing on large datasets. In this case, you should not only save your model at the end of\n",
      "training, but also save checkpoints at regular intervals during training. But how can\n",
      "you tell the fit() method to save checkpoints? The answer is: using callbacks.\n",
      "\n",
      "Using Callbacks\n",
      "The fit() method accepts a callbacks argument that lets you specify a list of objects\n",
      "that Keras will call during training at the start and end of training, at the start and end\n",
      "of each epoch and even before and after processing each batch. For example, the Mod\n",
      "elCheckpoint  callback  saves  checkpoints  of  your  model  at  regular  intervals  during\n",
      "training, by default at the end of each epoch:\n",
      "\n",
      "Implementing MLPs with Keras \n",
      "\n",
      "| \n",
      "\n",
      "311\n",
      "\n",
      "\f[...] # build and compile the model\n",
      "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\")\n",
      "history = model.fit(X_train, y_train, epochs=10, callbacks=[checkpoint_cb])\n",
      "\n",
      "set  during \n",
      "\n",
      "if  you  use  a  validation \n",
      "\n",
      "set\n",
      "Moreover, \n",
      "save_best_only=True when creating the ModelCheckpoint. In this case, it will only\n",
      "save  your  model  when  its  performance  on  the  validation  set  is  the  best  so  far.  This\n",
      "way, you do not need to worry about training for too long and overfitting the training\n",
      "set: simply restore the last model saved after training, and this will be the best model\n",
      "on the validation set. This is a simple way to implement early stopping (introduced in\n",
      "Chapter 4):\n",
      "\n",
      "training,  you  can \n",
      "\n",
      "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\",\n",
      "                                                save_best_only=True)\n",
      "history = model.fit(X_train, y_train, epochs=10,\n",
      "                    validation_data=(X_valid, y_valid),\n",
      "                    callbacks=[checkpoint_cb])\n",
      "model = keras.models.load_model(\"my_keras_model.h5\") # rollback to best model\n",
      "\n",
      "Another  way  to  implement  early  stopping  is  to  simply  use  the  EarlyStopping  call‐\n",
      "back. It will interrupt training when it measures no progress on the validation set for\n",
      "a number of epochs (defined by the  patience argument), and it will optionally roll\n",
      "back to the best model. You can combine both callbacks to both save checkpoints of\n",
      "your  model  (in  case  your  computer  crashes),  and  actually  interrupt  training  early\n",
      "when there is no more progress (to avoid wasting time and resources):\n",
      "\n",
      "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
      "                                                  restore_best_weights=True)\n",
      "history = model.fit(X_train, y_train, epochs=100,\n",
      "                    validation_data=(X_valid, y_valid),\n",
      "                    callbacks=[checkpoint_cb, early_stopping_cb])\n",
      "\n",
      "The number of epochs can be set to a large value since training will stop automati‐\n",
      "cally when there is no more progress. Moreover, there is no need to restore the best\n",
      "model saved in this case since the EarlyStopping callback will keep track of the best\n",
      "weights and restore them for us at the end of training.\n",
      "\n",
      "There  are  many  other  callbacks  available  in  the  keras.callbacks\n",
      "package. See https://keras.io/callbacks/.\n",
      "\n",
      "If you need extra control, you can easily write your own custom callbacks. For exam‐\n",
      "ple,  the  following  custom  callback  will  display  the  ratio  between  the  validation  loss\n",
      "and the training loss during training (e.g., to detect overfitting):\n",
      "\n",
      "312 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
      "\n",
      "\fclass PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
      "    def on_epoch_end(self, epoch, logs):\n",
      "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))\n",
      "\n",
      "As  you  might  expect,  you  can  implement  on_train_begin(),  on_train_end(),\n",
      "on_epoch_begin(),  on_epoch_begin(),  on_batch_end()  and  on_batch_end().\n",
      "Moreover, callbacks can also be used during evaluation and predictions, should you\n",
      "ever  need  them  (e.g.,  for  debugging).  In  this  case,  you  should  implement\n",
      "on_test_begin(), \n",
      "or\n",
      "on_test_batch_end()  (called  by  evaluate()),  or  on_predict_begin(),  on_pre\n",
      "dict_end(),  on_predict_batch_begin(),  or  on_predict_batch_end()  (called  by\n",
      "predict()).\n",
      "\n",
      "on_test_batch_begin(), \n",
      "\n",
      "on_test_end(), \n",
      "\n",
      "Now  let’s  take  a  look  at  one  more  tool  you  should  definitely  have  in  your  toolbox\n",
      "when using tf.keras: TensorBoard.\n",
      "\n",
      "Visualization Using TensorBoard\n",
      "TensorBoard  is  a  great  interactive  visualization  tool  that  you  can  use  to  view  the\n",
      "learning curves during training, compare learning curves between multiple runs, vis‐\n",
      "ualize  the  computation  graph,  analyze  training  statistics,  view  images  generated  by\n",
      "your  model,  visualize  complex  multidimensional  data  projected  down  to  3D  and\n",
      "automatically clustered for you, and more! This tool is installed automatically when\n",
      "you install TensorFlow, so you already have it!\n",
      "\n",
      "To use it, you must modify your program so that it outputs the data you want to visu‐\n",
      "alize  to  special  binary  log  files  called  event  files.  Each  binary  data  record  is  called  a\n",
      "summary. The TensorBoard server will monitor the log directory, and it will automat‐\n",
      "ically pick up the changes and update the visualizations: this allows you to visualize\n",
      "live data (with a short delay), such as the learning curves during training. In general,\n",
      "you want to point the TensorBoard server to a root log directory, and configure your\n",
      "program so that it writes to a different subdirectory every time it runs. This way, the\n",
      "same TensorBoard server instance will allow you to visualize and compare data from\n",
      "multiple runs of your program, without getting everything mixed up.\n",
      "\n",
      "So let’s start by defining the root log directory we will use for our TensorBoard logs,\n",
      "plus a small function that will generate a subdirectory path based on the current date\n",
      "and time, so that it is different at every run. You may want to include extra informa‐\n",
      "tion in the log directory name, such as hyperparameter values that you are testing, to\n",
      "make it easier to know what you are looking at in TensorBoard:\n",
      "\n",
      "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
      "\n",
      "def get_run_logdir():\n",
      "    import time\n",
      "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
      "    return os.path.join(root_logdir, run_id)\n",
      "\n",
      "Implementing MLPs with Keras \n",
      "\n",
      "| \n",
      "\n",
      "313\n",
      "\n",
      "\frun_logdir = get_run_logdir() # e.g., './my_logs/run_2019_01_16-11_28_43'\n",
      "\n",
      "Next, the good news is that Keras provides a nice TensorBoard callback:\n",
      "\n",
      "[...] # Build and compile your model\n",
      "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
      "history = model.fit(X_train, y_train, epochs=30,\n",
      "                    validation_data=(X_valid, y_valid),\n",
      "                    callbacks=[tensorboard_cb])\n",
      "\n",
      "And that’s all there is to it! It could hardly be easier to use. If you run this code, the\n",
      "TensorBoard callback will take care of creating the log directory for you (along with\n",
      "its parent directories if needed), and during training it will create event files and write\n",
      "summaries  to  them.  After  running  the  program  a  second  time  (perhaps  changing\n",
      "some  hyperparameter  value),  you  will  end  up  with  a  directory  structure  similar  to\n",
      "this one:\n",
      "\n",
      "my_logs\n",
      "├── run_2019_01_16-16_51_02\n",
      "│   └── events.out.tfevents.1547628669.mycomputer.local.v2\n",
      "└── run_2019_01_16-16_56_50\n",
      "    └── events.out.tfevents.1547629020.mycomputer.local.v2\n",
      "\n",
      "Next you need to start the TensorBoard server. If you installed TensorFlow within a\n",
      "virtualenv, you should activate it. Next, run the following command at the root of the\n",
      "project (or from anywhere else as long as you point to the appropriate log directory).\n",
      "If  your  shell  cannot  find  the  tensorboard  script,  then  you  must  update  your  PATH\n",
      "environment  variable  so  that  it  contains  the  directory  in  which  the  script  was\n",
      "installed  (alternatively,  you  can  just  replace  tensorboard  with  python3  -m  tensor\n",
      "board.main).\n",
      "\n",
      "$ tensorboard --logdir=./my_logs --port=6006\n",
      "TensorBoard 2.0.0 at http://mycomputer.local:6006 (Press CTRL+C to quit)\n",
      "\n",
      "Finally, open up a web browser to http://localhost:6006. You should see TensorBoard’s\n",
      "web  interface.  Click  on  the  SCALARS  tab  to  view  the  learning  curves  (see\n",
      "Figure 10-16). Notice that the training loss went down nicely during both runs, but\n",
      "the second run went down much faster. Indeed, we used a larger learning rate by set‐\n",
      "ting  optimizer=keras.optimizers.SGD(lr=0.05) \n",
      "instead  of  optimizer=\"sgd\",\n",
      "which defaults to a learning rate of 0.001.\n",
      "\n",
      "314 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
      "\n",
      "\fFigure 10-16. Visualizing Learning Curves with TensorBoard\n",
      "\n",
      "Unfortunately, at the time of writing, no other data is exported by the TensorBoard\n",
      "callback, but this issue will probably be fixed by the time you read these lines. In Ten‐\n",
      "sorFlow 1, this callback exported the computation graph and many useful statistics:\n",
      "type help(keras.callbacks.TensorBoard) to see all the options.\n",
      "\n",
      "Let’s  summarize  what  you  learned  so  far  in  this  chapter:  we  saw  where  neural  nets\n",
      "came from, what an MLP is and how you can use it for classification and regression,\n",
      "how  to  build  MLPs  using  tf.keras’s  Sequential  API,  or  more  complex  architectures\n",
      "using the Functional API or Model Subclassing, you learned how to save and restore a\n",
      "model, use callbacks for checkpointing, early stopping, and more, and finally how to\n",
      "use TensorBoard for visualization. You can already go ahead and use neural networks\n",
      "to tackle many problems! However, you may wonder how to choose the number of\n",
      "hidden layers, the number of neurons in the network, and all the other hyperparame‐\n",
      "ters. Let’s look at this now.\n",
      "\n",
      "Fine-Tuning Neural Network Hyperparameters\n",
      "The flexibility of neural networks is also one of their main drawbacks: there are many\n",
      "hyperparameters  to  tweak.  Not  only  can  you  use  any  imaginable  network  architec‐\n",
      "ture, but even in a simple MLP you can change the number of layers, the number of\n",
      "neurons per layer, the type of activation function to use in each layer, the weight initi‐\n",
      "\n",
      "Fine-Tuning Neural Network Hyperparameters \n",
      "\n",
      "| \n",
      "\n",
      "315\n",
      "\n",
      "\falization logic, and much more. How do you know what combination of hyperpara‐\n",
      "meters is the best for your task?\n",
      "\n",
      "One  option  is  to  simply  try  many  combinations  of  hyperparameters  and  see  which\n",
      "one works best on the validation set (or using K-fold cross-validation). For this, one\n",
      "approach is simply use GridSearchCV or RandomizedSearchCV to explore the hyper‐\n",
      "parameter space, as we did in Chapter 2. For this, we need to wrap our Keras models\n",
      "in objects that mimic regular Scikit-Learn regressors. The first step is to create a func‐\n",
      "tion that will build and compile a Keras model, given a set of hyperparameters:\n",
      "\n",
      "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
      "    model = keras.models.Sequential()\n",
      "    options = {\"input_shape\": input_shape}\n",
      "    for layer in range(n_hidden):\n",
      "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\", **options))\n",
      "        options = {}\n",
      "    model.add(keras.layers.Dense(1, **options))\n",
      "    optimizer = keras.optimizers.SGD(learning_rate)\n",
      "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
      "    return model\n",
      "\n",
      "This function creates a simple Sequential model for univariate regression (only one\n",
      "output  neuron),  with  the  given  input  shape  and  the  given  number  of  hidden  layers\n",
      "and  neurons,  and  it  compiles  it  using  an  SGD  optimizer  configured  with  the  given\n",
      "learning rate. The options dict is used to ensure that the first layer is properly given\n",
      "the input shape (note that if n_hidden=0, the first layer will be the output layer). It is\n",
      "good practice to provide reasonable defaults to as many hyperparameters as you can,\n",
      "as Scikit-Learn does.\n",
      "\n",
      "Next, let’s create a KerasRegressor based on this build_model() function:\n",
      "\n",
      "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n",
      "\n",
      "The  KerasRegressor  object  is  a  thin  wrapper  around  the  Keras  model  built  using\n",
      "build_model(). Since we did not specify any hyperparameter when creating it, it will\n",
      "just use the default hyperparameters we defined in build_model(). Now we can use\n",
      "this  object  like  a  regular  Scikit-Learn  regressor:  we  can  train  it  using  its  fit()\n",
      "method,  then  evaluate  it  using  its  score()  method,  and  use  it  to  make  predictions\n",
      "using  its  predict()  method.  Note  that  any  extra  parameter  you  pass  to  the  fit()\n",
      "method  will  simply  get  passed  to  the  underlying  Keras  model.  Also  note  that  the\n",
      "score  will  be  the  opposite  of  the  MSE  because  Scikit-Learn  wants  scores,  not  losses\n",
      "(i.e., higher should be better).\n",
      "\n",
      "keras_reg.fit(X_train, y_train, epochs=100,\n",
      "              validation_data=(X_valid, y_valid),\n",
      "              callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
      "mse_test = keras_reg.score(X_test, y_test)\n",
      "y_pred = keras_reg.predict(X_new)\n",
      "\n",
      "316 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
      "\n",
      "\fHowever, we do not actually want to train and evaluate a single model like this, we\n",
      "want to train hundreds of variants and see which one performs best on the validation\n",
      "set. Since there are many hyperparameters, it is preferable to use a randomized search\n",
      "rather than grid search (as we discussed in Chapter 2). Let’s try to explore the number\n",
      "of hidden layers, the number of neurons and the learning rate:\n",
      "\n",
      "from scipy.stats import reciprocal\n",
      "from sklearn.model_selection import RandomizedSearchCV\n",
      "\n",
      "param_distribs = {\n",
      "    \"n_hidden\": [0, 1, 2, 3],\n",
      "    \"n_neurons\": np.arange(1, 100),\n",
      "    \"learning_rate\": reciprocal(3e-4, 3e-2),\n",
      "}\n",
      "\n",
      "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
      "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
      "                  validation_data=(X_valid, y_valid),\n",
      "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
      "\n",
      "As you can see, this is identical to what we did in Chapter 2, with the exception that\n",
      "we pass extra parameters to the fit() method: they simply get relayed to the under‐\n",
      "lying Keras models. Note that RandomizedSearchCV uses K-fold cross-validation, so it\n",
      "does not use X_valid and y_valid. These are just used for early stopping.\n",
      "\n",
      "The  exploration  may  last  many  hours  depending  on  the  hardware,  the  size  of  the\n",
      "dataset, the complexity of the model and the value of n_iter and cv. When it is over,\n",
      "you can access the best parameters found, the best score, and the trained Keras model\n",
      "like this:\n",
      "\n",
      ">>> rnd_search_cv.best_params_\n",
      "{'learning_rate': 0.0033625641252688094, 'n_hidden': 2, 'n_neurons': 42}\n",
      ">>> rnd_search_cv.best_score_\n",
      "-0.3189529188278931\n",
      ">>> model = rnd_search_cv.best_estimator_.model\n",
      "\n",
      "You can now save this model, evaluate it on the test set, and if you are satisfied with\n",
      "its  performance,  deploy  it  to  production.  Using  randomized  search  is  not  too  hard,\n",
      "and  it  works  well  for  many  fairly  simple  problems.  However,  when  training  is  slow\n",
      "(e.g.,  for  more  complex  problems  with  larger  datasets),  this  approach  will  only\n",
      "explore  a  tiny  portion  of  the  hyperparameter  space.  You  can  partially  alleviate  this\n",
      "problem  by  assisting  the  search  process  manually:  first  run  a  quick  random  search\n",
      "using  wide  ranges  of  hyperparameter  values,  then  run  another  search  using  smaller\n",
      "ranges of values centered on the best ones found during the first run, and so on. This\n",
      "will hopefully zoom in to a good set of hyperparameters. However, this is very time\n",
      "consuming, and probably not the best use of your time.\n",
      "\n",
      "Fortunately,  there  are  many  techniques  to  explore  a  search  space  much  more  effi‐\n",
      "ciently than randomly. Their core idea is simple: when a region of the space turns out\n",
      "\n",
      "Fine-Tuning Neural Network Hyperparameters \n",
      "\n",
      "| \n",
      "\n",
      "317\n",
      "\n",
      "\fto be good, it should be explored more. This takes care of the “zooming” process for\n",
      "you  and  leads  to  much  better  solutions  in  much  less  time.  Here  are  a  few  Python\n",
      "libraries you can use to optimize hyperparameters:\n",
      "\n",
      "• Hyperopt:  a  popular  Python  library  for  optimizing  over  all  sorts  of  complex\n",
      "search spaces (including real values such as the learning rate, or discrete values\n",
      "such as the number of layers).\n",
      "\n",
      "• Hyperas,  kopt  or  Talos:  optimizing  hyperparameters  for  Keras  model  (the  first\n",
      "\n",
      "two are based on Hyperopt).\n",
      "\n",
      "• Scikit-Optimize  (skopt):  a  general-purpose  optimization  library.  The  Bayes\n",
      "SearchCV class performs Bayesian optimization using an interface similar to Grid\n",
      "SearchCV.\n",
      "\n",
      "• Spearmint: a Bayesian optimization library.\n",
      "\n",
      "• Sklearn-Deap:  a  hyperparameter  optimization  library  based  on  evolutionary\n",
      "\n",
      "algorithms, also with a GridSearchCV-like interface.\n",
      "\n",
      "• And many more!\n",
      "\n",
      "Moreover,  many  companies  offer  services  for  hyperparameter  optimization.  For\n",
      "example Google Cloud ML Engine has a hyperparameter tuning service. Other com‐\n",
      "panies provide APIs for hyperparameter optimization, such as Arimo, SigOpt, Oscar\n",
      "and many more.\n",
      "\n",
      "Hyperparameter tuning is still an active area of research. Evolutionary algorithms are\n",
      "making a comeback lately. For example, check out DeepMind’s excellent 2017 paper16,\n",
      "where they jointly optimize a population of models and their hyperparameters. Goo‐\n",
      "gle  also  used  an  evolutionary  approach,  not  just  to  search  for  hyperparameters,  but\n",
      "also to look for the best neural network architecture for the problem. They call this\n",
      "AutoML,  and  it  is  already  available  as  a  cloud  service.  Perhaps  the  days  of  building\n",
      "neural networks manually will soon be over? Check out Google’s post on this topic. In\n",
      "fact, evolutionary algorithms have also been used successfully to train individual neu‐\n",
      "ral networks, replacing the ubiquitous Gradient Descent! See this 2017 post by Uber\n",
      "where they introduce their Deep Neuroevolution technique.\n",
      "\n",
      "Despite all this exciting progress, and all these tools and services, it still helps to have\n",
      "an  idea  of  what  values  are  reasonable  for  each  hyperparameter,  so  you  can  build  a\n",
      "quick prototype, and restrict the search space. Here are a few guidelines for choosing\n",
      "the number of hidden layers and neurons in an MLP, and selecting good values for\n",
      "some of the main hyperparameters.\n",
      "\n",
      "16 “Population Based Training of Neural Networks,” Max Jaderberg et al. (2017).\n",
      "\n",
      "318 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
      "\n",
      "\fNumber of Hidden Layers\n",
      "For  many  problems,  you  can  just  begin  with  a  single  hidden  layer  and  you  will  get\n",
      "reasonable results. It has actually been shown that an MLP with just one hidden layer\n",
      "can model even the most complex functions provided it has enough neurons. For a\n",
      "long time, these facts convinced researchers that there was no need to investigate any\n",
      "deeper neural networks. But they overlooked the fact that deep networks have a much\n",
      "higher  parameter  efficiency  than  shallow  ones:  they  can  model  complex  functions\n",
      "using  exponentially  fewer  neurons  than  shallow  nets,  allowing  them  to  reach  much\n",
      "better performance with the same amount of training data.\n",
      "\n",
      "To understand why, suppose you are asked to draw a forest using some drawing soft‐\n",
      "ware,  but  you  are  forbidden  to  use  copy/paste.  You  would  have  to  draw  each  tree\n",
      "individually,  branch  per  branch,  leaf  per  leaf.  If  you  could  instead  draw  one  leaf,\n",
      "copy/paste  it  to  draw  a  branch,  then  copy/paste  that  branch  to  create  a  tree,  and\n",
      "finally copy/paste this tree to make a forest, you would be finished in no time. Real-\n",
      "world data is often structured in such a hierarchical way and Deep Neural Networks\n",
      "automatically take advantage of this fact: lower hidden layers model low-level struc‐\n",
      "tures  (e.g.,  line  segments  of  various  shapes  and  orientations),  intermediate  hidden\n",
      "layers combine these low-level structures to model intermediate-level structures (e.g.,\n",
      "squares,  circles),  and  the  highest  hidden  layers  and  the  output  layer  combine  these\n",
      "intermediate structures to model high-level structures (e.g., faces).\n",
      "\n",
      "Not only does this hierarchical architecture help DNNs converge faster to a good sol‐\n",
      "ution, it also improves their ability to generalize to new datasets. For example, if you\n",
      "have  already  trained  a  model  to  recognize  faces  in  pictures,  and  you  now  want  to\n",
      "train a new neural network to recognize hairstyles, then you can kickstart training by\n",
      "reusing  the  lower  layers  of  the  first  network.  Instead  of  randomly  initializing  the\n",
      "weights and biases of the first few layers of the new neural network, you can initialize\n",
      "them to the value of the weights and biases of the lower layers of the first network.\n",
      "This way the network will not have to learn from scratch all the low-level structures\n",
      "that occur in most pictures; it will only have to learn the higher-level structures (e.g.,\n",
      "hairstyles). This is called transfer learning.\n",
      "\n",
      "In summary, for many problems you can start with just one or two hidden layers and\n",
      "it  will  work  just  fine  (e.g.,  you  can  easily  reach  above  97%  accuracy  on  the  MNIST\n",
      "dataset using just one hidden layer with a few hundred neurons, and above 98% accu‐\n",
      "racy using two hidden layers with the same total amount of neurons, in roughly the\n",
      "same amount of training time). For more complex problems, you can gradually ramp\n",
      "up the number of hidden layers, until you start overfitting the training set. Very com‐\n",
      "plex tasks, such as large image classification or speech recognition, typically require\n",
      "networks with dozens of layers (or even hundreds, but not fully connected ones, as\n",
      "we will see in Chapter 14), and they need a huge amount of training data. However,\n",
      "you will rarely have to train such networks from scratch: it is much more common to\n",
      "\n",
      "Fine-Tuning Neural Network Hyperparameters \n",
      "\n",
      "| \n",
      "\n",
      "319\n",
      "\n",
      "\freuse  parts  of  a  pretrained  state-of-the-art  network  that  performs  a  similar  task.\n",
      "Training will be a lot faster and require much less data (we will discuss this in Chap‐\n",
      "ter 11).\n",
      "\n",
      "Number of Neurons per Hidden Layer\n",
      "Obviously the number of neurons in the input and output layers is determined by the\n",
      "type of input and output your task requires. For example, the MNIST task requires 28\n",
      "x 28 = 784 input neurons and 10 output neurons.\n",
      "\n",
      "As for the hidden layers, it used to be a common practice to size them to form a pyra‐\n",
      "mid, with fewer and fewer neurons at each layer—the rationale being that many low-\n",
      "level  features  can  coalesce  into  far  fewer  high-level  features.  For  example,  a  typical\n",
      "neural network for MNIST may have three hidden layers, the first with 300 neurons,\n",
      "the second with 200, and the third with 100. However, this practice has been largely\n",
      "abandoned now, as it seems that simply using the same number of neurons in all hid‐\n",
      "den  layers  performs  just  as  well  in  most  cases,  or  even  better,  and  there  is  just  one\n",
      "hyperparameter to tune instead of one per layer—for example, all hidden layers could\n",
      "simply have 150 neurons. However, depending on the dataset, it can sometimes help\n",
      "to make the first hidden layer bigger than the others.\n",
      "\n",
      "Just like for the number of layers, you can try increasing the number of neurons grad‐\n",
      "ually  until  the  network  starts  overfitting.  In  general  you  will  get  more  bang  for  the\n",
      "buck  by  increasing  the  number  of  layers  than  the  number  of  neurons  per  layer.\n",
      "Unfortunately, as you can see, finding the perfect amount of neurons is still somewhat\n",
      "of a dark art.\n",
      "\n",
      "A  simpler  approach  is  to  pick  a  model  with  more  layers  and  neurons  than  you\n",
      "actually need, then use early stopping to prevent it from overfitting (and other regu‐\n",
      "larization  techniques,  such  as  dropout,  as  we  will  see  in  Chapter  11).  This  has  been\n",
      "dubbed the “stretch pants” approach:17 instead of wasting time looking for pants that\n",
      "perfectly  match  your  size,  just  use  large  stretch  pants  that  will  shrink  down  to  the\n",
      "right size.\n",
      "\n",
      "Learning Rate, Batch Size and Other Hyperparameters\n",
      "The number of hidden layers and neurons are not the only hyperparameters you can\n",
      "tweak in an MLP. Here are some of the most important ones, and some tips on how\n",
      "to set them:\n",
      "\n",
      "• The learning rate is arguably the most important hyperparameter. In general, the\n",
      "optimal learning rate is about half of the maximum learning rate (i.e., the learn‐\n",
      "\n",
      "17 By Vincent Vanhoucke in his Deep Learning class on Udacity.com.\n",
      "\n",
      "320 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
      "\n",
      "\fing rate above which the training algorithm diverges, as we saw in Chapter 4). So\n",
      "a simple approach for tuning the learning rate is to start with a large value that\n",
      "makes the training algorithm diverge, then divide this value by 3 and try again,\n",
      "and repeat until the training algorithm stops diverging. At that point, you gener‐\n",
      "ally  won’t  be  too  far  from  the  optimal  learning  rate.  That  said,  it  is  sometimes\n",
      "useful to reduce the learning rate during training: we will discuss this in Chap‐\n",
      "ter 11.\n",
      "\n",
      "• Choosing  a  better  optimizer  than  plain  old  Mini-batch  Gradient  Descent  (and\n",
      "tuning its hyperparameters) is also quite important. We will discuss this in Chap‐\n",
      "ter 11.\n",
      "\n",
      "• The  batch  size  can  also  have  a  significant  impact  on  your  model’s  performance\n",
      "and the training time. In general the optimal batch size will be lower than 32 (in\n",
      "April  2018,  Yann  Lecun  even  tweeted  \"Friends  don’t  let  friends  use  mini-batches\n",
      "larger  than  32“).  A  small  batch  size  ensures  that  each  training  iteration  is  very\n",
      "fast, and although a large batch size will give a more precise estimate of the gradi‐\n",
      "ents,  in  practice  this  does  not  matter  much  since  the  optimization  landscape  is\n",
      "quite  complex  and  the  direction  of  the  true  gradients  do  not  point  precisely  in\n",
      "the direction of the optimum. However, having a batch size greater than 10 helps\n",
      "take advantage of hardware and software optimizations, in particular for matrix\n",
      "multiplications, so it will speed up training. Moreover, if you use Batch Normal‐\n",
      "ization (see Chapter 11), the batch size should not be too small (in general no less\n",
      "than 20).\n",
      "\n",
      "• We discussed the choice of the activation function earlier in this chapter: in gen‐\n",
      "eral, the ReLU activation function will be a good default for all hidden layers. For\n",
      "the output layer, it really depends on your task.\n",
      "\n",
      "• In  most  cases,  the  number  of  training  iterations  does  not  actually  need  to  be\n",
      "\n",
      "tweaked: just use early stopping instead.\n",
      "\n",
      "For more best practices, make sure to read Yoshua Bengio’s great 2012 paper18, which\n",
      "presents many practical recommendations for deep networks.\n",
      "\n",
      "This concludes this introduction to artificial neural networks and their implementa‐\n",
      "tion  with  Keras.  In  the  next  few  chapters,  we  will  discuss  techniques  to  train  very\n",
      "deep nets, we will see how to customize your models using TensorFlow’s lower-level\n",
      "API and how to load and preprocess data efficiently using the Data API, and we will\n",
      "dive into other popular neural network architectures: convolutional neural networks\n",
      "for image processing, recurrent neural networks for sequential data, autoencoders for\n",
      "\n",
      "18 “Practical recommendations for gradient-based training of deep architectures,” Yoshua Bengio (2012).\n",
      "\n",
      "Fine-Tuning Neural Network Hyperparameters \n",
      "\n",
      "| \n",
      "\n",
      "321\n",
      "\n",
      "\frepresentation  learning,  and  generative  adversarial  networks  to  model  and  generate\n",
      "data.19\n",
      "\n",
      "Exercises\n",
      "\n",
      "1. Visit the TensorFlow Playground at https://playground.tensorflow.org/\n",
      "\n",
      "• Layers and patterns: try training the default neural network by clicking the run\n",
      "button (top left). Notice how it quickly finds a good solution for the classifica‐\n",
      "tion task. Notice that the neurons in the first hidden layer have learned simple\n",
      "patterns,  while  the  neurons  in  the  second  hidden  layer  have  learned  to  com‐\n",
      "bine the simple patterns of the first hidden layer into more complex patterns.\n",
      "In general, the more layers, the more complex the patterns can be.\n",
      "\n",
      "• Activation function: try replacing the Tanh activation function with the ReLU\n",
      "activation function, and train the network again. Notice that it finds a solution\n",
      "even faster, but this time the boundaries are linear. This is due to the shape of\n",
      "the ReLU function.\n",
      "\n",
      "• Local minima: modify the network architecture to have just one hidden layer\n",
      "with three neurons. Train it multiple times (to reset the network weights, click\n",
      "the reset button next to the play button). Notice that the training time varies a\n",
      "lot, and sometimes it even gets stuck in a local minimum.\n",
      "\n",
      "• Too small: now remove one neuron to keep just 2. Notice that the neural net‐\n",
      "work  is  now  incapable  of  finding  a  good  solution,  even  if  you  try  multiple\n",
      "times.  The  model  has  too  few  parameters  and  it  systematically  underfits  the\n",
      "training set.\n",
      "\n",
      "• Large enough: next, set the number of neurons to 8 and train the network sev‐\n",
      "eral  times.  Notice  that  it  is  now  consistently  fast  and  never  gets  stuck.  This\n",
      "highlights  an  important  finding  in  neural  network  theory:  large  neural  net‐\n",
      "works  almost  never  get  stuck  in  local  minima,  and  even  when  they  do  these\n",
      "local optima are almost as good as the global optimum. However, they can still\n",
      "get stuck on long plateaus for a long time.\n",
      "\n",
      "• Deep net and vanishing gradients: now change the dataset to be the spiral (bot‐\n",
      "tom right dataset under “DATA”). Change the network architecture to have 4\n",
      "hidden layers with 8 neurons each. Notice that training takes much longer, and\n",
      "often gets stuck on plateaus for long periods of time. Also notice that the neu‐\n",
      "rons in the highest layers (i.e. on the right) tend to evolve faster than the neu‐\n",
      "rons in the lowest layers (i.e. on the left). This problem, called the “vanishing\n",
      "gradients”  problem,  can  be  alleviated  using  better  weight  initialization  and\n",
      "\n",
      "19 A few extra ANN architectures are presented in ???.\n",
      "\n",
      "322 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
      "\n",
      "\fother  techniques,  better  optimizers  (such  as  AdaGrad  or  Adam),  or  using\n",
      "Batch Normalization.\n",
      "\n",
      "• More: go ahead and play with the other parameters to get a feel of what they\n",
      "do. In fact, you should definitely play with this UI for at least one hour, it will\n",
      "grow your intuitions about neural networks significantly.\n",
      "\n",
      "2. Draw an ANN using the original artificial neurons (like the ones in Figure 10-3)\n",
      "that computes A ⊕ B (where ⊕ represents the XOR operation). Hint: A ⊕ B = (A\n",
      "∧ ¬ B) ∨ (¬ A ∧ B).\n",
      "\n",
      "3. Why is it generally preferable to use a Logistic Regression classifier rather than a\n",
      "classical Perceptron (i.e., a single layer of threshold logic units trained using the\n",
      "Perceptron  training  algorithm)?  How  can  you  tweak  a  Perceptron  to  make  it\n",
      "equivalent to a Logistic Regression classifier?\n",
      "\n",
      "4. Why  was  the  logistic  activation  function  a  key  ingredient  in  training  the  first\n",
      "\n",
      "MLPs?\n",
      "\n",
      "5. Name three popular activation functions. Can you draw them?\n",
      "\n",
      "6. Suppose  you  have  an  MLP  composed  of  one  input  layer  with  10  passthrough\n",
      "neurons, followed by one hidden layer with 50 artificial neurons, and finally one\n",
      "output layer with 3 artificial neurons. All artificial neurons use the ReLU activa‐\n",
      "tion function.\n",
      "\n",
      "• What is the shape of the input matrix X?\n",
      "\n",
      "• What about the shape of the hidden layer’s weight vector Wh, and the shape of\n",
      "\n",
      "its bias vector bh?\n",
      "\n",
      "• What is the shape of the output layer’s weight vector Wo, and its bias vector bo?\n",
      "• What is the shape of the network’s output matrix Y?\n",
      "\n",
      "• Write the equation that computes the network’s output matrix Y as a function\n",
      "\n",
      "of X, Wh, bh, Wo and bo.\n",
      "\n",
      "7. How many neurons do you need in the output layer if you want to classify email\n",
      "into spam or ham? What activation function should you use in the output layer?\n",
      "If instead you want to tackle MNIST, how many neurons do you need in the out‐\n",
      "put layer, using what activation function? Answer the same questions for getting\n",
      "your network to predict housing prices as in Chapter 2.\n",
      "\n",
      "8. What is backpropagation and how does it work? What is the difference between\n",
      "\n",
      "backpropagation and reverse-mode autodiff?\n",
      "\n",
      "9. Can you list all the hyperparameters you can tweak in an MLP? If the MLP over‐\n",
      "fits the training data, how could you tweak these hyperparameters to try to solve\n",
      "the problem?\n",
      "\n",
      "Exercises \n",
      "\n",
      "| \n",
      "\n",
      "323\n",
      "\n",
      "\f10. Train a deep MLP on the MNIST dataset and see if you can get over 98% preci‐\n",
      "sion. Try adding all the bells and whistles (i.e., save checkpoints, use early stop‐\n",
      "ping, plot learning curves using TensorBoard, and so on).\n",
      "\n",
      "Solutions to these exercises are available in ???.\n",
      "\n",
      "324 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
      "\n",
      "\fCHAPTER 11\n",
      "Training Deep Neural Networks\n",
      "\n",
      "With  Early  Release  ebooks,  you  get  books  in  their  earliest  form—\n",
      "the author’s raw and unedited content as he or she writes—so you\n",
      "can  take  advantage  of  these  technologies  long  before  the  official\n",
      "release of these titles. The following will be Chapter 11 in the final\n",
      "release of the book.\n",
      "\n",
      "In  Chapter  10  we  introduced  artificial  neural  networks  and  trained  our  first  deep\n",
      "neural networks. But they were very shallow nets, with just a few hidden layers. What\n",
      "if you need to tackle a very complex problem, such as detecting hundreds of types of\n",
      "objects in high-resolution images? You may need to train a much deeper DNN, per‐\n",
      "haps with 10 layers or much more, each containing hundreds of neurons, connected\n",
      "by hundreds of thousands of connections. This would not be a walk in the park:\n",
      "\n",
      "• First,  you  would  be  faced  with  the  tricky  vanishing  gradients  problem  (or  the\n",
      "related exploding gradients problem) that affects deep neural networks and makes\n",
      "lower layers very hard to train.\n",
      "\n",
      "• Second, you might not have enough training data for such a large network, or it\n",
      "\n",
      "might be too costly to label.\n",
      "\n",
      "• Third, training may be extremely slow.\n",
      "\n",
      "• Fourth, a model with millions of parameters would severely risk overfitting the\n",
      "training set, especially if there are not enough training instances, or they are too\n",
      "noisy.\n",
      "\n",
      "In this chapter, we will go through each of these problems in turn and present techni‐\n",
      "ques to solve them. We will start by explaining the vanishing gradients problem and\n",
      "exploring some of the most popular solutions to this problem. Next, we will look at\n",
      "transfer  learning  and  unsupervised  pretraining,  which  can  help  you  tackle  complex\n",
      "\n",
      "325\n",
      "\n",
      "\ftasks even when you have little labeled data. Then we will discuss various optimizers\n",
      "that  can  speed  up  training  large  models  tremendously  compared  to  plain  Gradient\n",
      "Descent. Finally, we will go through a few popular regularization techniques for large\n",
      "neural networks.\n",
      "\n",
      "With these tools, you will be able to train very deep nets: welcome to Deep Learning!\n",
      "\n",
      "Vanishing/Exploding Gradients Problems\n",
      "As we discussed in Chapter 10, the backpropagation algorithm works by going from\n",
      "the output layer to the input layer, propagating the error gradient on the way. Once\n",
      "the  algorithm  has  computed  the  gradient  of  the  cost  function  with  regards  to  each\n",
      "parameter  in  the  network,  it  uses  these  gradients  to  update  each  parameter  with  a\n",
      "Gradient Descent step.\n",
      "\n",
      "Unfortunately,  gradients  often  get  smaller  and  smaller  as  the  algorithm  progresses\n",
      "down to the lower layers. As a result, the Gradient Descent update leaves the lower\n",
      "layer connection weights virtually unchanged, and training never converges to a good\n",
      "solution. This is called the vanishing gradients problem. In some cases, the opposite\n",
      "can  happen:  the  gradients  can  grow  bigger  and  bigger,  so  many  layers  get  insanely\n",
      "large weight updates and the algorithm diverges. This is the exploding gradients prob‐\n",
      "lem, which is mostly encountered in recurrent neural networks (see ???). More gener‐\n",
      "ally, deep neural networks suffer from unstable gradients; different layers may learn at\n",
      "widely different speeds.\n",
      "\n",
      "Although this unfortunate behavior has been empirically observed for quite a while\n",
      "(it  was  one  of  the  reasons  why  deep  neural  networks  were  mostly  abandoned  for  a\n",
      "long time), it is only around 2010 that significant progress was made in understand‐\n",
      "ing  it.  A  paper  titled  “Understanding  the  Difficulty  of  Training  Deep  Feedforward\n",
      "Neural Networks” by Xavier Glorot and Yoshua Bengio1 found a few suspects, includ‐\n",
      "ing  the  combination  of  the  popular  logistic  sigmoid  activation  function  and  the\n",
      "weight initialization technique that was most popular at the time, namely random ini‐\n",
      "tialization using a normal distribution with a mean of 0 and a standard deviation of 1.\n",
      "In short, they showed that with this activation function and this initialization scheme,\n",
      "the  variance  of  the  outputs  of  each  layer  is  much  greater  than  the  variance  of  its\n",
      "inputs. Going forward in the network, the variance keeps increasing after each layer\n",
      "until the activation function saturates at the top layers. This is actually made worse by\n",
      "the  fact  that  the  logistic  function  has  a  mean  of  0.5,  not  0  (the  hyperbolic  tangent\n",
      "function has a mean of 0 and behaves slightly better than the logistic function in deep\n",
      "networks).\n",
      "\n",
      "1 “Understanding the Difficulty of Training Deep Feedforward Neural Networks,” X. Glorot, Y Bengio (2010).\n",
      "\n",
      "326 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "\fLooking  at  the  logistic  activation  function  (see  Figure  11-1),  you  can  see  that  when\n",
      "inputs  become  large  (negative  or  positive),  the  function  saturates  at  0  or  1,  with  a\n",
      "derivative extremely close to 0. Thus when backpropagation kicks in, it has virtually\n",
      "no  gradient  to  propagate  back  through  the  network,  and  what  little  gradient  exists\n",
      "keeps getting diluted as backpropagation progresses down through the top layers, so\n",
      "there is really nothing left for the lower layers.\n",
      "\n",
      "Figure 11-1. Logistic activation function saturation\n",
      "\n",
      "Glorot and He Initialization\n",
      "In their paper, Glorot and Bengio propose a way to significantly alleviate this prob‐\n",
      "lem. We need the signal to flow properly in both directions: in the forward direction\n",
      "when making predictions, and in the reverse direction when backpropagating gradi‐\n",
      "ents. We don’t want the signal to die out, nor do we want it to explode and saturate.\n",
      "For  the  signal  to  flow  properly,  the  authors  argue  that  we  need  the  variance  of  the\n",
      "outputs of each layer to be equal to the variance of its inputs,2 and we also need the\n",
      "gradients  to  have  equal  variance  before  and  after  flowing  through  a  layer  in  the\n",
      "reverse direction (please check out the paper if you are interested in the mathematical\n",
      "details).  It  is  actually  not  possible  to  guarantee  both  unless  the  layer  has  an  equal\n",
      "number of inputs and neurons (these numbers are called the fan-in and fan-out of the\n",
      "layer),  but  they  proposed  a  good  compromise  that  has  proven  to  work  very  well  in\n",
      "practice:  the  connection  weights  of  each  layer  must  be  initialized  randomly  as\n",
      "\n",
      "2 Here’s an analogy: if you set a microphone amplifier’s knob too close to zero, people won’t hear your voice, but\n",
      "if you set it too close to the max, your voice will be saturated and people won’t understand what you are say‐\n",
      "ing. Now imagine a chain of such amplifiers: they all need to be set properly in order for your voice to come\n",
      "out loud and clear at the end of the chain. Your voice has to come out of each amplifier at the same amplitude\n",
      "as it came in.\n",
      "\n",
      "Vanishing/Exploding Gradients Problems \n",
      "\n",
      "| \n",
      "\n",
      "327\n",
      "\n",
      "\fdescribed  in  Equation  11-1,  where  f anavg = f anin + f anout /2.  This  initialization\n",
      "strategy is called Xavier initialization (after the author’s first name) or Glorot initiali‐\n",
      "zation (after his last name).\n",
      "\n",
      "Equation 11-1. Glorot initialization (when using the logistic activation function)\n",
      "\n",
      "Normal distribution with mean 0 and variance σ2 =\n",
      "\n",
      "1\n",
      "fan\n",
      "\n",
      "avg\n",
      "\n",
      "Or a uniform distribution between −r and  + r, with r =\n",
      "\n",
      "3\n",
      "fan\n",
      "\n",
      "avg\n",
      "\n",
      "If you just replace fanavg with fanin in Equation 11-1, you get an initialization strategy\n",
      "that was actually already proposed by Yann LeCun in the 1990s, called LeCun initiali‐\n",
      "zation, which was even recommended in the 1998 book Neural Networks: Tricks of the\n",
      "Trade  by  Genevieve  Orr  and  Klaus-Robert  Müller  (Springer).  It  is  equivalent  to\n",
      "Glorot initialization when fanin = fanout. It took over a decade for researchers to realize\n",
      "just how important this trick really is. Using Glorot initialization can speed up train‐\n",
      "ing  considerably,  and  it  is  one  of  the  tricks  that  led  to  the  current  success  of  Deep\n",
      "Learning.\n",
      "\n",
      "Some  papers3  have  provided  similar  strategies  for  different  activation  functions.\n",
      "These strategies differ only by the scale of the variance and whether they use fanavg or\n",
      "fanin, as shown in Table 11-1 (for the uniform distribution, just compute r = 3σ2).\n",
      "The initialization strategy for the ReLU activation function (and its variants, includ‐\n",
      "ing the ELU activation described shortly) is sometimes called He initialization (after\n",
      "the last name of its author). The SELU activation function will be explained later in\n",
      "this  chapter.  It  should  be  used  with  LeCun  initialization  (preferably  with  a  normal\n",
      "distribution, as we will see).\n",
      "\n",
      "Table 11-1. Initialization parameters for each type of activation function\n",
      "\n",
      "Initialization Activation functions\n",
      "Glorot\n",
      "\n",
      "None, Tanh, Logistic, Softmax\n",
      "\n",
      "He\n",
      "\n",
      "LeCun\n",
      "\n",
      "ReLU & variants\n",
      "\n",
      "SELU\n",
      "\n",
      "σ² (Normal)\n",
      "1 / fanavg\n",
      "2 / fanin\n",
      "1 / fanin\n",
      "\n",
      "By  default,  Keras  uses  Glorot  initialization  with  a  uniform  distribution.  You  can\n",
      "change this to He initialization by setting kernel_initializer=\"he_uniform\" or ker\n",
      "nel_initializer=\"he_normal\" when creating a layer, like this:\n",
      "\n",
      "3 Such as “Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,” K.\n",
      "\n",
      "He et al. (2015).\n",
      "\n",
      "328 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "\fkeras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
      "\n",
      "If you want He initialization with a uniform distribution, but based on fanavg rather\n",
      "than fanin, you can use the VarianceScaling initializer like this:\n",
      "\n",
      "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg',\n",
      "                                                 distribution='uniform')\n",
      "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)\n",
      "\n",
      "Nonsaturating Activation Functions\n",
      "One of the insights in the 2010 paper by Glorot and Bengio was that the vanishing/\n",
      "exploding  gradients  problems  were  in  part  due  to  a  poor  choice  of  activation  func‐\n",
      "tion.  Until  then  most  people  had  assumed  that  if  Mother  Nature  had  chosen  to  use\n",
      "roughly sigmoid activation functions in biological neurons, they must be an excellent\n",
      "choice.  But  it  turns  out  that  other  activation  functions  behave  much  better  in  deep\n",
      "neural  networks,  in  particular  the  ReLU  activation  function,  mostly  because  it  does\n",
      "not saturate for positive values (and also because it is quite fast to compute).\n",
      "\n",
      "Unfortunately, the ReLU activation function is not perfect. It suffers from a problem\n",
      "known  as  the  dying  ReLUs:  during  training,  some  neurons  effectively  die,  meaning\n",
      "they stop outputting anything other than 0. In some cases, you may find that half of\n",
      "your network’s neurons are dead, especially if you used a large learning rate. A neu‐\n",
      "ron  dies  when  its  weights  get  tweaked  in  such  a  way  that  the  weighted  sum  of  its\n",
      "inputs  are  negative  for  all  instances  in  the  training  set.  When  this  happens,  it  just\n",
      "keeps outputting 0s, and gradient descent does not affect it anymore since the gradi‐\n",
      "ent of the ReLU function is 0 when its input is negative.4\n",
      "\n",
      "To solve this problem, you may want to use a variant of the ReLU function, such as\n",
      "the  leaky  ReLU.  This  function  is  defined  as  LeakyReLUα(z)  =  max(αz,  z)  (see\n",
      "Figure 11-2). The hyperparameter α defines how much the function “leaks”: it is the\n",
      "slope of the function for z < 0, and is typically set to 0.01. This small slope ensures\n",
      "that leaky ReLUs never die; they can go into a long coma, but they have a chance to\n",
      "eventually wake up. A 2015 paper5 compared several variants of the ReLU activation\n",
      "function and one of its conclusions was that the leaky variants always outperformed\n",
      "the  strict  ReLU  activation  function.  In  fact,  setting  α  =  0.2  (huge  leak)  seemed  to\n",
      "result  in  better  performance  than  α  =  0.01  (small  leak).  They  also  evaluated  the\n",
      "randomized leaky ReLU (RReLU), where α is picked randomly in a given range during\n",
      "training, and it is fixed to an average value during testing. It also performed fairly well\n",
      "and seemed to act as a regularizer (reducing the risk of overfitting the training set).\n",
      "\n",
      "4 Unless it is part of the first hidden layer, a dead neuron may sometimes come back to life: gradient descent\n",
      "may indeed tweak neurons in the layers below in such a way that the weighted sum of the dead neuron’s\n",
      "inputs is positive again.\n",
      "\n",
      "5 “Empirical Evaluation of Rectified Activations in Convolution Network,” B. Xu et al. (2015).\n",
      "\n",
      "Vanishing/Exploding Gradients Problems \n",
      "\n",
      "| \n",
      "\n",
      "329\n",
      "\n",
      "\fFinally, they also evaluated the parametric leaky ReLU (PReLU), where α is authorized\n",
      "to  be  learned  during  training  (instead  of  being  a  hyperparameter,  it  becomes  a\n",
      "parameter that can be modified by backpropagation like any other parameter). This\n",
      "was  reported  to  strongly  outperform  ReLU  on  large  image  datasets,  but  on  smaller\n",
      "datasets it runs the risk of overfitting the training set.\n",
      "\n",
      "Figure 11-2. Leaky ReLU\n",
      "\n",
      "Last but not least, a 2015 paper by Djork-Arné Clevert et al.6 proposed a new activa‐\n",
      "tion function called the exponential linear unit (ELU) that outperformed all the ReLU\n",
      "variants in their experiments: training time was reduced and the neural network per‐\n",
      "formed  better  on  the  test  set.  It  is  represented  in  Figure  11-3,  and  Equation  11-2\n",
      "shows its definition.\n",
      "\n",
      "Equation 11-2. ELU activation function\n",
      "\n",
      "ELUα z =\n",
      "\n",
      "α exp z − 1 if z < 0\n",
      "if z ≥ 0\n",
      "z\n",
      "\n",
      "6 “Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs),” D. Clevert, T. Unterthiner,\n",
      "\n",
      "S. Hochreiter (2015).\n",
      "\n",
      "330 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "\fFigure 11-3. ELU activation function\n",
      "\n",
      "It looks a lot like the ReLU function, with a few major differences:\n",
      "\n",
      "• First  it  takes  on  negative  values  when  z  <  0,  which  allows  the  unit  to  have  an\n",
      "average output closer to 0. This helps alleviate the vanishing gradients problem,\n",
      "as discussed earlier. The hyperparameter α defines the value that the ELU func‐\n",
      "tion approaches when z is a large negative number. It is usually set to 1, but you\n",
      "can tweak it like any other hyperparameter if you want.\n",
      "\n",
      "• Second, it has a nonzero gradient for z < 0, which avoids the dead neurons prob‐\n",
      "\n",
      "lem.\n",
      "\n",
      "• Third,  if  α  is  equal  to  1  then  the  function  is  smooth  everywhere,  including\n",
      "around z = 0, which helps speed up Gradient Descent, since it does not bounce as\n",
      "much left and right of z = 0.\n",
      "\n",
      "The  main  drawback  of  the  ELU  activation  function  is  that  it  is  slower  to  compute\n",
      "than the ReLU and its variants (due to the use of the exponential function), but dur‐\n",
      "ing training this is compensated by the faster convergence rate. However, at test time\n",
      "an ELU network will be slower than a ReLU network.\n",
      "\n",
      "Moreover,  in  a  2017  paper7  by  Günter  Klambauer  et  al.,  called  “Self-Normalizing\n",
      "Neural Networks”, the authors showed that if you build a neural network composed\n",
      "exclusively of a stack of dense layers, and if all hidden layers use the SELU activation\n",
      "function  (which  is  just  a  scaled  version  of  the  ELU  activation  function,  as  its  name\n",
      "suggests), then the network will self-normalize: the output of each layer will tend to\n",
      "preserve mean 0 and standard deviation 1 during training, which solves the vanish‐\n",
      "ing/exploding  gradients  problem.  As  a  result,  this  activation  function  often  outper‐\n",
      "\n",
      "7 “Self-Normalizing Neural Networks, \" G. Klambauer, T. Unterthiner and A. Mayr (2017).\n",
      "\n",
      "Vanishing/Exploding Gradients Problems \n",
      "\n",
      "| \n",
      "\n",
      "331\n",
      "\n",
      "\fforms  other  activation  functions  very  significantly  for  such  neural  nets  (especially\n",
      "deep ones). However, there are a few conditions for self-normalization to happen:\n",
      "\n",
      "• The input features must be standardized (mean 0 and standard deviation 1).\n",
      "\n",
      "• Every hidden layer’s weights must also be initialized using LeCun normal initiali‐\n",
      "\n",
      "zation. In Keras, this means setting kernel_initializer=\"lecun_normal\".\n",
      "\n",
      "• The  network’s  architecture  must  be  sequential.  Unfortunately,  if  you  try  to  use\n",
      "SELU  in  non-sequential  architectures,  such  as  recurrent  networks  (see  ???)  or\n",
      "networks with skip connections (i.e., connections that skip layers, such as in wide\n",
      "& deep nets), self-normalization will not be guaranteed, so SELU will not neces‐\n",
      "sarily outperform other activation functions.\n",
      "\n",
      "• The paper only guarantees self-normalization if all layers are dense. However, in\n",
      "practice  the  SELU  activation  function  seems  to  work  great  with  convolutional\n",
      "neural nets as well (see Chapter 14).\n",
      "\n",
      "So which activation function should you use for the hidden layers\n",
      "of your deep neural networks? Although your mileage will vary, in\n",
      "general SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh\n",
      ">  logistic.  If  the  network’s  architecture  prevents  it  from  self-\n",
      "normalizing, then ELU may perform better than SELU (since SELU\n",
      "is not smooth at z = 0). If you care a lot about runtime latency, then\n",
      "you may prefer leaky ReLU. If you don’t want to tweak yet another\n",
      "hyperparameter,  you  may  just  use  the  default  α  values  used  by\n",
      "Keras  (e.g.,  0.3  for  the  leaky  ReLU).  If  you  have  spare  time  and\n",
      "computing  power,  you  can  use  cross-validation  to  evaluate  other\n",
      "activation functions, in particular RReLU if your network is over‐\n",
      "fitting, or PReLU if you have a huge training set.\n",
      "\n",
      "To use the leaky ReLU activation function, you must create a LeakyReLU instance like\n",
      "this:\n",
      "\n",
      "leaky_relu = keras.layers.LeakyReLU(alpha=0.2)\n",
      "layer = keras.layers.Dense(10, activation=leaky_relu,\n",
      "                           kernel_initializer=\"he_normal\")\n",
      "\n",
      "For PReLU, just replace LeakyRelu(alpha=0.2) with PReLU(). There is currently no\n",
      "official implementation of RReLU in Keras, but you can fairly easily implement your\n",
      "own (see the exercises at the end of Chapter 12).\n",
      "\n",
      "For  SELU  activation, \n",
      "izer=\"lecun_normal\" when creating a layer:\n",
      "\n",
      "just \n",
      "\n",
      "set  activation=\"selu\"  and  kernel_initial\n",
      "\n",
      "layer = keras.layers.Dense(10, activation=\"selu\",\n",
      "                           kernel_initializer=\"lecun_normal\")\n",
      "\n",
      "332 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "\fBatch Normalization\n",
      "Although using He initialization along with ELU (or any variant of ReLU) can signifi‐\n",
      "cantly reduce the vanishing/exploding gradients problems at the beginning of train‐\n",
      "ing, it doesn’t guarantee that they won’t come back during training.\n",
      "\n",
      "In  a  2015  paper,8  Sergey  Ioffe  and  Christian  Szegedy  proposed  a  technique  called\n",
      "Batch  Normalization  (BN)  to  address  the  vanishing/exploding  gradients  problems.\n",
      "The technique consists of adding an operation in the model just before or after the\n",
      "activation function of each hidden layer, simply zero-centering and normalizing each\n",
      "input, then scaling and shifting the result using two new parameter vectors per layer:\n",
      "one for scaling, the other for shifting. In other words, this operation lets the model\n",
      "learn the optimal scale and mean of each of the layer’s inputs. In many cases, if you\n",
      "add  a  BN  layer  as  the  very  first  layer  of  your  neural  network,  you  do  not  need  to\n",
      "standardize your training set (e.g., using a StandardScaler): the BN layer will do it\n",
      "for you (well, approximately, since it only looks at one batch at a time, and it can also\n",
      "rescale and shift each input feature).\n",
      "\n",
      "In  order  to  zero-center  and  normalize  the  inputs,  the  algorithm  needs  to  estimate\n",
      "each input’s mean and standard deviation. It does so by evaluating the mean and stan‐\n",
      "dard  deviation  of  each  input  over  the  current  mini-batch  (hence  the  name  “Batch\n",
      "Normalization”). The whole operation is summarized in Equation 11-3.\n",
      "\n",
      "Equation 11-3. Batch Normalization algorithm\n",
      "\n",
      "μB =\n",
      "\n",
      "m\n",
      "B\n",
      "∑\n",
      "i = 1\n",
      "\n",
      "1\n",
      "mB\n",
      "\n",
      "x i\n",
      "\n",
      "2\n",
      "\n",
      "x i − μB\n",
      "\n",
      "σB\n",
      "\n",
      "2 =\n",
      "\n",
      "m\n",
      "B\n",
      "1\n",
      "∑\n",
      "mB\n",
      "i = 1\n",
      "x i − μB\n",
      "2 + \n",
      "σB\n",
      "z i = γ ⊗ x i + β\n",
      "\n",
      "x i =\n",
      "\n",
      "1 .\n",
      "\n",
      "2 .\n",
      "\n",
      "3 .\n",
      "\n",
      "4 .\n",
      "\n",
      "• μB is the vector of input means, evaluated over the whole mini-batch B (it con‐\n",
      "\n",
      "tains one mean per input).\n",
      "\n",
      "8 “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,” S. Ioffe\n",
      "\n",
      "and C. Szegedy (2015).\n",
      "\n",
      "Vanishing/Exploding Gradients Problems \n",
      "\n",
      "| \n",
      "\n",
      "333\n",
      "\n",
      "\f• σB is the vector of input standard deviations, also evaluated over the whole mini-\n",
      "\n",
      "batch (it contains one standard deviation per input).\n",
      "\n",
      "• mB is the number of instances in the mini-batch.\n",
      "• x(i) is the vector of zero-centered and normalized inputs for instance i.\n",
      "\n",
      "• γ is the output scale parameter vector for the layer (it contains one scale parame‐\n",
      "\n",
      "ter per input).\n",
      "\n",
      "• ⊗ represents element-wise multiplication (each input is multiplied by its corre‐\n",
      "\n",
      "sponding output scale parameter).\n",
      "\n",
      "• β is the output shift (offset) parameter vector for the layer (it contains one offset\n",
      "parameter per input). Each input is offset by its corresponding shift parameter.\n",
      "• ϵ  is  a  tiny  number  to  avoid  division  by  zero  (typically  10–5).  This  is  called  a\n",
      "\n",
      "smoothing term.\n",
      "\n",
      "• z(i)  is  the  output  of  the  BN  operation:  it  is  a  rescaled  and  shifted  version  of  the\n",
      "\n",
      "inputs.\n",
      "\n",
      "So  during  training,  BN  just  standardizes  its  inputs  then  rescales  and  offsets  them.\n",
      "Good!  What  about  at  test  time?  Well  it  is  not  that  simple.  Indeed,  we  may  need  to\n",
      "make predictions for individual instances rather than for batches of instances: in this\n",
      "case,  we  will  have  no  way  to  compute  each  input’s  mean  and  standard  deviation.\n",
      "Moreover, even if we do have a batch of instances, it may be too small, or the instan‐\n",
      "ces may not be independent and identically distributed (IID), so computing statistics\n",
      "over the batch instances would be unreliable (during training, the batches should not\n",
      "be too small, if possible more than 30 instances, and all instances should be IID, as we\n",
      "saw in Chapter 4). One solution could be to wait until the end of training, then run\n",
      "the whole training set through the neural network, and compute the mean and stan‐\n",
      "dard deviation of each input of the BN layer. These “final” input means and standard\n",
      "deviations can then be used instead of the batch input means and standard deviations\n",
      "when making predictions. However, it is often preferred to estimate these final statis‐\n",
      "tics during training using a moving average of the layer’s input means and standard\n",
      "deviations. To sum up, four parameter vectors are learned in each batch-normalized\n",
      "layer: γ (the ouput scale vector) and β (the output offset vector) are learned through\n",
      "regular backpropagation, and μ (the final input mean vector), and σ (the final input\n",
      "standard deviation vector) are estimated using an exponential moving average. Note\n",
      "that μ and σ are estimated during training, but they are not used at all during train‐\n",
      "ing, only after training (to replace the batch input means and standard deviations in\n",
      "Equation 11-3).\n",
      "\n",
      "The  authors  demonstrated  that  this  technique  considerably  improved  all  the  deep\n",
      "neural  networks  they  experimented  with,  leading  to  a  huge  improvement  in  the\n",
      "ImageNet  classification  task  (ImageNet  is  a  large  database  of  images  classified  into\n",
      "many classes and commonly used to evaluate computer vision systems). The vanish‐\n",
      "\n",
      "334 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "\fing gradients problem was strongly reduced, to the point that they could use saturat‐\n",
      "ing  activation  functions  such  as  the  tanh  and  even  the  logistic  activation  function.\n",
      "The  networks  were  also  much  less  sensitive  to  the  weight  initialization.  They  were\n",
      "able to use much larger learning rates, significantly speeding up the learning process.\n",
      "Specifically,  they  note  that  “Applied  to  a  state-of-the-art  image  classification  model,\n",
      "Batch Normalization achieves the same accuracy with 14 times fewer training steps,\n",
      "and  beats  the  original  model  by  a  significant  margin.  […]  Using  an  ensemble  of\n",
      "batch-normalized networks, we improve upon the best published result on ImageNet\n",
      "classification:  reaching  4.9%  top-5  validation  error  (and  4.8%  test  error),  exceeding\n",
      "the accuracy of human raters.” Finally, like a gift that keeps on giving, Batch Normal‐\n",
      "ization also acts like a regularizer, reducing the need for other regularization techni‐\n",
      "ques (such as dropout, described later in this chapter).\n",
      "\n",
      "Batch Normalization does, however, add some complexity to the model (although it\n",
      "can remove the need for normalizing the input data, as we discussed earlier). More‐\n",
      "over, there is a runtime penalty: the neural network makes slower predictions due to\n",
      "the  extra  computations  required  at  each  layer.  So  if  you  need  predictions  to  be\n",
      "lightning-fast, you may want to check how well plain ELU + He initialization perform\n",
      "before playing with Batch Normalization.\n",
      "\n",
      "You may find that training is rather slow, because each epoch takes\n",
      "much more time when you use batch normalization. However, this\n",
      "is  usually  counterbalanced  by  the  fact  that  convergence  is  much\n",
      "faster with BN, so it will take fewer epochs to reach the same per‐\n",
      "formance.  All  in  all,  wall  time  will  usually  be  smaller  (this  is  the\n",
      "time measured by the clock on your wall).\n",
      "\n",
      "Implementing Batch Normalization with Keras\n",
      "\n",
      "As with most things with Keras, implementing Batch Normalization is quite simple.\n",
      "Just  add  a  BatchNormalization  layer  before  or  after  each  hidden  layer’s  activation\n",
      "function, and optionally add a BN layer as well as the first layer in your model. For\n",
      "example, this model applies BN after every hidden layer and as the first layer in the\n",
      "model (after flattening the input images):\n",
      "\n",
      "model = keras.models.Sequential([\n",
      "    keras.layers.Flatten(input_shape=[28, 28]),\n",
      "    keras.layers.BatchNormalization(),\n",
      "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
      "    keras.layers.BatchNormalization(),\n",
      "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
      "    keras.layers.BatchNormalization(),\n",
      "    keras.layers.Dense(10, activation=\"softmax\")\n",
      "])\n",
      "\n",
      "Vanishing/Exploding Gradients Problems \n",
      "\n",
      "| \n",
      "\n",
      "335\n",
      "\n",
      "\fThat’s  all!  In  this  tiny  example  with  just  two  hidden  layers,  it’s  unlikely  that  Batch\n",
      "Normalization will have a very positive impact, but for deeper networks it can make a\n",
      "tremendous difference.\n",
      "\n",
      "Let’s zoom in a bit. If you display the model summary, you can see that each BN layer\n",
      "adds 4 parameters per input: γ, β, μ and σ (for example, the first BN layer adds 3136\n",
      "parameters, which is 4 times 784). The last two parameters, μ and σ, are the moving\n",
      "averages,  they  are  not  affected  by  backpropagation,  so  Keras  calls  them  “Non-\n",
      "trainable”9 (if you count the total number of BN parameters, 3136 + 1200 + 400, and\n",
      "divide  by  two,  you  get  2,368,  which  is  the  total  number  of  non-trainable  params  in\n",
      "this model).\n",
      "\n",
      ">>> model.summary()\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #\n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 784)               0\n",
      "_________________________________________________________________\n",
      "batch_normalization_v2 (Batc (None, 784)               3136\n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 300)               235500\n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_1 (Ba (None, 300)               1200\n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 100)               30100\n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_2 (Ba (None, 100)               400\n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 10)                1010\n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "\n",
      "Let’s look at the parameters of the first BN layer. Two are trainable (by backprop), and\n",
      "two are not:\n",
      "\n",
      ">>> [(var.name, var.trainable) for var in model.layers[1].variables]\n",
      "[('batch_normalization_v2/gamma:0', True),\n",
      " ('batch_normalization_v2/beta:0', True),\n",
      " ('batch_normalization_v2/moving_mean:0', False),\n",
      " ('batch_normalization_v2/moving_variance:0', False)]\n",
      "\n",
      "Now when you create a BN layer in Keras, it also creates two operations that will be\n",
      "called  by  Keras  at  each  iteration  during  training.  These  operations  will  update  the\n",
      "\n",
      "9 However, they are estimated during training, based on the training data, so arguably they are trainable. In\n",
      "\n",
      "Keras, “Non-trainable” really means “untouched by backpropagation”.\n",
      "\n",
      "336 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "\fmoving  averages.  Since  we  are  using  the  TensorFlow  backend,  these  operations  are\n",
      "TensorFlow operations (we will discuss TF operations in Chapter 12).\n",
      "\n",
      ">>> model.layers[1].updates\n",
      "[<tf.Operation 'cond_2/Identity' type=Identity>,\n",
      " <tf.Operation 'cond_3/Identity' type=Identity>]\n",
      "\n",
      "The authors of the BN paper argued in favor of adding the BN layers before the acti‐\n",
      "vation functions, rather than after (as we just did). There is some debate about this, as\n",
      "it seems to depend on the task. So that’s one more thing you can experiment with to\n",
      "see which option works best on your dataset. To add the BN layers before the activa‐\n",
      "tion functions, we must remove the activation function from the hidden layers, and\n",
      "add them as separate layers after the BN layers. Moreover, since a Batch Normaliza‐\n",
      "tion layer includes one offset parameter per input, you can remove the bias term from\n",
      "the previous layer (just pass use_bias=False when creating it):\n",
      "\n",
      "model = keras.models.Sequential([\n",
      "    keras.layers.Flatten(input_shape=[28, 28]),\n",
      "    keras.layers.BatchNormalization(),\n",
      "    keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
      "    keras.layers.BatchNormalization(),\n",
      "    keras.layers.Activation(\"elu\"),\n",
      "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
      "    keras.layers.Activation(\"elu\"),\n",
      "    keras.layers.BatchNormalization(),\n",
      "    keras.layers.Dense(10, activation=\"softmax\")\n",
      "])\n",
      "\n",
      "The BatchNormalization class has quite a few hyperparameters you can tweak. The\n",
      "defaults  will  usually  be  fine,  but  you  may  occasionally  need  to  tweak  the  momentum.\n",
      "This hyperparameter is used when updating the exponential moving averages: given a\n",
      "new value v (i.e., a new vector of input means or standard deviations computed over\n",
      "the current batch), the running average  is updated using the following equation:\n",
      "\n",
      "v\n",
      "\n",
      "v × momentum + v × 1 − momentum\n",
      "\n",
      "A good momentum value is typically close to 1—for example, 0.9, 0.99, or 0.999 (you\n",
      "want more 9s for larger datasets and smaller mini-batches).\n",
      "\n",
      "Another important hyperparameter is axis: it determines which axis should be nor‐\n",
      "malized. It defaults to –1, meaning that by default it will normalize the last axis (using\n",
      "the  means  and  standard  deviations  computed  across  the  other  axes).  For  example,\n",
      "when the input batch is 2D (i.e., the batch shape is [batch size, features]), this means\n",
      "that each input feature will be normalized based on the mean and standard deviation\n",
      "computed across all the instances in the batch. For example, the first BN layer in the\n",
      "previous code example will independently normalize (and rescale and shift) each of\n",
      "the  784  input  features.  However,  if  we  move  the  first  BN  layer  before  the  Flatten\n",
      "\n",
      "Vanishing/Exploding Gradients Problems \n",
      "\n",
      "| \n",
      "\n",
      "337\n",
      "\n",
      "\flayer, then the input batches will be 3D, with shape [batch size, height, width], there‐\n",
      "fore the BN layer will compute 28 means and 28 standard deviations (one per column\n",
      "of pixels, computed across all instances in the batch, and all rows in the column), and\n",
      "it will normalize all pixels in a given column using the same mean and standard devi‐\n",
      "ation. There will also be just 28 scale parameters and 28 shift parameters. If instead\n",
      "you  still  want  to  treat  each  of  the  784  pixels  independently,  then  you  should  set\n",
      "axis=[1, 2].\n",
      "\n",
      "Notice that the BN layer does not perform the same computation during training and\n",
      "after  training:  it  uses  batch  statistics  during  training,  and  the  “final”  statistics  after\n",
      "training (i.e., the final value of the moving averages). Let’s take a peek at the source\n",
      "code of this class to see how this is handled:\n",
      "\n",
      "class BatchNormalization(Layer):\n",
      "    [...]\n",
      "    def call(self, inputs, training=None):\n",
      "        if training is None:\n",
      "            training = keras.backend.learning_phase()\n",
      "        [...]\n",
      "\n",
      "The  call() method is the one that actually performs the computations, and as you\n",
      "can  see  it  has  an  extra  training  argument:  if  it  is  None  it  falls  back  to  keras.back\n",
      "end.learning_phase(), which returns 1 during training (the fit() method ensures\n",
      "that). Otherwise, it returns 0. If you ever need to write a custom layer, and it needs to\n",
      "behave differently during training and testing, simply use the same pattern (we will\n",
      "discuss custom layers in Chapter 12).\n",
      "\n",
      "Batch  Normalization  has  become  one  of  the  most  used  layers  in  deep  neural  net‐\n",
      "works, to the point that it is often omitted in the diagrams, as it is assumed that BN is\n",
      "added after every layer. However, a very recent paper10 by Hongyi Zhang et al. may\n",
      "well change this: the authors show that by using a novel fixed-update (fixup) weight\n",
      "initialization technique, they manage to train a very deep neural network (10,000 lay‐\n",
      "ers!) without BN, achieving state-of-the-art performance on complex image classifi‐\n",
      "cation tasks.\n",
      "\n",
      "Gradient Clipping\n",
      "Another  popular  technique  to  lessen  the  exploding  gradients  problem  is  to  simply\n",
      "clip the gradients during backpropagation so that they never exceed some threshold.\n",
      "This is called Gradient Clipping.11 This technique is most often used in recurrent neu‐\n",
      "\n",
      "10 “Fixup Initialization: Residual Learning Without Normalization,” Hongyi Zhang, Yann N. Dauphin, Tengyu\n",
      "\n",
      "Ma (2019).\n",
      "\n",
      "11 “On the difficulty of training recurrent neural networks,” R. Pascanu et al. (2013).\n",
      "\n",
      "338 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "\fral networks, as Batch Normalization is tricky to use in RNNs, as we will see in ???.\n",
      "For other types of networks, BN is usually sufficient.\n",
      "\n",
      "In Keras, implementing Gradient Clipping is just a matter of setting the clipvalue or\n",
      "clipnorm argument when creating an optimizer. For example:\n",
      "\n",
      "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
      "model.compile(loss=\"mse\", optimizer=optimizer)\n",
      "\n",
      "This will clip every component of the gradient vector to a value between –1.0 and 1.0.\n",
      "This means that all the partial derivatives of the loss (with regards to each and every\n",
      "trainable parameter) will be clipped between –1.0 and 1.0. The threshold is a hyper‐\n",
      "parameter you can tune. Note that it may change the orientation of the gradient vec‐\n",
      "tor: for example, if the original gradient vector is [0.9, 100.0], it points mostly in the\n",
      "direction  of  the  second  axis,  but  once  you  clip  it  by  value,  you  get  [0.9,  1.0],  which\n",
      "points  roughly  in  the  diagonal  between  the  two  axes.  In  practice  however,  this\n",
      "approach works well. If you want to ensure that Gradient Clipping does not change\n",
      "the  direction  of  the  gradient  vector,  you  should  clip  by  norm  by  setting  clipnorm\n",
      "instead of clipvalue. This will clip the whole gradient if its ℓ2 norm is greater than\n",
      "the threshold you picked. For example, if you set clipnorm=1.0, then the vector [0.9,\n",
      "100.0]  will  be  clipped  to  [0.00899964,  0.9999595],  preserving  its  orientation,  but\n",
      "almost  eliminating  the  first  component.  If  you  observe  that  the  gradients  explode\n",
      "during training (you can track the size of the gradients using TensorBoard), you may\n",
      "want  to  try  both  clipping  by  value  and  clipping  by  norm,  with  different  threshold,\n",
      "and see which option performs best on the validation set.\n",
      "\n",
      "Reusing Pretrained Layers\n",
      "It is generally not a good idea to train a very large DNN from scratch: instead, you\n",
      "should always try to find an existing neural network that accomplishes a similar task\n",
      "to the one you are trying to tackle (we will discuss how to find them in Chapter 14),\n",
      "then just reuse the lower layers of this network: this is called transfer learning. It will\n",
      "not only speed up training considerably, but will also require much less training data.\n",
      "\n",
      "For example, suppose that you have access to a DNN that was trained to classify pic‐\n",
      "tures into 100 different categories, including animals, plants, vehicles, and everyday\n",
      "objects.  You  now  want  to  train  a  DNN  to  classify  specific  types  of  vehicles.  These\n",
      "tasks are very similar, even partly overlapping, so you should try to reuse parts of the\n",
      "first network (see Figure 11-4).\n",
      "\n",
      "Reusing Pretrained Layers \n",
      "\n",
      "| \n",
      "\n",
      "339\n",
      "\n",
      "\fFigure 11-4. Reusing pretrained layers\n",
      "\n",
      "If  the  input  pictures  of  your  new  task  don’t  have  the  same  size  as\n",
      "the  ones  used  in  the  original  task,  you  will  usually  have  to  add  a\n",
      "preprocessing step to resize them to the size expected by the origi‐\n",
      "nal  model.  More  generally,  transfer  learning  will  work  best  when\n",
      "the inputs have similar low-level features.\n",
      "\n",
      "The  output  layer  of  the  original  model  should  usually  be  replaced  since  it  is  most\n",
      "likely not useful at all for the new task, and it may not even have the right number of\n",
      "outputs for the new task.\n",
      "\n",
      "Similarly, the upper hidden layers of the original model are less likely to be as useful\n",
      "as the lower layers, since the high-level features that are most useful for the new task\n",
      "may differ significantly from the ones that were most useful for the original task. You\n",
      "want to find the right number of layers to reuse.\n",
      "\n",
      "The more similar the tasks are, the more layers you want to reuse\n",
      "(starting with the lower layers). For very similar tasks, you can try\n",
      "keeping all the hidden layers and just replace the output layer.\n",
      "\n",
      "Try freezing all the reused layers first (i.e., make their weights non-trainable, so gradi‐\n",
      "ent  descent  won’t  modify  them),  then  train  your  model  and  see  how  it  performs.\n",
      "Then try unfreezing one or two of the top hidden layers to let backpropagation tweak\n",
      "them and see if performance improves. The more training data you have, the more\n",
      "\n",
      "340 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "\flayers you can unfreeze. It is also useful to reduce the learning rate when you unfreeze\n",
      "reused layers: this will avoid wrecking their fine-tuned weights.\n",
      "\n",
      "If you still cannot get good performance, and you have little training data, try drop‐\n",
      "ping  the  top  hidden  layer(s)  and  freeze  all  remaining  hidden  layers  again.  You  can\n",
      "iterate until you find the right number of layers to reuse. If you have plenty of train‐\n",
      "ing data, you may try replacing the top hidden layers instead of dropping them, and\n",
      "even add more hidden layers.\n",
      "\n",
      "Transfer Learning With Keras\n",
      "Let’s look at an example. Suppose the fashion MNIST dataset only contained 8 classes,\n",
      "for  example  all  classes  except  for  sandals  and  shirts.  Someone  built  and  trained  a\n",
      "Keras model on that set and got reasonably good performance (>90% accuracy). Let’s\n",
      "call this model A. You now want to tackle a different task: you have images of sandals\n",
      "and  shirts,  and  you  want  to  train  a  binary  classifier  (positive=shirts,  negative=san‐\n",
      "dals). However, your dataset is quite small, you only have 200 labeled images. When\n",
      "you train a new model for this task (let’s call it model B), with the same architecture\n",
      "as model A, it performs reasonably well (97.2% accuracy), but since it’s a much easier\n",
      "task (there are just 2 classes), you were hoping for more. While drinking your morn‐\n",
      "ing  coffee,  you  realize  that  your  task  is  quite  similar  to  task  A,  so  perhaps  transfer\n",
      "learning can help? Let’s find out!\n",
      "\n",
      "First, you need to load model A, and create a new model based on the model A’s lay‐\n",
      "ers. Let’s reuse all layers except for the output layer:\n",
      "\n",
      "model_A = keras.models.load_model(\"my_model_A.h5\")\n",
      "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
      "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
      "\n",
      "Note  that  model_A  and  model_B_on_A  now  share  some  layers.  When  you  train\n",
      "model_B_on_A, it will also affect model_A. If you want to avoid that, you need to clone\n",
      "model_A before you reuse its layers. To do this, you must clone model A’s architecture,\n",
      "then copy its weights (since clone_model() does not clone the weights):\n",
      "\n",
      "model_A_clone = keras.models.clone_model(model_A)\n",
      "model_A_clone.set_weights(model_A.get_weights())\n",
      "\n",
      "Now we could just train model_B_on_A for task B, but since the new output layer was\n",
      "initialized randomly, it will make large errors, at least during the first few epochs, so\n",
      "there will be large error gradients that may wreck the reused weights. To avoid this,\n",
      "one approach is to freeze the reused layers during the first few epochs, giving the new\n",
      "layer some time to learn reasonable weights. To do this, simply set every layer’s train\n",
      "able attribute to False and compile the model:\n",
      "\n",
      "for layer in model_B_on_A.layers[:-1]:\n",
      "    layer.trainable = False\n",
      "\n",
      "Reusing Pretrained Layers \n",
      "\n",
      "| \n",
      "\n",
      "341\n",
      "\n",
      "\fmodel_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\",\n",
      "                     metrics=[\"accuracy\"])\n",
      "\n",
      "You must always compile your model after you freeze or unfreeze\n",
      "layers.\n",
      "\n",
      "Next, we can train the model for a few epochs, then unfreeze the reused layers (which\n",
      "requires  compiling  the  model  again)  and  continue  training  to  fine-tune  the  reused\n",
      "layers for task B. After unfreezing the reused layers, it is usually a good idea to reduce\n",
      "the learning rate, once again to avoid damaging the reused weights:\n",
      "\n",
      "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
      "                           validation_data=(X_valid_B, y_valid_B))\n",
      "\n",
      "for layer in model_B_on_A.layers[:-1]:\n",
      "    layer.trainable = True\n",
      "\n",
      "optimizer = keras.optimizers.SGD(lr=1e-4) # the default lr is 1e-3\n",
      "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n",
      "                     metrics=[\"accuracy\"])\n",
      "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
      "                           validation_data=(X_valid_B, y_valid_B))\n",
      "\n",
      "So,  what’s  the  final  verdict?  Well  this  model’s  test  accuracy  is  99.25%,  which  means\n",
      "that transfer learning reduced the error rate from 2.8% down to almost 0.7%! That’s a\n",
      "factor of 4!\n",
      "\n",
      ">>> model_B_on_A.evaluate(X_test_B, y_test_B)\n",
      "[0.06887910133600235, 0.9925]\n",
      "\n",
      "Are you convinced? Well you shouldn’t be: I cheated! :) I tried many configurations\n",
      "until I found one that demonstrated a strong improvement. If you try to change the\n",
      "classes  or  the  random  seed,  you  will  see  that  the  improvement  generally  drops,  or\n",
      "even vanishes or reverses. What I did is called “torturing the data until it confesses”.\n",
      "When  a  paper  just  looks  too  positive,  you  should  be  suspicious:  perhaps  the  flashy\n",
      "new technique does not help much (in fact, it may even degrade performance), but\n",
      "the authors tried many variants and reported only the best results (which may be due\n",
      "to shear luck), without mentioning how many failures they encountered on the way.\n",
      "Most of the time, this is not malicious at all, but it is part of the reason why so many\n",
      "results in Science can never be reproduced.\n",
      "\n",
      "So why did I cheat? Well it turns out that transfer learning does not work very well\n",
      "with small dense networks: it works best with deep convolutional neural networks, so\n",
      "we  will  revisit  transfer  learning  in  Chapter  14,  using  the  same  techniques  (and  this\n",
      "time there will be no cheating, I promise!).\n",
      "\n",
      "342 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "\fUnsupervised Pretraining\n",
      "Suppose  you  want  to  tackle  a  complex  task  for  which  you  don’t  have  much  labeled\n",
      "training  data,  but  unfortunately  you  cannot  find  a  model  trained  on  a  similar  task.\n",
      "Don’t  lose  all  hope!  First,  you  should  of  course  try  to  gather  more  labeled  training\n",
      "data, but if this is too hard or too expensive, you may still be able to perform unsuper‐\n",
      "vised pretraining (see Figure 11-5). It is often rather cheap to gather unlabeled train‐\n",
      "ing examples, but quite expensive to label them. If you can gather plenty of unlabeled\n",
      "training data, you can try to train the layers one by one, starting with the lowest layer\n",
      "and then going up, using an unsupervised feature detector algorithm such as Restric‐\n",
      "ted  Boltzmann  Machines  (RBMs;  see  ???)  or  autoencoders  (see  ???).  Each  layer  is\n",
      "trained on the output of the previously trained layers (all layers except the one being\n",
      "trained are frozen). Once all layers have been trained this way, you can add the output\n",
      "layer  for  your  task,  and  fine-tune  the  final  network  using  supervised  learning  (i.e.,\n",
      "with the labeled training examples). At this point, you can unfreeze all the pretrained\n",
      "layers, or just some of the upper ones.\n",
      "\n",
      "Figure 11-5. Unsupervised pretraining\n",
      "\n",
      "This  is  a  rather  long  and  tedious  process,  but  it  often  works  well;  in  fact,  it  is  this\n",
      "technique  that  Geoffrey  Hinton  and  his  team  used  in  2006  and  which  led  to  the\n",
      "revival  of  neural  networks  and  the  success  of  Deep  Learning.  Until  2010,  unsuper‐\n",
      "vised pretraining (typically using RBMs) was the norm for deep nets, and it was only\n",
      "after the vanishing gradients problem was alleviated that it became much more com‐\n",
      "\n",
      "Reusing Pretrained Layers \n",
      "\n",
      "| \n",
      "\n",
      "343\n",
      "\n",
      "\fmon  to  train  DNNs  purely  using  supervised  learning.  However,  unsupervised  pre‐\n",
      "training (today typically using autoencoders rather than RBMs) is still a good option\n",
      "when  you  have  a  complex  task  to  solve,  no  similar  model  you  can  reuse,  and  little\n",
      "labeled training data but plenty of unlabeled training data.\n",
      "\n",
      "Pretraining on an Auxiliary Task\n",
      "If you do not have much labeled training data, one last option is to train a first neural\n",
      "network  on  an  auxiliary  task  for  which  you  can  easily  obtain  or  generate  labeled\n",
      "training  data,  then  reuse  the  lower  layers  of  that  network  for  your  actual  task.  The\n",
      "first neural network’s lower layers will learn feature detectors that will likely be reusa‐\n",
      "ble by the second neural network.\n",
      "\n",
      "For example, if you want to build a system to recognize faces, you may only have a\n",
      "few pictures of each individual—clearly not enough to train a good classifier. Gather‐\n",
      "ing hundreds of pictures of each person would not be practical. However, you could\n",
      "gather a lot of pictures of random people on the web and train a first neural network\n",
      "to detect whether or not two different pictures feature the same person. Such a net‐\n",
      "work would learn good feature detectors for faces, so reusing its lower layers would\n",
      "allow you to train a good face classifier using little training data.\n",
      "\n",
      "For natural language processing (NLP) applications, you can easily download millions\n",
      "of text documents and automatically generate labeled data from it. For example, you\n",
      "could randomly mask out some words and train a model to predict what the missing\n",
      "words  are  (e.g.,  it  should  predict  that  the  missing  word  in  the  sentence  “What  ___\n",
      "you saying?” is probably “are” or “were”). If you can train a model to reach good per‐\n",
      "formance on this task, then it will already know quite a lot about language, and you\n",
      "can certainly reuse it for your actual task, and fine-tune it on your labeled data (we\n",
      "will discuss more pretraining tasks in ???).\n",
      "\n",
      "Self-supervised  learning  is  when  you  automatically  generate  the\n",
      "labels from the data itself, then you train a model on the resulting\n",
      "“labeled”  dataset  using  supervised  learning  techniques.  Since  this\n",
      "approach  requires  no  human  labeling  whatsoever,  it  is  best  classi‐\n",
      "fied as a form of unsupervised learning.\n",
      "\n",
      "Faster Optimizers\n",
      "Training a very large deep neural network can be painfully slow. So far we have seen\n",
      "four ways to speed up training (and reach a better solution): applying a good initiali‐\n",
      "zation  strategy  for  the  connection  weights,  using  a  good  activation  function,  using\n",
      "Batch Normalization, and reusing parts of a pretrained network (possibly built on an\n",
      "auxiliary task or using unsupervised learning). Another huge speed boost comes from\n",
      "using a faster optimizer than the regular Gradient Descent optimizer. In this section\n",
      "\n",
      "344 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "\fwe will present the most popular ones: Momentum optimization, Nesterov Acceler‐\n",
      "ated Gradient, AdaGrad, RMSProp, and finally Adam and Nadam optimization.\n",
      "\n",
      "Momentum Optimization\n",
      "Imagine a bowling ball rolling down a gentle slope on a smooth surface: it will start\n",
      "out slowly, but it will quickly pick up momentum until it eventually reaches terminal\n",
      "velocity (if there is some friction or air resistance). This is the very simple idea behind\n",
      "Momentum  optimization,  proposed  by  Boris  Polyak  in  1964.12  In  contrast,  regular\n",
      "Gradient Descent will simply take small regular steps down the slope, so it will take\n",
      "much more time to reach the bottom.\n",
      "\n",
      "Recall that Gradient Descent simply updates the weights θ by directly subtracting the\n",
      "gradient of the cost function J(θ) with regards to the weights (∇θJ(θ)) multiplied by\n",
      "the learning rate η. The equation is: θ ← θ – η∇θJ(θ). It does not care about what the\n",
      "earlier gradients were. If the local gradient is tiny, it goes very slowly.\n",
      "\n",
      "Momentum  optimization  cares  a  great  deal  about  what  previous  gradients  were:  at\n",
      "each  iteration,  it  subtracts  the  local  gradient  from  the  momentum  vector  m  (multi‐\n",
      "plied  by  the  learning  rate  η),  and  it  updates  the  weights  by  simply  adding  this\n",
      "momentum vector (see Equation 11-4). In other words, the gradient is used for accel‐\n",
      "eration, not for speed. To simulate some sort of friction mechanism and prevent the\n",
      "momentum from growing too large, the algorithm introduces a new hyperparameter\n",
      "β,  simply  called  the  momentum,  which  must  be  set  between  0  (high  friction)  and  1\n",
      "(no friction). A typical momentum value is 0.9.\n",
      "\n",
      "Equation 11-4. Momentum algorithm\n",
      "1 . m βm − η∇θJ θ\n",
      "θ + m\n",
      "2 .\n",
      "\n",
      "θ\n",
      "\n",
      "You can easily verify that if the gradient remains constant, the terminal velocity (i.e.,\n",
      "the maximum size of the weight updates) is equal to that gradient multiplied by the\n",
      "learning rate η multiplied by  1\n",
      "1 − β  (ignoring the sign). For example, if β = 0.9, then the\n",
      "terminal velocity is equal to 10 times the gradient times the learning rate, so Momen‐\n",
      "tum optimization ends up going 10 times faster than Gradient Descent! This allows\n",
      "Momentum optimization to escape from plateaus much faster than Gradient Descent.\n",
      "In particular, we saw in Chapter 4 that when the inputs have very different scales the \n",
      "cost function will look like an elongated bowl (see Figure 4-7). Gradient Descent goes\n",
      "down the steep slope quite fast, but then it takes a very long time to go down the val‐\n",
      "\n",
      "12 “Some methods of speeding up the convergence of iteration methods,” B. Polyak (1964).\n",
      "\n",
      "Faster Optimizers \n",
      "\n",
      "| \n",
      "\n",
      "345\n",
      "\n",
      "\fley.  In  contrast,  Momentum  optimization  will  roll  down  the  valley  faster  and  faster\n",
      "until  it  reaches  the  bottom  (the  optimum).  In  deep  neural  networks  that  don’t  use\n",
      "Batch Normalization, the upper layers will often end up having inputs with very dif‐\n",
      "ferent scales, so using Momentum optimization helps a lot. It can also help roll past\n",
      "local optima.\n",
      "\n",
      "Due  to  the  momentum,  the  optimizer  may  overshoot  a  bit,  then\n",
      "come  back,  overshoot  again,  and  oscillate  like  this  many  times\n",
      "before stabilizing at the minimum. This is one of the reasons why it\n",
      "is  good  to  have  a  bit  of  friction  in  the  system:  it  gets  rid  of  these\n",
      "oscillations and thus speeds up convergence.\n",
      "\n",
      "Implementing  Momentum  optimization  in  Keras  is  a  no-brainer:  just  use  the  SGD\n",
      "optimizer and set its momentum hyperparameter, then lie back and profit!\n",
      "\n",
      "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)\n",
      "\n",
      "The one drawback of Momentum optimization is that it adds yet another hyperpara‐\n",
      "meter  to  tune.  However,  the  momentum  value  of  0.9  usually  works  well  in  practice\n",
      "and almost always goes faster than regular Gradient Descent.\n",
      "\n",
      "Nesterov Accelerated Gradient\n",
      "One small variant to Momentum optimization, proposed by Yurii Nesterov in 1983,13\n",
      "is  almost  always  faster  than  vanilla  Momentum  optimization.  The  idea  of  Nesterov\n",
      "Momentum optimization, or Nesterov Accelerated Gradient (NAG), is to measure the\n",
      "gradient of the cost function not at the local position but slightly ahead in the direc‐\n",
      "tion  of  the  momentum  (see  Equation  11-5).  The  only  difference  from  vanilla\n",
      "Momentum optimization is that the gradient is measured at θ + βm rather than at θ.\n",
      "\n",
      "Equation 11-5. Nesterov Accelerated Gradient algorithm\n",
      "1 . m βm − η∇θJ θ + βm\n",
      "θ + m\n",
      "2 .\n",
      "\n",
      "θ\n",
      "\n",
      "This small tweak works because in general the momentum vector will be pointing in\n",
      "the right direction (i.e., toward the optimum), so it will be slightly more accurate to\n",
      "use the gradient measured a bit farther in that direction rather than using the gradi‐\n",
      "ent  at  the  original  position,  as  you  can  see  in  Figure  11-6  (where  ∇1  represents  the\n",
      "gradient of the cost function measured at the starting point θ, and ∇2 represents the\n",
      "\n",
      "13 “A Method for Unconstrained Convex Minimization Problem with the Rate of Convergence O(1/k2),” Yurii\n",
      "\n",
      "Nesterov (1983).\n",
      "\n",
      "346 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "\fgradient at the point located at θ + βm). As you can see, the Nesterov update ends up\n",
      "slightly closer to the optimum. After a while, these small improvements add up and\n",
      "NAG ends up being significantly faster than regular Momentum optimization. More‐\n",
      "over, note that when the momentum pushes the weights across a valley, ∇1 continues\n",
      "to push further across the valley, while ∇2 pushes back toward the bottom of the val‐\n",
      "ley. This helps reduce oscillations and thus converges faster.\n",
      "\n",
      "NAG will almost always speed up training compared to regular Momentum optimi‐\n",
      "zation. To use it, simply set nesterov=True when creating the SGD optimizer:\n",
      "\n",
      "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)\n",
      "\n",
      "Figure 11-6. Regular versus Nesterov Momentum optimization\n",
      "\n",
      "AdaGrad\n",
      "Consider the elongated bowl problem again: Gradient Descent starts by quickly going\n",
      "down the steepest slope, then slowly goes down the bottom of the valley. It would be\n",
      "nice if the algorithm could detect this early on and correct its direction to point a bit\n",
      "more toward the global optimum.\n",
      "\n",
      "The AdaGrad algorithm14 achieves this by scaling down the gradient vector along the\n",
      "steepest dimensions (see Equation 11-6):\n",
      "\n",
      "Equation 11-6. AdaGrad algorithm\n",
      "\n",
      "1 .\n",
      "\n",
      "2 .\n",
      "\n",
      "s\n",
      "\n",
      "θ\n",
      "\n",
      "s + ∇θJ θ ⊗ ∇θJ θ\n",
      "θ − η ∇θJ θ ⊘ s + \n",
      "\n",
      "14 “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization,” J. Duchi et al. (2011).\n",
      "\n",
      "Faster Optimizers \n",
      "\n",
      "| \n",
      "\n",
      "347\n",
      "\n",
      "\fThe first step accumulates the square of the gradients into the vector s (recall that the\n",
      "⊗ symbol represents the element-wise multiplication). This vectorized form is equiv‐\n",
      "alent to computing si ← si + (∂ J(θ) / ∂ θi)2 for each element si of the vector s; in other\n",
      "words,  each  si  accumulates  the  squares  of  the  partial  derivative  of  the  cost  function\n",
      "with regards to parameter θi. If the cost function is steep along the ith dimension, then\n",
      "si will get larger and larger at each iteration.\n",
      "\n",
      "The second step is almost identical to Gradient Descent, but with one big difference:\n",
      "the gradient vector is scaled down by a factor of   +  (the ⊘ symbol represents the\n",
      "element-wise division, and ϵ is a smoothing term to avoid division by zero, typically\n",
      "set \n",
      "computing\n",
      "θi\n",
      "\n",
      "to \n",
      "equivalent \n",
      "θi − η ∂J θ / ∂θi/ si +  for all parameters θi (simultaneously).\n",
      "\n",
      "10–10).  This \n",
      "\n",
      "vectorized \n",
      "\n",
      "form \n",
      "\n",
      "to \n",
      "\n",
      "is \n",
      "\n",
      "In short, this algorithm decays the learning rate, but it does so faster for steep dimen‐\n",
      "sions than for dimensions with gentler slopes. This is called an adaptive learning rate. \n",
      "It  helps  point  the  resulting  updates  more  directly  toward  the  global  optimum  (see\n",
      "Figure 11-7). One additional benefit is that it requires much less tuning of the learn‐\n",
      "ing rate hyperparameter η.\n",
      "\n",
      "Figure 11-7. AdaGrad versus Gradient Descent\n",
      "\n",
      "AdaGrad  often  performs  well  for  simple  quadratic  problems,  but  unfortunately  it\n",
      "often  stops  too  early  when  training  neural  networks.  The  learning  rate  gets  scaled\n",
      "down  so  much  that  the  algorithm  ends  up  stopping  entirely  before  reaching  the\n",
      "global optimum. So even though Keras has an Adagrad optimizer, you should not use\n",
      "it to train deep neural networks (it may be efficient for simpler tasks such as Linear\n",
      "Regression,  though).  However,  understanding  Adagrad  is  helpful  to  grasp  the  other\n",
      "adaptive learning rate optimizers.\n",
      "\n",
      "348 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "\fRMSProp\n",
      "Although  AdaGrad  slows  down  a  bit  too  fast  and  ends  up  never  converging  to  the\n",
      "global optimum, the RMSProp algorithm15 fixes this by accumulating only the gradi‐\n",
      "ents from the most recent iterations (as opposed to all the gradients since the begin‐\n",
      "ning of training). It does so by using exponential decay in the first step (see Equation\n",
      "11-7).\n",
      "\n",
      "Equation 11-7. RMSProp algorithm\n",
      "\n",
      "1 .\n",
      "\n",
      "2 .\n",
      "\n",
      "s\n",
      "\n",
      "θ\n",
      "\n",
      "βs + 1 − β ∇θJ θ ⊗ ∇θJ θ\n",
      "θ − η ∇θJ θ ⊘ s + \n",
      "\n",
      "The decay rate β is typically set to 0.9. Yes, it is once again a new hyperparameter, but\n",
      "this default value often works well, so you may not need to tune it at all.\n",
      "\n",
      "As you might expect, Keras has an RMSProp optimizer:\n",
      "\n",
      "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)\n",
      "\n",
      "Except on very simple problems, this optimizer almost always performs much better\n",
      "than AdaGrad. In fact, it was the preferred optimization algorithm of many research‐\n",
      "ers until Adam optimization came around.\n",
      "\n",
      "Adam and Nadam Optimization\n",
      "Adam,16 which stands for adaptive moment estimation, combines the ideas of Momen‐\n",
      "tum optimization and RMSProp: just like Momentum optimization it keeps track of\n",
      "an exponentially decaying average of past gradients, and just like RMSProp it keeps\n",
      "track  of  an  exponentially  decaying  average  of  past  squared  gradients  (see  Equation\n",
      "11-8).17\n",
      "\n",
      "15 This algorithm was created by Geoffrey Hinton and Tijmen Tieleman in 2012, and presented by Geoffrey\n",
      "Hinton in his Coursera class on neural networks (slides: https://homl.info/57; video: https://homl.info/58).\n",
      "Amusingly, since the authors did not write a paper to describe it, researchers often cite “slide 29 in lecture 6”\n",
      "in their papers.\n",
      "\n",
      "16 “Adam: A Method for Stochastic Optimization,” D. Kingma, J. Ba (2015).\n",
      "\n",
      "17 These are estimations of the mean and (uncentered) variance of the gradients. The mean is often called the\n",
      "\n",
      "first moment, while the variance is often called the second moment, hence the name of the algorithm.\n",
      "\n",
      "Faster Optimizers \n",
      "\n",
      "| \n",
      "\n",
      "349\n",
      "\n",
      "\fEquation 11-8. Adam algorithm\n",
      "\n",
      "1 . m β1m − 1 − β1\n",
      "2 .\n",
      "\n",
      "s\n",
      "\n",
      "∇θJ θ\n",
      "\n",
      "∇θJ θ ⊗ ∇θJ θ\n",
      "\n",
      "t\n",
      "\n",
      "β2s + 1 − β2\n",
      "m\n",
      "1 − β1\n",
      "s\n",
      "1 − β2\n",
      "θ + η m ⊘ s + \n",
      "\n",
      "t\n",
      "\n",
      "3 . m\n",
      "\n",
      "4 .\n",
      "\n",
      "5 .\n",
      "\n",
      "s\n",
      "\n",
      "θ\n",
      "\n",
      "• t represents the iteration number (starting at 1).\n",
      "\n",
      "If  you  just  look  at  steps  1,  2,  and  5,  you  will  notice  Adam’s  close  similarity  to  both\n",
      "Momentum optimization and RMSProp. The only difference is that step 1 computes\n",
      "an  exponentially  decaying  average  rather  than  an  exponentially  decaying  sum,  but\n",
      "these are actually equivalent except for a constant factor (the decaying average is just\n",
      "1 – β1 times the decaying sum). Steps 3 and 4 are somewhat of a technical detail: since\n",
      "m and s are initialized at 0, they will be biased toward 0 at the beginning of training,\n",
      "so these two steps will help boost m and s at the beginning of training.\n",
      "\n",
      "The momentum decay hyperparameter β1 is typically initialized to 0.9, while the scal‐\n",
      "ing  decay  hyperparameter  β2  is  often  initialized  to  0.999.  As  earlier,  the  smoothing\n",
      "term ϵ is usually initialized to a tiny number such as 10–7. These are the default values\n",
      "for  the  Adam  class  (to  be  precise,  epsilon  defaults  to  None,  which  tells  Keras  to  use\n",
      "keras.backend.epsilon(),  which  defaults  to  10–7;  you  can  change  it  using\n",
      "keras.backend.set_epsilon()).\n",
      "\n",
      "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
      "\n",
      "Since Adam is an adaptive learning rate algorithm (like AdaGrad and RMSProp), it\n",
      "requires  less  tuning  of  the  learning  rate  hyperparameter  η.  You  can  often  use  the\n",
      "default value η = 0.001, making Adam even easier to use than Gradient Descent.\n",
      "\n",
      "If you are starting to feel overwhelmed by all these different techni‐\n",
      "ques,  and  wondering  how  to  choose  the  right  ones  for  your  task,\n",
      "don’t  worry:  some  practical  guidelines  are  provided  at  the  end  of\n",
      "this chapter.\n",
      "\n",
      "Finally, two variants of Adam are worth mentioning:\n",
      "\n",
      "350 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "\fmax β2\n",
      "\n",
      "• Adamax, introduced in the same paper as Adam: notice that in step 2 of Equation\n",
      "11-8, Adam accumulates the squares of the gradients in s (with a greater weight\n",
      "for more recent weights). In step 5, if we ignore ϵ and steps 3 and 4 (which are\n",
      "technical  details  anyway),  Adam  just  scales  down  the  parameter  updates  by  the\n",
      "square  root  of  s.  In  short,  Adam  scales  down  the  parameter  updates  by  the  ℓ2\n",
      "norm of the time-decayed gradients (recall that the ℓ2 norm is the square root of\n",
      "the sum of squares). Adamax just replaces the ℓ2 norm with the ℓ∞ norm (a fancy\n",
      "way  of  saying  the  max).  Specifically,  it  replaces  step  2  in  Equation  11-8  with\n",
      ", ∇θJ θ , it drops step 4, and in step 5 it scales down the gradient\n",
      "\n",
      "updates by a factor of s, which is just the max of the time-decayed gradients. In\n",
      "practice, this can make Adamax more stable than Adam, but this really depends\n",
      "on  the  dataset,  and  in  general  Adam  actually  performs  better.  So  it’s  just  one\n",
      "more optimizer you can try if you experience problems with Adam on some task.\n",
      "• Nadam  optimization18  is  more  important:  it  is  simply  Adam  optimization  plus\n",
      "the  Nesterov  trick,  so  it  will  often  converge  slightly  faster  than  Adam.  In  his\n",
      "report, Timothy Dozat compares many different optimizers on various tasks, and\n",
      "finds that Nadam generally outperforms Adam, but is sometimes outperformed\n",
      "by RMSProp.\n",
      "\n",
      "Adaptive  optimization  methods  (including  RMSProp,  Adam  and\n",
      "Nadam optimization) are often great, converging fast to a good sol‐\n",
      "ution.  However,  a  2017  paper19  by  Ashia  C.  Wilson  et  al.  showed\n",
      "that they can lead to solutions that generalize poorly on some data‐\n",
      "sets.  So  when  you  are  disappointed  by  your  model’s  performance,\n",
      "try using plain Nesterov Accelerated Gradient instead: your dataset\n",
      "may just be allergic to adaptive gradients. Also check out the latest\n",
      "research, it is moving fast (e.g., AdaBound).\n",
      "\n",
      "All  the  optimization  techniques  discussed  so  far  only  rely  on  the  first-order  partial\n",
      "derivatives  (Jacobians).  The  optimization  literature  contains  amazing  algorithms\n",
      "based  on  the  second-order  partial  derivatives  (the  Hessians,  which  are  the  partial\n",
      "derivatives of the Jacobians). Unfortunately, these algorithms are very hard to apply\n",
      "to  deep  neural  networks  because  there  are  n2  Hessians  per  output  (where  n  is  the\n",
      "number of parameters), as opposed to just n Jacobians per output. Since DNNs typi‐\n",
      "cally have tens of thousands of parameters, the second-order optimization algorithms\n",
      "\n",
      "18 “Incorporating Nesterov Momentum into Adam,” Timothy Dozat (2015).\n",
      "\n",
      "19 “The Marginal Value of Adaptive Gradient Methods in Machine Learning,” A. C. Wilson et al. (2017).\n",
      "\n",
      "Faster Optimizers \n",
      "\n",
      "| \n",
      "\n",
      "351\n",
      "\n",
      "\foften don’t even fit in memory, and even when they do, computing the Hessians is \n",
      "just too slow.\n",
      "\n",
      "Training Sparse Models\n",
      "All the optimization algorithms just presented produce dense models, meaning that\n",
      "most parameters will be nonzero. If you need a blazingly fast model at runtime, or if\n",
      "you  need  it  to  take  up  less  memory,  you  may  prefer  to  end  up  with  a  sparse  model\n",
      "instead.\n",
      "\n",
      "One trivial way to achieve this is to train the model as usual, then get rid of the tiny\n",
      "weights (set them to 0). However, this will typically not lead to a very sparse model,\n",
      "and it may degrade the model’s performance.\n",
      "\n",
      "A  better  option  is  to  apply  strong  ℓ1  regularization  during  training,  as  it  pushes  the\n",
      "optimizer to zero out as many weights as it can (as discussed in Chapter 4 about Lasso\n",
      "Regression).\n",
      "\n",
      "However, in some cases these techniques may remain insufficient. One last option is\n",
      "to apply Dual Averaging, often called Follow The Regularized Leader (FTRL), a techni‐\n",
      "que  proposed  by  Yurii  Nesterov.20  When  used  with  ℓ1  regularization,  this  technique\n",
      "often leads to very sparse models. Keras implements a variant of FTRL called FTRL-\n",
      "Proximal21 in the FTRL optimizer.\n",
      "\n",
      "Learning Rate Scheduling\n",
      "Finding  a  good  learning  rate  can  be  tricky.  If  you  set  it  way  too  high,  training  may\n",
      "actually  diverge  (as  we  discussed  in  Chapter  4).  If  you  set  it  too  low,  training  will\n",
      "eventually  converge  to  the  optimum,  but  it  will  take  a  very  long  time.  If  you  set  it\n",
      "slightly too high, it will make progress very quickly at first, but it will end up dancing\n",
      "around  the  optimum,  never  really  settling  down.  If  you  have  a  limited  computing\n",
      "budget, you may have to interrupt training before it has converged properly, yielding\n",
      "a suboptimal solution (see Figure 11-8).\n",
      "\n",
      "20 “Primal-Dual Subgradient Methods for Convex Problems,” Yurii Nesterov (2005).\n",
      "\n",
      "21 “Ad Click Prediction: a View from the Trenches,” H. McMahan et al. (2013).\n",
      "\n",
      "352 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "\fFigure 11-8. Learning curves for various learning rates η\n",
      "\n",
      "As we discussed in Chapter 10, one approach is to start with a large learning rate, and\n",
      "divide  it  by  3  until  the  training  algorithm  stops  diverging.  You  will  not  be  too  far\n",
      "from the optimal learning rate, which will learn quickly and converge to good solu‐\n",
      "tion.\n",
      "\n",
      "However,  you  can  do  better  than  a  constant  learning  rate:  if  you  start  with  a  high\n",
      "learning rate and then reduce it once it stops making fast progress, you can reach a\n",
      "good solution faster than with the optimal constant learning rate. There are many dif‐\n",
      "ferent strategies to reduce the learning rate during training. These strategies are called\n",
      "learning schedules (we briefly introduced this concept in Chapter 4), the most com‐\n",
      "mon of which are:\n",
      "\n",
      "Power scheduling\n",
      "\n",
      "Set the learning rate to a function of the iteration number t: η(t) = η0 / (1 + t/k)c.\n",
      "The  initial  learning  rate  η0,  the  power  c  (typically  set  to  1)  and  the  steps  s  are\n",
      "hyperparameters. The learning rate drops at each step, and after s steps it is down\n",
      "to η0 / 2. After s more steps, it is down to η0 / 3. Then down to η0 / 4, then η0 / 5,\n",
      "and so on. As you can see, this schedule first drops quickly, then more and more\n",
      "slowly. Of course, this requires tuning η0, s (and possibly c).\n",
      "\n",
      "Exponential scheduling\n",
      "\n",
      "Set the learning rate to: η(t) = η0 0.1t/s. The learning rate will gradually drop by a\n",
      "factor of 10 every s steps. While power scheduling reduces the learning rate more\n",
      "and more slowly, exponential scheduling keeps slashing it by a factor of 10 every\n",
      "s steps.\n",
      "\n",
      "Piecewise constant scheduling\n",
      "\n",
      "Use a constant learning rate for a number of epochs (e.g., η0 = 0.1 for 5 epochs),\n",
      "then a smaller learning rate for another number of epochs (e.g., η1 = 0.001 for 50\n",
      "epochs),  and  so  on.  Although  this  solution  can  work  very  well,  it  requires  fid‐\n",
      "\n",
      "Faster Optimizers \n",
      "\n",
      "| \n",
      "\n",
      "353\n",
      "\n",
      "\fdling around to figure out the right sequence of learning rates, and how long to\n",
      "use each of them.\n",
      "\n",
      "Performance scheduling\n",
      "\n",
      "Measure  the  validation  error  every  N  steps  (just  like  for  early  stopping)  and\n",
      "reduce the learning rate by a factor of λ when the error stops dropping.\n",
      "\n",
      "A  2013  paper22  by  Andrew  Senior  et  al.  compared  the  performance  of  some  of  the\n",
      "most popular learning schedules when training deep neural networks for speech rec‐\n",
      "ognition using Momentum optimization. The authors concluded that, in this setting,\n",
      "both  performance  scheduling  and  exponential  scheduling  performed  well.  They\n",
      "favored exponential scheduling because it was easy to tune and it converged slightly\n",
      "faster  to  the  optimal  solution  (they  also  mentioned  that  it  was  easier  to  implement\n",
      "than performance scheduling, but in Keras both options are easy).\n",
      "\n",
      "Implementing  power  scheduling  in  Keras  is  the  easiest  option:  just  set  the  decay\n",
      "hyperparameter when creating an optimizer. The decay is the inverse of s (the num‐\n",
      "ber of steps it takes to divide the learning rate by one more unit), and Keras assumes\n",
      "that c is equal to 1. For example:\n",
      "\n",
      "optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-4)\n",
      "\n",
      "Exponential scheduling and piecewise scheduling are quite simple too. You first need\n",
      "to  define  a  function  that  takes  the  current  epoch  and  returns  the  learning  rate.  For\n",
      "example, let’s implement exponential scheduling:\n",
      "\n",
      "def exponential_decay_fn(epoch):\n",
      "    return 0.01 * 0.1**(epoch / 20)\n",
      "\n",
      "If  you  do  not  want  to  hard-code  η0  and  s,  you  can  create  a  function  that  returns  a\n",
      "configured function:\n",
      "\n",
      "def exponential_decay(lr0, s):\n",
      "    def exponential_decay_fn(epoch):\n",
      "        return lr0 * 0.1**(epoch / s)\n",
      "    return exponential_decay_fn\n",
      "\n",
      "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)\n",
      "\n",
      "Next, just create a LearningRateScheduler callback, giving it the schedule function,\n",
      "and pass this callback to the fit() method:\n",
      "\n",
      "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
      "history = model.fit(X_train_scaled, y_train, [...], callbacks=[lr_scheduler])\n",
      "\n",
      "22 “An Empirical Study of Learning Rates in Deep Neural Networks for Speech Recognition,” A. Senior et al.\n",
      "\n",
      "(2013).\n",
      "\n",
      "354 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "\fThe LearningRateScheduler will update the optimizer’s learning_rate attribute at\n",
      "the beginning of each epoch. Updating the learning rate just once per epoch is usually\n",
      "enough, but if you want it to be updated more often, for example at every step, you\n",
      "need to write your own callback (see the notebook for an example). This can make\n",
      "sense if there are many steps per epoch.\n",
      "\n",
      "The schedule function can optionally take the current learning rate as a second argu‐\n",
      "ment.  For  example,  the  following  schedule  function  just  multiplies  the  previous\n",
      "learning rate by 0.1&1/20, which results in the same exponential decay (except the decay\n",
      "now starts at the beginning of epoch 0 instead of 1). This implementation relies on\n",
      "the  optimizer’s  initial  learning  rate  (contrary  to  the  previous  implementation),  so\n",
      "make sure to set it appropriately.\n",
      "\n",
      "def exponential_decay_fn(epoch, lr):\n",
      "    return lr * 0.1**(1 / 20)\n",
      "\n",
      "When you save a model, the optimizer and its learning rate get saved along with it.\n",
      "This means that with this new schedule function, you could just load a trained model\n",
      "and continue training where it left off, no problem. However, things are not so simple\n",
      "if  your  schedule  function  uses  the  epoch  argument:  indeed,  the  epoch  does  not  get\n",
      "saved, and it gets reset to 0 every time you call the fit() method. This could lead to a\n",
      "very large learning rate when you continue training a model where it left off, which\n",
      "would likely damage your model’s weights. One solution is to manually set the fit()\n",
      "method’s initial_epoch argument so the epoch starts at the right value.\n",
      "\n",
      "For piecewise constant scheduling, you can use a schedule function like the following\n",
      "one (as earlier, you can define a more general function if you want, see the notebook\n",
      "for  an  example),  then  create  a  LearningRateScheduler  callback  with  this  function\n",
      "and pass it to the fit() method, just like we did for exponential scheduling:\n",
      "\n",
      "def piecewise_constant_fn(epoch):\n",
      "    if epoch < 5:\n",
      "        return 0.01\n",
      "    elif epoch < 15:\n",
      "        return 0.005\n",
      "    else:\n",
      "        return 0.001\n",
      "\n",
      "For performance scheduling, simply use the ReduceLROnPlateau callback. For exam‐\n",
      "ple, if you pass the following callback to the fit() method, it will multiply the learn‐\n",
      "ing rate by 0.5 whenever the best validation loss does not improve for 5 consecutive\n",
      "epochs  (other  options  are  available,  please  check  the  documentation  for  more\n",
      "details):\n",
      "\n",
      "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
      "\n",
      "Lastly,  tf.keras  offers  an  alternative  way  to  implement  learning  rate  scheduling:  just\n",
      "define  the  learning  rate  using  one  of  the  schedules  available  in  keras.optimiz\n",
      "\n",
      "Faster Optimizers \n",
      "\n",
      "| \n",
      "\n",
      "355\n",
      "\n",
      "\fers.schedules, then pass this learning rate to any optimizer. This approach updates\n",
      "the learning rate at each step rather than at each epoch. For example, here is how to\n",
      "implement the same exponential schedule as earlier:\n",
      "\n",
      "s = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\n",
      "learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n",
      "optimizer = keras.optimizers.SGD(learning_rate)\n",
      "\n",
      "This  is  nice  and  simple,  plus  when  you  save  the  model,  the  learning  rate  and  its\n",
      "schedule (including its state) get saved as well. However, this approach is not part of\n",
      "the Keras API, it is specific to tf.keras.\n",
      "\n",
      "To sum up, exponential decay or performance scheduling can considerably speed up\n",
      "convergence, so give them a try!\n",
      "\n",
      "Avoiding Overfitting Through Regularization\n",
      "\n",
      "With  four  parameters  I  can  fit  an  elephant  and  with  five  I  can  make  him  wiggle  his\n",
      "trunk.\n",
      "\n",
      "—John von Neumann, cited by Enrico Fermi in Nature 427\n",
      "\n",
      "With thousands of parameters you can fit the whole zoo. Deep neural networks typi‐\n",
      "cally have tens of thousands of parameters, sometimes even millions. With so many\n",
      "parameters, the network has an incredible amount of freedom and can fit a huge vari‐\n",
      "ety of complex datasets. But this great flexibility also means that it is prone to overfit‐\n",
      "ting the training set. We need regularization.\n",
      "\n",
      "We  already  implemented  one  of  the  best  regularization  techniques  in  Chapter  10:\n",
      "early  stopping.  Moreover,  even  though  Batch  Normalization  was  designed  to  solve\n",
      "the vanishing/exploding gradients problems, is also acts like a pretty good regularizer.\n",
      "In this section we will present other popular regularization techniques for neural net‐\n",
      "works: ℓ1 and ℓ2 regularization, dropout and max-norm regularization.\n",
      "\n",
      "ℓ1 and ℓ2 Regularization\n",
      "Just like you did in Chapter 4 for simple linear models, you can use ℓ1 and ℓ2 regulari‐\n",
      "zation  to  constrain  a  neural  network’s  connection  weights  (but  typically  not  its  bia‐\n",
      "ses).  Here  is  how  to  apply  ℓ2  regularization  to  a  Keras  layer’s  connection  weights,\n",
      "using a regularization factor of 0.01:\n",
      "\n",
      "layer = keras.layers.Dense(100, activation=\"elu\",\n",
      "                           kernel_initializer=\"he_normal\",\n",
      "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
      "\n",
      "The l2() function returns a regularizer that will be called to compute the regulariza‐\n",
      "tion  loss,  at  each  step  during  training.  This  regularization  loss  is  then  added  to  the\n",
      "final  loss.  As  you  might  expect,  you  can  just  use  keras.regularizers.l1()  if  you\n",
      "\n",
      "356 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "\fwant ℓ1 regularization, and if you want both ℓ1 and ℓ2 regularization, use keras.regu\n",
      "larizers.l1_l2() (specifying both regularization factors).\n",
      "\n",
      "Since you will typically want to apply the same regularizer to all layers in your net‐\n",
      "work, as well as the same activation function and the same initialization strategy in all\n",
      "hidden  layers,  you  may  find  yourself  repeating  the  same  arguments  over  and  over.\n",
      "This makes it ugly and error-prone. To avoid this, you can try refactoring your code\n",
      "to use loops. Another option is to use Python’s functools.partial() function: it lets\n",
      "you  create  a  thin  wrapper  for  any  callable,  with  some  default  argument  values.  For\n",
      "example:\n",
      "\n",
      "from functools import partial\n",
      "\n",
      "RegularizedDense = partial(keras.layers.Dense,\n",
      "                           activation=\"elu\",\n",
      "                           kernel_initializer=\"he_normal\",\n",
      "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
      "\n",
      "model = keras.models.Sequential([\n",
      "    keras.layers.Flatten(input_shape=[28, 28]),\n",
      "    RegularizedDense(300),\n",
      "    RegularizedDense(100),\n",
      "    RegularizedDense(10, activation=\"softmax\",\n",
      "                     kernel_initializer=\"glorot_uniform\")\n",
      "])\n",
      "\n",
      "Dropout\n",
      "Dropout  is  one  of  the  most  popular  regularization  techniques  for  deep  neural  net‐\n",
      "works. It was proposed23 by Geoffrey Hinton in 2012 and further detailed in a paper24\n",
      "by Nitish Srivastava et al., and it has proven to be highly successful: even the state-of-\n",
      "the-art neural networks got a 1–2% accuracy boost simply by adding dropout. This\n",
      "may not sound like a lot, but when a model already has 95% accuracy, getting a 2%\n",
      "accuracy boost means dropping the error rate by almost 40% (going from 5% error to\n",
      "roughly 3%).\n",
      "\n",
      "It  is  a  fairly  simple  algorithm:  at  every  training  step,  every  neuron  (including  the\n",
      "input neurons, but always excluding the output neurons) has a probability p of being\n",
      "temporarily  “dropped  out,”  meaning  it  will  be  entirely  ignored  during  this  training\n",
      "step, but it may be active during the next step (see Figure 11-9). The hyperparameter\n",
      "p is called the dropout rate, and it is typically set to 50%. After training, neurons don’t\n",
      "get  dropped  anymore.  And  that’s  all  (except  for  a  technical  detail  we  will  discuss\n",
      "momentarily).\n",
      "\n",
      "23 “Improving neural networks by preventing co-adaptation of feature detectors,” G. Hinton et al. (2012).\n",
      "\n",
      "24 “Dropout: A Simple Way to Prevent Neural Networks from Overfitting,” N. Srivastava et al. (2014).\n",
      "\n",
      "Avoiding Overfitting Through Regularization \n",
      "\n",
      "| \n",
      "\n",
      "357\n",
      "\n",
      "\fFigure 11-9. Dropout regularization\n",
      "\n",
      "It  is  quite  surprising  at  first  that  this  rather  brutal  technique  works  at  all.  Would  a\n",
      "company  perform  better  if  its  employees  were  told  to  toss  a  coin  every  morning  to\n",
      "decide whether or not to go to work? Well, who knows; perhaps it would! The com‐\n",
      "pany would obviously be forced to adapt its organization; it could not rely on any sin‐\n",
      "gle  person  to  fill  in  the  coffee  machine  or  perform  any  other  critical  tasks,  so  this\n",
      "expertise  would  have  to  be  spread  across  several  people.  Employees  would  have  to\n",
      "learn  to  cooperate  with  many  of  their  coworkers,  not  just  a  handful  of  them.  The\n",
      "company  would  become  much  more  resilient.  If  one  person  quit,  it  wouldn’t  make\n",
      "much of a difference. It’s unclear whether this idea would actually work for compa‐\n",
      "nies, but it certainly does for neural networks. Neurons trained with dropout cannot\n",
      "co-adapt  with  their  neighboring  neurons;  they  have  to  be  as  useful  as  possible  on\n",
      "their own. They also cannot rely excessively on just a few input neurons; they must\n",
      "pay attention to each of their input neurons. They end up being less sensitive to slight\n",
      "changes in the inputs. In the end you get a more robust network that generalizes bet‐\n",
      "ter.\n",
      "\n",
      "Another  way  to  understand  the  power  of  dropout  is  to  realize  that  a  unique  neural\n",
      "network is generated at each training step. Since each neuron can be either present or\n",
      "absent, there is a total of 2N possible networks (where N is the total number of drop‐\n",
      "pable neurons). This is such a huge number that it is virtually impossible for the same\n",
      "neural network to be sampled twice. Once you have run a 10,000 training steps, you\n",
      "have essentially trained 10,000 different neural networks (each with just one training\n",
      "instance).  These  neural  networks  are  obviously  not  independent  since  they  share\n",
      "many  of  their  weights,  but  they  are  nevertheless  all  different.  The  resulting  neural\n",
      "network can be seen as an averaging ensemble of all these smaller neural networks.\n",
      "\n",
      "There  is  one  small  but  important  technical  detail.  Suppose  p  =  50%,  in  which  case\n",
      "during testing a neuron will be connected to twice as many input neurons as it was\n",
      "(on average) during training. To compensate for this fact, we need to multiply each\n",
      "\n",
      "358 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "\fneuron’s input connection weights by 0.5 after training. If we don’t, each neuron will\n",
      "get a total input signal roughly twice as large as what the network was trained on, and\n",
      "it is unlikely to perform well. More generally, we need to multiply each input connec‐\n",
      "tion weight by the keep probability (1 – p) after training. Alternatively, we can divide\n",
      "each  neuron’s  output  by  the  keep  probability  during  training  (these  alternatives  are\n",
      "not perfectly equivalent, but they work equally well).\n",
      "\n",
      "To  implement  dropout  using  Keras,  you  can  use  the  keras.layers.Dropout  layer.\n",
      "During training, it randomly drops some inputs (setting them to 0) and divides the\n",
      "remaining inputs by the keep probability. After training, it does nothing at all, it just\n",
      "passes the inputs to the next layer. For example, the following code applies dropout\n",
      "regularization before every Dense layer, using a dropout rate of 0.2:\n",
      "\n",
      "model = keras.models.Sequential([\n",
      "    keras.layers.Flatten(input_shape=[28, 28]),\n",
      "    keras.layers.Dropout(rate=0.2),\n",
      "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
      "    keras.layers.Dropout(rate=0.2),\n",
      "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
      "    keras.layers.Dropout(rate=0.2),\n",
      "    keras.layers.Dense(10, activation=\"softmax\")\n",
      "])\n",
      "\n",
      "Since  dropout  is  only  active  during  training,  the  training  loss  is\n",
      "penalized  compared  to  the  validation  loss,  so  comparing  the  two\n",
      "can  be  misleading.  In  particular,  a  model  may  be  overfitting  the\n",
      "training set and yet have similar training and validation losses. So\n",
      "make sure to evaluate the training loss without dropout (e.g., after\n",
      "training).  Alternatively,  you  can  call  the  fit()  method  inside  a\n",
      "with  keras.backend.learning_phase_scope(1)  block:  this  will\n",
      "force dropout to be active during both training and validation.25\n",
      "\n",
      "If you observe that the model is overfitting, you can increase the dropout rate. Con‐\n",
      "versely, you should try decreasing the dropout rate if the model underfits the training\n",
      "set.  It  can  also  help  to  increase  the  dropout  rate  for  large  layers,  and  reduce  it  for\n",
      "small ones. Moreover, many state-of-the-art architectures only use dropout after the\n",
      "last hidden layer, so you may want to try this if full dropout is too strong.\n",
      "\n",
      "Dropout does tend to significantly slow down convergence, but it usually results in a\n",
      "much better model when tuned properly. So, it is generally well worth the extra time\n",
      "and effort.\n",
      "\n",
      "25 This is specific to tf.keras, so you may prefer to use keras.backend.set_learning_phase(1) before calling\n",
      "\n",
      "the fit() method (and set it back to 0 right after).\n",
      "\n",
      "Avoiding Overfitting Through Regularization \n",
      "\n",
      "| \n",
      "\n",
      "359\n",
      "\n",
      "\fIf you want to regularize a self-normalizing network based on the\n",
      "SELU  activation  function  (as  discussed  earlier),  you  should  use\n",
      "AlphaDropout: this is a variant of dropout that preserves the mean\n",
      "and standard deviation of its inputs (it was introduced in the same\n",
      "paper as SELU, as regular dropout would break self-normalization).\n",
      "\n",
      "Monte-Carlo (MC) Dropout\n",
      "In 2016, a paper26 by Yarin Gal and Zoubin Ghahramani added more good reasons to\n",
      "use dropout:\n",
      "\n",
      "• First,  the  paper  establishes  a  profound  connection  between  dropout  networks\n",
      "(i.e., neural networks containing a dropout layer before every weight layer) and\n",
      "approximate Bayesian inference27, giving dropout a solid mathematical justifica‐\n",
      "tion.\n",
      "\n",
      "• Second,  they  introduce  a  powerful  technique  called  MC  Dropout,  which  can\n",
      "boost the performance of any trained dropout model, without having to retrain it\n",
      "or even modify it at all!\n",
      "\n",
      "• Moreover,  MC  Dropout  also  provides  a  much  better  measure  of  the  model’s\n",
      "\n",
      "uncertainty.\n",
      "\n",
      "• Finally,  it  is  also  amazingly  simple  to  implement.  If  this  all  sounds  like  a  “one\n",
      "weird  trick”  advertisement,  then  take  a  look  at  the  following  code.  It  is  the  full\n",
      "implementation of MC Dropout, boosting the dropout model we trained earlier,\n",
      "without retraining it:\n",
      "\n",
      "with keras.backend.learning_phase_scope(1): # force training mode = dropout on\n",
      "    y_probas = np.stack([model.predict(X_test_scaled)\n",
      "                         for sample in range(100)])\n",
      "y_proba = y_probas.mean(axis=0)\n",
      "\n",
      "We  first  force  training  mode  on,  using  a  learning_phase_scope(1)  context.  This\n",
      "turns dropout on within the with block. Then we make 100 predictions over the test\n",
      "set,  and  we  stack  them.  Since  dropout  is  on,  all  predictions  will  be  different.  Recall\n",
      "that predict() returns a matrix with one row per instance, and one column per class.\n",
      "Since there are 10,000 instances in the test set, and 10 classes, this is a matrix of shape\n",
      "[10000, 10]. We stack 100 such matrices, so y_probas is an array of shape [100, 10000,\n",
      "10]. Once we average over the first dimension (axis=0), we get y_proba, an array of\n",
      "shape  [10000,  10],  like  we  would  get  with  a  single  prediction.  That’s  all!  Averaging\n",
      "\n",
      "26 “Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning,” Y. Gal and Z.\n",
      "\n",
      "Ghahramani (2016).\n",
      "\n",
      "27 Specifically, they show that training a dropout network is mathematically equivalent to approximate Bayesian\n",
      "\n",
      "inference in a specific type of probabilistic model called a deep Gaussian Process.\n",
      "\n",
      "360 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "\fover  multiple  predictions  with  dropout  on  gives  us  a  Monte  Carlo  estimate  that  is\n",
      "generally  more  reliable  than  the  result  of  a  single  prediction  with  dropout  off.  For\n",
      "example, let’s look at the model’s prediction for the first instance in the test set, with\n",
      "dropout off:\n",
      "\n",
      ">>> np.round(model.predict(X_test_scaled[:1]), 2)\n",
      "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
      "      dtype=float32)\n",
      "\n",
      "The  model  seems  almost  certain  that  this  image  belongs  to  class  9  (ankle  boot).\n",
      "Should  you  trust  it?  Is  there  really  so  little  room  for  doubt?  Compare  this  with  the\n",
      "predictions made when dropout is activated:\n",
      "\n",
      ">>> np.round(y_probas[:, :1], 2)\n",
      "array([[[0.  , 0.  , 0.  , 0.  , 0.  , 0.14, 0.  , 0.17, 0.  , 0.68]],\n",
      "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.16, 0.  , 0.2 , 0.  , 0.64]],\n",
      "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.01, 0.  , 0.97]],\n",
      "       [...]\n",
      "\n",
      "This  tells  a  very  different  story:  apparently,  when  we  activate  dropout,  the  model  is\n",
      "not  sure  anymore.  It  still  seems  to  prefer  class  9,  but  sometimes  it  hesitates  with\n",
      "classes 5 (sandal) and 7 (sneaker), which makes sense given they’re all footwear. Once\n",
      "we average over the first dimension, we get the following MC dropout predictions:\n",
      "\n",
      ">>> np.round(y_proba[:1], 2)\n",
      "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.22, 0.  , 0.16, 0.  , 0.62]],\n",
      "      dtype=float32)\n",
      "\n",
      "The model still thinks this image belongs to class 9, but only with a 62% confidence,\n",
      "which seems much more reasonable than 99%. Plus it’s useful to know exactly which\n",
      "other classes it thinks are likely. And you can also take a look at the standard devia‐\n",
      "tion of the probability estimates:\n",
      "\n",
      ">>> y_std = y_probas.std(axis=0)\n",
      ">>> np.round(y_std[:1], 2)\n",
      "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.28, 0.  , 0.21, 0.02, 0.32]],\n",
      "      dtype=float32)\n",
      "\n",
      "Apparently  there’s  quite  a  lot  of  variance  in  the  probability  estimates:  if  you  were\n",
      "building a risk-sensitive system (e.g., a medical or financial system), you should prob‐\n",
      "ably  treat  such  an  uncertain  prediction  with  extreme  caution.  You  definitely  would\n",
      "not  treat  it  like  a  99%  confident  prediction.  Moreover,  the  model’s  accuracy  got  a\n",
      "small boost from 86.8 to 86.9:\n",
      "\n",
      ">>> accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
      ">>> accuracy\n",
      "0.8694\n",
      "\n",
      "Avoiding Overfitting Through Regularization \n",
      "\n",
      "| \n",
      "\n",
      "361\n",
      "\n",
      "\fThe number of Monte Carlo samples you use (100 in this example)\n",
      "is a hyperparameter you can tweak. The higher it is, the more accu‐\n",
      "rate  the  predictions  and  their  uncertainty  estimates  will  be.  How‐\n",
      "ever,  it  you  double  it,  inference  time  will  also  be  doubled.\n",
      "Moreover, above a certain number of samples, you will notice little\n",
      "improvement.  So  your  job  is  to  find  the  right  tradeoff  between\n",
      "latency and accuracy, depending on your application.\n",
      "\n",
      "If your model contains other layers that behave in a special way during training (such\n",
      "as Batch Normalization layers), then you should not force training mode like we just\n",
      "did.  Instead,  you  should  replace  the  Dropout  layers  with  the  following  MCDropout\n",
      "class:\n",
      "\n",
      "class MCDropout(keras.layers.Dropout):\n",
      "    def call(self, inputs):\n",
      "        return super().call(inputs, training=True)\n",
      "\n",
      "We just sublass the Dropout layer and override the call() method to force its train\n",
      "ing argument to True (see Chapter 12). Similarly, you could define an MCAlphaDrop\n",
      "out  class  by  subclassing  AlphaDropout  instead.  If  you  are  creating  a  model  from\n",
      "scratch, it’s just a matter of using MCDropout rather than Dropout. But if you have a\n",
      "model that was already trained using Dropout, you need to create a new model, iden‐\n",
      "tical to the existing model except replacing the Dropout layers with MCDropout, then\n",
      "copy the existing model’s weights to your new model.\n",
      "\n",
      "In short, MC Dropout is a fantastic technique that boosts dropout models and pro‐\n",
      "vides better uncertainty estimates. And of course, since it is just regular dropout dur‐\n",
      "ing training, it also acts like a regularizer.\n",
      "\n",
      "Max-Norm Regularization\n",
      "Another regularization technique that is quite popular for neural networks is called\n",
      "max-norm regularization: for each neuron, it constrains the weights w of the incom‐\n",
      "ing connections such that ∥ *w* ∥2 ≤ _r_, where r is the max-norm hyperparameter\n",
      "and ∥ · ∥2 is the ℓ2 norm.\n",
      "\n",
      "Max-norm regularization does not add a regularization loss term to the overall loss\n",
      "function. Instead, it is typically implemented by computing ∥w∥2 after each training\n",
      "step and clipping w if needed (w\n",
      ").\n",
      "\n",
      "w r\n",
      "∥ w ∥\n",
      "\n",
      "2\n",
      "\n",
      "Reducing r increases the amount of regularization and helps reduce overfitting. Max-\n",
      "norm  regularization  can  also  help  alleviate  the  vanishing/exploding  gradients  prob‐\n",
      "lems (if you are not using Batch Normalization).\n",
      "\n",
      "362 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "\fTo  implement  max-norm  regularization  in  Keras,  just  set  every  hidden  layer’s  ker\n",
      "nel_constraint  argument  to  a  max_norm()  constraint,  with  the  appropriate  max\n",
      "value, for example:\n",
      "\n",
      "keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
      "                   kernel_constraint=keras.constraints.max_norm(1.))\n",
      "\n",
      "After each training iteration, the model’s fit() method will call the object returned\n",
      "by  max_norm(),  passing  it  the  layer’s  weights  and  getting  clipped  weights  in  return,\n",
      "which then replace the layer’s weights. As we will see in Chapter 12, you can define\n",
      "your  own  custom  constraint  function  if  you  ever  need  to,  and  use  it  as  the  ker\n",
      "nel_constraint.  You  can  also  constrain  the  bias  terms  by  setting  the  bias_con\n",
      "straint argument.\n",
      "\n",
      "The max_norm() function has an axis argument that defaults to 0. A Dense layer usu‐\n",
      "ally  has  weights  of  shape  [number  of  inputs,  number  of  neurons],  so  using  axis=0\n",
      "means that the max norm constraint will apply independently to each neuron’s weight\n",
      "vector.  If  you  want  to  use  max-norm  with  convolutional  layers  (see  Chapter  14),\n",
      "make  sure  to  set  the  max_norm()  constraint’s  axis  argument  appropriately  (usually\n",
      "axis=[0, 1, 2]).\n",
      "\n",
      "Summary and Practical Guidelines\n",
      "In this chapter, we have covered a wide range of techniques and you may be wonder‐\n",
      "ing  which  ones  you  should  use.  The  configuration  in  Table  11-2  will  work  fine  in\n",
      "most cases, without requiring much hyperparameter tuning.\n",
      "\n",
      "Table 11-2. Default DNN configuration\n",
      "\n",
      "Hyperparameter\n",
      "Kernel initializer:\n",
      "\n",
      "Default value\n",
      "LeCun initialization\n",
      "\n",
      "Activation function:\n",
      "\n",
      "SELU\n",
      "\n",
      "Normalization:\n",
      "\n",
      "Regularization:\n",
      "\n",
      "Optimizer:\n",
      "\n",
      "None (self-normalization)\n",
      "\n",
      "Early stopping\n",
      "\n",
      "Nadam\n",
      "\n",
      "Learning rate schedule:\n",
      "\n",
      "Performance scheduling\n",
      "\n",
      "Don’t forget to standardize the input features! Of course, you should also try to reuse\n",
      "parts of a pretrained neural network if you can find one that solves a similar problem,\n",
      "or use unsupervised pretraining if you have a lot of unlabeled data, or pretraining on\n",
      "an auxiliary task if you have a lot of labeled data for a similar task.\n",
      "\n",
      "The default configuration in Table 11-2 may need to be tweaked:\n",
      "\n",
      "Summary and Practical Guidelines \n",
      "\n",
      "| \n",
      "\n",
      "363\n",
      "\n",
      "\f• If your model self-normalizes:\n",
      "\n",
      "— If it overfits the training set, then you should add alpha dropout (and always\n",
      "use early stopping as well). Do not use other regularization methods, or else\n",
      "they would break self-normalization.\n",
      "\n",
      "• If your model cannot self-normalize (e.g., it is a recurrent net or it contains skip\n",
      "\n",
      "connections):\n",
      "\n",
      "— You  can  try  using  ELU  (or  another  activation  function)  instead  of  SELU,  it\n",
      "may  perform  better.  Make  sure  to  change  the  initialization  method  accord‐\n",
      "ingly (e.g., He init for ELU or ReLU).\n",
      "\n",
      "— If it is a deep network, you should use Batch Normalization after every hidden\n",
      "layer. If it overfits the training set, you can also try using max-norm or ℓ2 reg‐\n",
      "ularization.\n",
      "\n",
      "• If you need a sparse model, you can use ℓ1 regularization (and optionally zero out\n",
      "the tiny weights after training). If you need an even sparser model, you can try\n",
      "using FTRL instead of Nadam optimization, along with ℓ1 regularization. In any\n",
      "case, this will break self-normalization, so you will need to switch to BN if your\n",
      "model is deep.\n",
      "\n",
      "• If  you  need  a  low-latency  model  (one  that  performs  lightning-fast  predictions),\n",
      "you may need to use less layers, avoid Batch Normalization, and possibly replace\n",
      "the  SELU  activation  function  with  the  leaky  ReLU.  Having  a  sparse  model  will\n",
      "also help. You may also want to reduce the float precision from 32-bits to 16-bit\n",
      "(or even 8-bits) (see ???).\n",
      "\n",
      "• If  you  are  building  a  risk-sensitive  application,  or  inference  latency  is  not  very\n",
      "important  in  your  application,  you  can  use  MC  Dropout  to  boost  performance\n",
      "and get more reliable probability estimates, along with uncertainty estimates.\n",
      "\n",
      "With these guidelines, you are now ready to train very deep nets! I hope you are now\n",
      "convinced  that  you  can  go  a  very  long  way  using  just  Keras.  However,  there  may\n",
      "come a time when you need to have even more control, for example to write a custom\n",
      "loss function or to tweak the training algorithm. For such cases, you will need to use\n",
      "TensorFlow’s lower-level API, as we will see in the next chapter.\n",
      "\n",
      "Exercises\n",
      "\n",
      "1. Is  it  okay  to  initialize  all  the  weights  to  the  same  value  as  long  as  that  value  is\n",
      "\n",
      "selected randomly using He initialization?\n",
      "\n",
      "2. Is it okay to initialize the bias terms to 0?\n",
      "\n",
      "3. Name three advantages of the SELU activation function over ReLU.\n",
      "\n",
      "364 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "\f4. In which cases would you want to use each of the following activation functions:\n",
      "\n",
      "SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
      "\n",
      "5. What  may  happen  if  you  set  the  momentum  hyperparameter  too  close  to  1  (e.g.,\n",
      "\n",
      "0.99999) when using an SGD optimizer?\n",
      "\n",
      "6. Name three ways you can produce a sparse model.\n",
      "\n",
      "7. Does  dropout  slow  down  training?  Does  it  slow  down  inference  (i.e.,  making\n",
      "\n",
      "predictions on new instances)? What are about MC dropout?\n",
      "\n",
      "8. Deep Learning.\n",
      "\n",
      "a. Build  a  DNN  with  five  hidden  layers  of  100  neurons  each,  He  initialization,\n",
      "\n",
      "and the ELU activation function.\n",
      "\n",
      "b. Using  Adam  optimization  and  early  stopping,  try  training  it  on  MNIST  but\n",
      "only  on  digits  0  to  4,  as  we  will  use  transfer  learning  for  digits  5  to  9  in  the\n",
      "next exercise. You will need a softmax output layer with five neurons, and as\n",
      "always  make  sure  to  save  checkpoints  at  regular  intervals  and  save  the  final\n",
      "model so you can reuse it later.\n",
      "\n",
      "c. Tune the hyperparameters using cross-validation and see what precision you\n",
      "\n",
      "can achieve.\n",
      "\n",
      "d. Now  try  adding  Batch  Normalization  and  compare  the  learning  curves:  is  it\n",
      "\n",
      "converging faster than before? Does it produce a better model?\n",
      "\n",
      "e. Is  the  model  overfitting  the  training  set?  Try  adding  dropout  to  every  layer\n",
      "\n",
      "and try again. Does it help?\n",
      "\n",
      "9. Transfer learning.\n",
      "\n",
      "a. Create a new DNN that reuses all the pretrained hidden layers of the previous\n",
      "model, freezes them, and replaces the softmax output layer with a new one.\n",
      "\n",
      "b. Train this new DNN on digits 5 to 9, using only 100 images per digit, and time\n",
      "how  long  it  takes.  Despite  this  small  number  of  examples,  can  you  achieve\n",
      "high precision?\n",
      "\n",
      "c. Try caching the frozen layers, and train the model again: how much faster is it\n",
      "\n",
      "now?\n",
      "\n",
      "d. Try  again  reusing  just  four  hidden  layers  instead  of  five.  Can  you  achieve  a\n",
      "\n",
      "higher precision?\n",
      "\n",
      "e. Now  unfreeze  the  top  two  hidden  layers  and  continue  training:  can  you  get\n",
      "\n",
      "the model to perform even better?\n",
      "\n",
      "10. Pretraining on an auxiliary task.\n",
      "\n",
      "a. In this exercise you will build a DNN that compares two MNIST digit images\n",
      "and predicts whether they represent the same digit or not. Then you will reuse\n",
      "the lower layers of this network to train an MNIST classifier using very little\n",
      "\n",
      "Exercises \n",
      "\n",
      "| \n",
      "\n",
      "365\n",
      "\n",
      "\ftraining data. Start by building two DNNs (let’s call them DNN A and B), both\n",
      "similar  to  the  one  you  built  earlier  but  without  the  output  layer:  each  DNN\n",
      "should have five hidden layers of 100 neurons each, He initialization, and ELU\n",
      "activation.  Next,  add  one  more  hidden  layer  with  10  units  on  top  of  both\n",
      "DNNs. To do this, you should use a keras.layers.Concatenate layer to con‐\n",
      "catenate the outputs of both DNNs for each instance, then feed the result to\n",
      "the hidden layer. Finally, add an output layer with a single neuron using the\n",
      "logistic activation function.\n",
      "\n",
      "b. Split  the  MNIST  training  set  in  two  sets:  split  #1  should  containing  55,000\n",
      "images,  and  split  #2  should  contain  contain  5,000  images.  Create  a  function\n",
      "that generates a training batch where each instance is a pair of MNIST images\n",
      "picked from split #1. Half of the training instances should be pairs of images\n",
      "that belong to the same class, while the other half should be images from dif‐\n",
      "ferent  classes.  For  each  pair,  the  training  label  should  be  0  if  the  images  are\n",
      "from the same class, or 1 if they are from different classes.\n",
      "\n",
      "c. Train the DNN on this training set. For each image pair, you can simultane‐\n",
      "ously  feed  the  first  image  to  DNN  A  and  the  second  image  to  DNN  B.  The\n",
      "whole  network  will  gradually  learn  to  tell  whether  two  images  belong  to  the\n",
      "same class or not.\n",
      "\n",
      "d. Now create a new DNN by reusing and freezing the hidden layers of DNN A\n",
      "and adding a softmax output layer on top with 10 neurons. Train this network\n",
      "on split #2 and see if you can achieve high performance despite having only\n",
      "500 images per class.\n",
      "\n",
      "Solutions to these exercises are available in ???.\n",
      "\n",
      "366 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 11: Training Deep Neural Networks\n",
      "\n",
      "\fCHAPTER 12\n",
      "Custom Models and Training with\n",
      "TensorFlow\n",
      "\n",
      "With  Early  Release  ebooks,  you  get  books  in  their  earliest  form—\n",
      "the author’s raw and unedited content as he or she writes—so you\n",
      "can  take  advantage  of  these  technologies  long  before  the  official\n",
      "release of these titles. The following will be Chapter 12 in the final\n",
      "release of the book.\n",
      "\n",
      "So far we have used only TensorFlow’s high level API, tf.keras, but it already got us\n",
      "pretty  far:  we  built  various  neural  network  architectures,  including  regression  and\n",
      "classification nets, wide & deep nets and self-normalizing nets, using all sorts of tech‐\n",
      "niques, such as Batch Normalization, dropout, learning rate schedules, and more. In\n",
      "fact,  95%  of  the  use  cases  you  will  encounter  will  not  require  anything  else  than\n",
      "tf.keras (and tf.data, see Chapter 13). But now it’s time to dive deeper into TensorFlow\n",
      "and take a look at its lower-level Python API. This will be useful when you need extra\n",
      "control,  to  write  custom  loss  functions,  custom  metrics,  layers,  models,  initializers,\n",
      "regularizers,  weight  constraints  and  more.  You  may  even  need  to  fully  control  the\n",
      "training loop itself, for example to apply special transformations or constraints to the\n",
      "gradients  (beyond  just  clipping  them),  or  to  use  multiple  optimizers  for  different\n",
      "parts  of  the  network.  We  will  cover  all  these  cases  in  this  chapter,  then  we  will  also\n",
      "look at how you can boost your custom models and training algorithms using Ten‐\n",
      "sorFlow’s automatic graph generation feature. But first, let’s take a quick tour of Ten‐\n",
      "sorFlow.\n",
      "\n",
      "367\n",
      "\n",
      "\fTensorFlow  2.0  was  released  in  March  2019,  making  TensorFlow\n",
      "much easier to use. The first edition of this book used TF 1, while\n",
      "this edition uses TF 2.\n",
      "\n",
      "A Quick Tour of TensorFlow\n",
      "As  you  know,  TensorFlow  is  a  powerful  library  for  numerical  computation,  particu‐\n",
      "larly well suited and fine-tuned for large-scale Machine Learning (but you could use\n",
      "it for anything else that requires heavy computations). It was developed by the Google\n",
      "Brain team and it powers many of Google’s large-scale services, such as Google Cloud\n",
      "Speech, Google Photos, and Google Search. It was open sourced in November 2015,\n",
      "and it is now the most popular deep learning library (in terms of citations in papers,\n",
      "adoption in companies, stars on github, etc.): countless projects use TensorFlow for\n",
      "all  sorts  of  Machine  Learning  tasks,  such  as  image  classification,  natural  language\n",
      "processing (NLP), recommender systems, time series forecasting, and much more.\n",
      "\n",
      "So what does TensorFlow actually offer? Here’s a summary:\n",
      "\n",
      "• Its core is very similar to NumPy, but with GPU support.\n",
      "\n",
      "• It also supports distributed computing (across multiple devices and servers).\n",
      "\n",
      "• It includes a kind of just-in-time (JIT) compiler that allows it to optimize compu‐\n",
      "tations  for  speed  and  memory  usage:  it  works  by  extracting  the  computation\n",
      "graph from a Python function, then optimizing it (e.g., by pruning unused nodes)\n",
      "and  finally  running  it  efficiently  (e.g.,  by  automatically  running  independent\n",
      "operations in parallel).\n",
      "\n",
      "• Computation  graphs  can  be  exported  to  a  portable  format,  so  you  can  train  a\n",
      "TensorFlow model in one environment (e.g., using Python on Linux), and run it\n",
      "in another (e.g., using Java on an Android device).\n",
      "\n",
      "• It  implements  autodiff  (see  Chapter  10  and  ???),  and  provides  some  excellent\n",
      "optimizers,  such  as  RMSProp,  Nadam  and  FTRL  (see  Chapter  11),  so  you  can\n",
      "easily minimize all sorts of loss functions.\n",
      "\n",
      "• TensorFlow  offers  many  more  features,  built  on  top  of  these  core  features:  the\n",
      "most important is of course tf.keras1, but it also has data loading & preprocessing\n",
      "ops  (tf.data,  tf.io,  etc.),  image  processing  ops  (tf.image),  signal  processing  ops\n",
      "(tf.signal),  and  more  (see  Figure  12-1  for  an  overview  of  TensorFlow’s  Python\n",
      "API).\n",
      "\n",
      "1 TensorFlow also includes another Deep Learning API called the Estimators API, but it is now recommended\n",
      "\n",
      "to use tf.keras instead.\n",
      "\n",
      "368 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      "\fFigure 12-1. TensorFlow’s Python API\n",
      "\n",
      "We  will  cover  many  of  the  packages  and  functions  of  the  Tensor‐\n",
      "Flow API, but it’s impossible to cover them all so you should really\n",
      "take some time to browse through the API: you will find that it is\n",
      "quite rich and well documented.\n",
      "\n",
      "At the lowest level, each TensorFlow operation is implemented using highly efficient\n",
      "C++ code2. Many operations (or ops for short) have multiple implementations, called\n",
      "kernels:  each  kernel  is  dedicated  to  a  specific  device  type,  such  as  CPUs,  GPUs,  or\n",
      "even TPUs (Tensor Processing Units). As you may know, GPUs can dramatically speed\n",
      "up  computations  by  splitting  computations  into  many  smaller  chunks  and  running\n",
      "them in parallel across many GPU threads. TPUs are even faster. You can purchase\n",
      "your own GPU devices (for now, TensorFlow only supports Nvidia cards with CUDA\n",
      "Compute  Capability  3.5+),  but  TPUs  are  only  available  on  Google  Cloud  Machine\n",
      "Learning Engine (see ???).3\n",
      "\n",
      "TensorFlow’s  architecture  is  shown  in  Figure  12-2:  most  of  the  time  your  code  will\n",
      "use the high level APIs (especially tf.keras and tf.data), but when you need more flexi‐\n",
      "bility  you  will  use  the  lower  level  Python  API,  handling  tensors  directly.  Note  that\n",
      "APIs  for  other  languages  are  also  available.  In  any  case,  TensorFlow’s  execution\n",
      "\n",
      "2 If you ever need to (but you probably won’t), you can write your own operations using the C++ API.\n",
      "\n",
      "3 If you are a researcher, you may be eligible to use these TPUs for free, see https://tensorflow.org/tfrc/ for more\n",
      "\n",
      "details.\n",
      "\n",
      "A Quick Tour of TensorFlow \n",
      "\n",
      "| \n",
      "\n",
      "369\n",
      "\n",
      "\fengine will take care of running the operations efficiently, even across multiple devi‐\n",
      "ces and machines if you tell it to.\n",
      "\n",
      "Figure 12-2. TensorFlow’s architecture\n",
      "\n",
      "TensorFlow runs not only on Windows, Linux, and MacOS, but also on mobile devi‐\n",
      "ces (using TensorFlow Lite), including both iOS and Android (see ???). If you do not\n",
      "want  to  use  the  Python  API,  there  are  also  C++,  Java,  Go  and  Swift  APIs.  There  is\n",
      "even  a  Javascript  implementation  called  TensorFlow.js  that  makes  it  possible  to  run\n",
      "your models directly in your browser.\n",
      "\n",
      "There’s  more  to  TensorFlow  than  just  the  library.  TensorFlow  is  at  the  center  of  an\n",
      "extensive  ecosystem  of  libraries.  First,  there’s  TensorBoard  for  visualization  (see\n",
      "Chapter 10). Next, there’s TensorFlow Extended (TFX), which is a set of libraries built\n",
      "by Google to productionize TensorFlow projects: it includes tools for data validation,\n",
      "preprocessing,  model  analysis  and  serving  (with  TF  Serving,  see  ???).  Google  also\n",
      "launched TensorFlow Hub, a way to easily download and reuse pretrained neural net‐\n",
      "works. You can also get many neural network architectures, some of them pretrained,\n",
      "in  TensorFlow’s  model  garden.  Check  out  the  TensorFlow  Resources,  or  https://\n",
      "github.com/jtoy/awesome-tensorflow  for  more  TensorFlow-based  projects.  You  will\n",
      "find hundreds of TensorFlow projects on GitHub, so it is often easy to find existing\n",
      "code for whatever you are trying to do.\n",
      "\n",
      "More and more ML papers are released along with their implemen‐\n",
      "tation,  and  sometimes  even  with  pretrained  models.  Check  out\n",
      "https://paperswithcode.com/ to easily find them.\n",
      "\n",
      "370 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      "\fLast but not least, TensorFlow has a dedicated team of passionate and helpful devel‐\n",
      "opers,  and  a  large  community  contributing  to  improving  it.  To  ask  technical  ques‐\n",
      "tions, you should use http://stackoverflow.com/ and tag your question with tensorflow\n",
      "and python. You can file bugs and feature requests through GitHub. For general dis‐\n",
      "cussions, join the Google group.\n",
      "\n",
      "Okay, it’s time to start coding!\n",
      "\n",
      "Using TensorFlow like NumPy\n",
      "TensorFlow’s API revolves around tensors, hence the name Tensor-Flow. A tensor is\n",
      "usually a multidimensional array (exactly like a NumPy ndarray), but it can also hold\n",
      "a scalar (a simple value, such as 42). These tensors will be important when we create\n",
      "custom  cost  functions,  custom  metrics,  custom  layers  and  more,  so  let’s  see  how  to\n",
      "create and manipulate them.\n",
      "\n",
      "Tensors and Operations\n",
      "You  can  easily  create  a  tensor,  using  tf.constant().  For  example,  here  is  a  tensor\n",
      "representing a matrix with two rows and three columns of floats:\n",
      "\n",
      ">>> tf.constant([[1., 2., 3.], [4., 5., 6.]]) # matrix\n",
      "<tf.Tensor: id=0, shape=(2, 3), dtype=float32, numpy=\n",
      "array([[1., 2., 3.],\n",
      "       [4., 5., 6.]], dtype=float32)>\n",
      ">>> tf.constant(42) # scalar\n",
      "<tf.Tensor: id=1, shape=(), dtype=int32, numpy=42>\n",
      "\n",
      "Just like an ndarray, a tf.Tensor has a shape and a data type (dtype):\n",
      "\n",
      ">>> t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
      ">>> t.shape\n",
      "TensorShape([2, 3])\n",
      ">>> t.dtype\n",
      "tf.float32\n",
      "\n",
      "Indexing works much like in NumPy:\n",
      "\n",
      ">>> t[:, 1:]\n",
      "<tf.Tensor: id=5, shape=(2, 2), dtype=float32, numpy=\n",
      "array([[2., 3.],\n",
      "       [5., 6.]], dtype=float32)>\n",
      ">>> t[..., 1, tf.newaxis]\n",
      "<tf.Tensor: id=15, shape=(2, 1), dtype=float32, numpy=\n",
      "array([[2.],\n",
      "       [5.]], dtype=float32)>\n",
      "\n",
      "Most importantly, all sorts of tensor operations are available:\n",
      "\n",
      ">>> t + 10\n",
      "<tf.Tensor: id=18, shape=(2, 3), dtype=float32, numpy=\n",
      "\n",
      "Using TensorFlow like NumPy \n",
      "\n",
      "| \n",
      "\n",
      "371\n",
      "\n",
      "\farray([[11., 12., 13.],\n",
      "       [14., 15., 16.]], dtype=float32)>\n",
      ">>> tf.square(t)\n",
      "<tf.Tensor: id=20, shape=(2, 3), dtype=float32, numpy=\n",
      "array([[ 1.,  4.,  9.],\n",
      "       [16., 25., 36.]], dtype=float32)>\n",
      ">>> t @ tf.transpose(t)\n",
      "<tf.Tensor: id=24, shape=(2, 2), dtype=float32, numpy=\n",
      "array([[14., 32.],\n",
      "       [32., 77.]], dtype=float32)>\n",
      "\n",
      "Note that writing t + 10 is equivalent to calling tf.add(t, 10) (indeed, Python calls\n",
      "the magic method t.__add__(10), which just calls tf.add(t, 10)). Other operators\n",
      "(like -, *, etc.) are also supported. The @ operator was added in Python 3.5, for matrix\n",
      "multiplication: it is equivalent to calling the tf.matmul() function.\n",
      "\n",
      "You will find all the basic math operations you need (e.g., tf.add(), tf.multiply(),\n",
      "tf.square(), tf.exp(), tf.sqrt()…), and more generally most operations that you\n",
      "can  find  in  NumPy  (e.g.,  tf.reshape(),  tf.squeeze(),  tf.tile()),  but  sometimes\n",
      "with a different name (e.g., tf.reduce_mean(), tf.reduce_sum(), tf.reduce_max(),\n",
      "tf.math.log() are the equivalent of np.mean(), np.sum(), np.max() and np.log()).\n",
      "When  the  name  differs,  there  is  often  a  good  reason  for  it:  for  example,  in  Tensor‐\n",
      "Flow you must write tf.transpose(t), you cannot just write t.T like in NumPy. The\n",
      "reason  is  that  it  does  not  do  exactly  the  same  thing:  in  TensorFlow,  a  new  tensor  is\n",
      "created with its own copy of the transposed data, while in NumPy, t.T is just a trans‐\n",
      "posed view on the same data. Similarly, the tf.reduce_sum() operation is named this\n",
      "way because its GPU kernel (i.e., GPU implementation) uses a reduce algorithm that\n",
      "does not guarantee the order in which the elements are added: because 32-bit floats\n",
      "have  limited  precision,  this  means  that  the  result  may  change  ever  so  slightly  every\n",
      "time  you  call  this  operation.  The  same  is  true  of  tf.reduce_mean()  (but  of  course\n",
      "tf.reduce_max() is deterministic).\n",
      "\n",
      "372 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      "\fMany  functions  and  classes  have  aliases.  For  example,  tf.add()\n",
      "and tf.math.add() are the same function. This allows TensorFlow\n",
      "to  have  concise  names  for  the  most  common  operations4,  while\n",
      "preserving well organized packages.\n",
      "\n",
      "Keras’ Low-Level API\n",
      "The  Keras  API  actually  has  its  own  low-level  API,  located  in  keras.backend.  It\n",
      "includes  functions  like  square(),  exp(),  sqrt()  and  so  on.  In  tf.keras,  these  func‐\n",
      "tions  generally  just  call  the  corresponding  TensorFlow  operations.  If  you  want  to\n",
      "write code that will be portable to other Keras implementations, you should use these\n",
      "Keras functions. However, they only cover a subset of all functions available in Ten‐\n",
      "sorFlow,  so  in  this  book  we  will  use  the  TensorFlow  operations  directly.  Here  is  as\n",
      "simple example using keras.backend, which is commonly named K for short:\n",
      "\n",
      ">>> from tensorflow import keras\n",
      ">>> K = keras.backend\n",
      ">>> K.square(K.transpose(t)) + 10\n",
      "<tf.Tensor: id=39, shape=(3, 2), dtype=float32, numpy=\n",
      "array([[11., 26.],\n",
      "       [14., 35.],\n",
      "       [19., 46.]], dtype=float32)>\n",
      "\n",
      "Tensors and NumPy\n",
      "Tensors play nice with NumPy: you can create a tensor from a NumPy array, and vice\n",
      "versa, and you can even apply TensorFlow operations to NumPy arrays and NumPy\n",
      "operations to tensors:\n",
      "\n",
      ">>> a = np.array([2., 4., 5.])\n",
      ">>> tf.constant(a)\n",
      "<tf.Tensor: id=111, shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>\n",
      ">>> t.numpy() # or np.array(t)\n",
      "array([[1., 2., 3.],\n",
      "       [4., 5., 6.]], dtype=float32)\n",
      ">>> tf.square(a)\n",
      "<tf.Tensor: id=116, shape=(3,), dtype=float64, numpy=array([4., 16., 25.])>\n",
      ">>> np.square(t)\n",
      "array([[ 1.,  4.,  9.],\n",
      "       [16., 25., 36.]], dtype=float32)\n",
      "\n",
      "4 A notable exception is tf.math.log() which is commonly used but there is no tf.log() alias (as it might be\n",
      "\n",
      "confused with logging).\n",
      "\n",
      "Using TensorFlow like NumPy \n",
      "\n",
      "| \n",
      "\n",
      "373\n",
      "\n",
      "\fNotice that NumPy uses 64-bit precision by default, while Tensor‐\n",
      "Flow uses 32-bit. This is because 32-bit precision is generally more\n",
      "than  enough  for  neural  networks,  plus  it  runs  faster  and  uses  less\n",
      "RAM. So when you create a tensor from a NumPy array, make sure\n",
      "to set dtype=tf.float32.\n",
      "\n",
      "Type Conversions\n",
      "Type  conversions  can  significantly  hurt  performance,  and  they  can  easily  go  unno‐\n",
      "ticed when they are done automatically. To avoid this, TensorFlow does not perform\n",
      "any type conversions automatically: it just raises an exception if you try to execute an\n",
      "operation  on  tensors  with  incompatible  types.  For  example,  you  cannot  add  a  float\n",
      "tensor and an integer tensor, and you cannot even add a 32-bit float and a 64-bit float:\n",
      "\n",
      ">>> tf.constant(2.) + tf.constant(40)\n",
      "Traceback[...]InvalidArgumentError[...]expected to be a float[...]\n",
      ">>> tf.constant(2.) + tf.constant(40., dtype=tf.float64)\n",
      "Traceback[...]InvalidArgumentError[...]expected to be a double[...]\n",
      "\n",
      "This may be a bit annoying at first, but remember that it’s for a good cause! And of\n",
      "course you can use tf.cast() when you really need to convert types:\n",
      "\n",
      ">>> t2 = tf.constant(40., dtype=tf.float64)\n",
      ">>> tf.constant(2.0) + tf.cast(t2, tf.float32)\n",
      "<tf.Tensor: id=136, shape=(), dtype=float32, numpy=42.0>\n",
      "\n",
      "Variables\n",
      "So  far,  we  have  used  constant  tensors:  as  their  name  suggests,  you  cannot  modify\n",
      "them. However, the weights in a neural network need to be tweaked by backpropaga‐\n",
      "tion,  and  other  parameters  may  also  need  to  change  over  time  (e.g.,  a  momentum\n",
      "optimizer keeps track of past gradients). What we need is a tf.Variable:\n",
      "\n",
      ">>> v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\n",
      ">>> v\n",
      "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
      "array([[1., 2., 3.],\n",
      "       [4., 5., 6.]], dtype=float32)>\n",
      "\n",
      "A  tf.Variable  acts  much  like  a  constant  tensor:  you  can  perform  the  same  opera‐\n",
      "tions with it, it plays nicely with NumPy as well, and it is just as picky with types. But\n",
      "it  can  also  be  modified  in  place  using  the  assign()  method  (or  assign_add()  or\n",
      "assign_sub()  which  increment  or  decrement  the  variable  by  the  given  value).  You\n",
      "can  also  modify  individual  cells  (or  slices),  using  the  cell’s  (or  slice’s)  assign()\n",
      "method  (direct  item  assignment  will  not  work),  or  using  the  scatter_update()  or\n",
      "scatter_nd_update() methods:\n",
      "\n",
      "v.assign(2 * v)           # => [[2., 4., 6.], [8., 10., 12.]]\n",
      "v[0, 1].assign(42)        # => [[2., 42., 6.], [8., 10., 12.]]\n",
      "\n",
      "374 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      "\fv[:, 2].assign([0., 1.])  # => [[2., 42., 0.], [8., 10., 1.]]\n",
      "v.scatter_nd_update(indices=[[0, 0], [1, 2]], updates=[100., 200.])\n",
      "                          # => [[100., 42., 0.], [8., 10., 200.]]\n",
      "\n",
      "In practice you will rarely have to create variables manually, since\n",
      "Keras provides an add_weight() method that will take care of it for\n",
      "you,  as  we  will  see.  Moreover,  model  parameters  will  generally  be\n",
      "updated  directly  by  the  optimizers,  so  you  will  rarely  need  to\n",
      "update variables manually.\n",
      "\n",
      "Other Data Structures\n",
      "TensorFlow supports several other data structures, including the following (please see\n",
      "the notebook or ??? for more details):\n",
      "\n",
      "• Sparse tensors (tf.SparseTensor) efficiently represent tensors containing mostly\n",
      "\n",
      "0s. The tf.sparse package contains operations for sparse tensors.\n",
      "\n",
      "• Tensor  arrays  (tf.TensorArray)  are  lists  of  tensors.  They  have  a  fixed  size  by\n",
      "default, but can optionally be made dynamic. All tensors they contain must have\n",
      "the same shape and data type.\n",
      "\n",
      "• Ragged tensors (tf.RaggedTensor) represent static lists of lists of tensors, where\n",
      "every tensor has the same shape and data type. The tf.ragged package contains\n",
      "operations for ragged tensors.\n",
      "\n",
      "• String tensors are regular tensors of type tf.string. These actually represent byte\n",
      "strings,  not  Unicode  strings,  so  if  you  create  a  string  tensor  using  a  Unicode\n",
      "string  (e.g.,  a  regular  Python  3  string  like  \"café\"`),  then  it  will  get  encoded  to\n",
      "UTF-8  automatically  (e.g.,  b\"caf\\xc3\\xa9\").  Alternatively,  you  can  represent\n",
      "Unicode  strings  using  tensors  of  type  tf.int32,  where  each  item  represents  a\n",
      "Unicode codepoint (e.g., [99, 97, 102, 233]). The tf.strings package (with\n",
      "an s) contains ops for byte strings and Unicode strings (and to convert one into\n",
      "the other).\n",
      "\n",
      "• Sets are just represented as regular tensors (or sparse tensors) containing one or\n",
      "more  sets,  and  you  can  manipulate  them  using  operations  from  the  tf.sets\n",
      "package.\n",
      "\n",
      "• Queues, including First In, First Out (FIFO) queues (FIFOQueue), queues that can\n",
      "prioritize  some  items  (PriorityQueue),  queues  that  shuffle  their  items  (Random\n",
      "ShuffleQueue), and queues that can batch items of different shapes by padding\n",
      "(PaddingFIFOQueue). These classes are all in the tf.queue package.\n",
      "\n",
      "With tensors, operations, variables and various data structures at your disposal, you\n",
      "are now ready to customize your models and training algorithms!\n",
      "\n",
      "Using TensorFlow like NumPy \n",
      "\n",
      "| \n",
      "\n",
      "375\n",
      "\n",
      "\fCustomizing Models and Training Algorithms\n",
      "Let’s start by creating a custom loss function, which is a simple and common use case.\n",
      "\n",
      "Custom Loss Functions\n",
      "Suppose you want to train a regression model, but your training set is a bit noisy. Of\n",
      "course, you start by trying to clean up your dataset by removing or fixing the outliers,\n",
      "but it turns out to be insufficient, the dataset is still noisy. Which loss function should\n",
      "you  use?  The  mean  squared  error  might  penalize  large  errors  too  much,  so  your\n",
      "model will end up being imprecise. The mean absolute error would not penalize out‐\n",
      "liers  as  much,  but  training  might  take  a  while  to  converge  and  the  trained  model\n",
      "might not be very precise. This is probably a good time to use the Huber loss (intro‐\n",
      "duced in Chapter 10) instead of the good old MSE. The Huber loss is not currently\n",
      "part of the official Keras API, but it is available in tf.keras (just use an instance of the\n",
      "keras.losses.Huber class). But let’s pretend it’s not there: implementing it is easy as\n",
      "pie! Just create a function that takes the labels and predictions as arguments, and use\n",
      "TensorFlow operations to compute every instance’s loss:\n",
      "\n",
      "def huber_fn(y_true, y_pred):\n",
      "    error = y_true - y_pred\n",
      "    is_small_error = tf.abs(error) < 1\n",
      "    squared_loss = tf.square(error) / 2\n",
      "    linear_loss  = tf.abs(error) - 0.5\n",
      "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
      "\n",
      "For  better  performance,  you  should  use  a  vectorized  implementa‐\n",
      "tion, as in this example. Moreover, if you want to benefit from Ten‐\n",
      "sorFlow’s  graph  features,  you  should  use  only  TensorFlow\n",
      "operations.\n",
      "\n",
      "It  is  also  preferable  to  return  a  tensor  containing  one  loss  per  instance,  rather  than\n",
      "returning  the  mean  loss.  This  way,  Keras  can  apply  class  weights  or  sample  weights\n",
      "when requested (see Chapter 10).\n",
      "\n",
      "Next, you can just use this loss when you compile the Keras model, then train your\n",
      "model:\n",
      "\n",
      "model.compile(loss=huber_fn, optimizer=\"nadam\")\n",
      "model.fit(X_train, y_train, [...])\n",
      "\n",
      "And that’s it! For each batch during training, Keras will call the huber_fn() function\n",
      "to compute the loss, and use it to perform a Gradient Descent step. Moreover, it will\n",
      "keep  track  of  the  total  loss  since  the  beginning  of  the  epoch,  and  it  will  display  the\n",
      "mean loss.\n",
      "\n",
      "376 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      "\fBut what happens to this custom loss when we save the model?\n",
      "\n",
      "Saving and Loading Models That Contain Custom Components\n",
      "Saving a model containing a custom loss function actually works fine, as Keras just\n",
      "saves the name of the function. However, whenever you load it, you need to provide a\n",
      "dictionary that maps the function name to the actual function. More generally, when\n",
      "you  load  a  model  containing  custom  objects,  you  need  to  map  the  names  to  the\n",
      "objects:\n",
      "\n",
      "model = keras.models.load_model(\"my_model_with_a_custom_loss.h5\",\n",
      "                                custom_objects={\"huber_fn\": huber_fn})\n",
      "\n",
      "With the current implementation, any error between -1 and 1 is considered “small”.\n",
      "But  what  if  we  want  a  different  threshold?  One  solution  is  to  create  a  function  that\n",
      "creates a configured loss function:\n",
      "\n",
      "def create_huber(threshold=1.0):\n",
      "    def huber_fn(y_true, y_pred):\n",
      "        error = y_true - y_pred\n",
      "        is_small_error = tf.abs(error) < threshold\n",
      "        squared_loss = tf.square(error) / 2\n",
      "        linear_loss  = threshold * tf.abs(error) - threshold**2 / 2\n",
      "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
      "    return huber_fn\n",
      "\n",
      "model.compile(loss=create_huber(2.0), optimizer=\"nadam\")\n",
      "\n",
      "Unfortunately, when you save the model, the threshold will not be saved. This means\n",
      "that you will have to specify the threshold value when loading the model (note that\n",
      "the name to use is \"huber_fn\", which is the name of the function we gave Keras, not\n",
      "the name of the function that created it):\n",
      "\n",
      "model = keras.models.load_model(\"my_model_with_a_custom_loss_threshold_2.h5\",\n",
      "                                custom_objects={\"huber_fn\": create_huber(2.0)})\n",
      "\n",
      "You can solve this by creating a subclass of the keras.losses.Loss class, and imple‐\n",
      "ment its get_config() method:\n",
      "\n",
      "class HuberLoss(keras.losses.Loss):\n",
      "    def __init__(self, threshold=1.0, **kwargs):\n",
      "        self.threshold = threshold\n",
      "        super().__init__(**kwargs)\n",
      "    def call(self, y_true, y_pred):\n",
      "        error = y_true - y_pred\n",
      "        is_small_error = tf.abs(error) < self.threshold\n",
      "        squared_loss = tf.square(error) / 2\n",
      "        linear_loss  = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
      "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
      "    def get_config(self):\n",
      "        base_config = super().get_config()\n",
      "        return {**base_config, \"threshold\": self.threshold}\n",
      "\n",
      "Customizing Models and Training Algorithms \n",
      "\n",
      "| \n",
      "\n",
      "377\n",
      "\n",
      "\fThe Keras API only specifies how to use subclassing to define lay‐\n",
      "ers, models, callbacks, and regularizers. If you build other compo‐\n",
      "nents  (such  as  losses,  metrics,  initializers  or  constraints)  using\n",
      "subclassing, they may not be portable to other Keras implementa‐\n",
      "tions.\n",
      "\n",
      "Let’s walk through this code:\n",
      "\n",
      "• The  constructor  accepts  **kwargs  and  passes  them  to  the  parent  constructor,\n",
      "which handles standard hyperparameters: the name of the loss and the reduction\n",
      "algorithm  to  use  to  aggregate  the  individual  instance  losses.  By  default,  it  is\n",
      "\"sum_over_batch_size\",  which  means  that  the  loss  will  be  the  sum  of  the\n",
      "instance losses, possibly weighted by the sample weights, if any, and then divide\n",
      "the result by the batch size (not by the sum of weights, so this is not the weighted\n",
      "mean).5. Other possible values are \"sum\" and None.\n",
      "\n",
      "• The  call()  method  takes  the  labels  and  predictions,  computes  all  the  instance\n",
      "\n",
      "losses, and returns them.\n",
      "\n",
      "• The  get_config()  method  returns  a  dictionary  mapping  each  hyperparameter\n",
      "name to its value. It first calls the parent class’s get_config() method, then adds\n",
      "the new hyperparameters to this dictionary (note that the convenient {**x} syn‐\n",
      "tax was added in Python 3.5).\n",
      "\n",
      "You can then use any instance of this class when you compile the model:\n",
      "\n",
      "model.compile(loss=HuberLoss(2.), optimizer=\"nadam\")\n",
      "\n",
      "When you save the model, the threshold will be saved along with it, and when you\n",
      "load the model you just need to map the class name to the class itself:\n",
      "\n",
      "model = keras.models.load_model(\"my_model_with_a_custom_loss_class.h5\",\n",
      "                                custom_objects={\"HuberLoss\": HuberLoss})\n",
      "\n",
      "When  you  save  a  model,  Keras  calls  the  loss  instance’s  get_config()  method  and\n",
      "saves  the  config  as  JSON  in  the  HDF5  file.  When  you  load  the  model,  it  calls  the\n",
      "from_config() class method on the HuberLoss class: this method is implemented by\n",
      "the base class (Loss) and just creates an instance of the class, passing **config to the\n",
      "constructor.\n",
      "\n",
      "That’s it for losses! It was not too hard, was it? Well it’s just as simple for custom acti‐\n",
      "vation functions, initializers, regularizers, and constraints. Let’s look at these now.\n",
      "\n",
      "5 It would not be a good idea to use a weighted mean: if we did, then two instances with the same weight but in\n",
      "\n",
      "different batches would have a different impact on training, depending on the total weight of each batch.\n",
      "\n",
      "378 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      "\fCustom Activation Functions, Initializers, Regularizers, and\n",
      "Constraints\n",
      "Most Keras functionalities, such as losses, regularizers, constraints, initializers, met‐\n",
      "rics, activation functions, layers and even full models can be customized in very much\n",
      "the same way. Most of the time, you will just need to write a simple function, with the\n",
      "appropriate inputs and outputs. For example, here are examples of a custom activa‐\n",
      "tion  function  (equivalent  to  keras.activations.softplus  or  tf.nn.softplus),  a\n",
      "custom Glorot initializer (equivalent to keras.initializers.glorot_normal), a cus‐\n",
      "tom ℓ1 regularizer (equivalent to keras.regularizers.l1(0.01)) and a custom con‐\n",
      "straint \n",
      "to\n",
      "are \n",
      "ensures \n",
      "keras.constraints.nonneg() or tf.nn.relu):\n",
      "\n",
      "(equivalent \n",
      "\n",
      "positive \n",
      "\n",
      "weights \n",
      "\n",
      "that \n",
      "\n",
      "all \n",
      "\n",
      "def my_softplus(z): # return value is just tf.nn.softplus(z)\n",
      "    return tf.math.log(tf.exp(z) + 1.0)\n",
      "\n",
      "def my_glorot_initializer(shape, dtype=tf.float32):\n",
      "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
      "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
      "\n",
      "def my_l1_regularizer(weights):\n",
      "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
      "\n",
      "def my_positive_weights(weights): # return value is just tf.nn.relu(weights)\n",
      "    return tf.where(weights < 0., tf.zeros_like(weights), weights)\n",
      "\n",
      "As you can see, the arguments depend on the type of custom function. These custom\n",
      "functions can then be used normally, for example:\n",
      "\n",
      "layer = keras.layers.Dense(30, activation=my_softplus,\n",
      "                           kernel_initializer=my_glorot_initializer,\n",
      "                           kernel_regularizer=my_l1_regularizer,\n",
      "                           kernel_constraint=my_positive_weights)\n",
      "\n",
      "The activation function will be applied to the output of this Dense layer, and its result\n",
      "will  be  passed  on  to  the  next  layer.  The  layer’s  weights  will  be  initialized  using  the\n",
      "value returned by the initializer. At each training step the weights will be passed to the\n",
      "regularization function to compute the regularization loss, which will be added to the\n",
      "main loss to get the final loss used for training. Finally, the constraint function will be\n",
      "called  after  each  training  step,  and  the  layer’s  weights  will  be  replaced  by  the  con‐\n",
      "strained weights.\n",
      "\n",
      "If a function has some hyperparameters that need to be saved along with the model,\n",
      "then  you  will  want  to  subclass  the  appropriate  class,  such  as  keras.regulariz\n",
      "ers.Regularizer,  keras.constraints.Constraint,  keras.initializers.Initial\n",
      "izer  or  keras.layers.Layer  (for  any  layer,  including  activation  functions).  For\n",
      "example, much like we did for the custom loss, here is a simple class for ℓ1 regulariza‐\n",
      "\n",
      "Customizing Models and Training Algorithms \n",
      "\n",
      "| \n",
      "\n",
      "379\n",
      "\n",
      "\ftion, that saves its factor hyperparameter (this time we do not need to call the parent\n",
      "constructor or the get_config() method, as they are not defined by the parent class):\n",
      "\n",
      "class MyL1Regularizer(keras.regularizers.Regularizer):\n",
      "    def __init__(self, factor):\n",
      "        self.factor = factor\n",
      "    def __call__(self, weights):\n",
      "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
      "    def get_config(self):\n",
      "        return {\"factor\": self.factor}\n",
      "\n",
      "Note that you must implement the call() method for losses, layers (including activa‐\n",
      "tion  functions)  and  models,  or  the  __call__()  method  for  regularizers,  initializers\n",
      "and constraints. For metrics, things are a bit different, as we will see now.\n",
      "\n",
      "Custom Metrics\n",
      "Losses and metrics are conceptually not the same thing: losses are used by Gradient\n",
      "Descent to train a model, so they must be differentiable (at least where they are evalu‐\n",
      "ated)  and  their  gradients  should  not  be  0  everywhere.  Plus,  it’s  okay  if  they  are  not\n",
      "easily  interpretable  by  humans  (e.g.  cross-entropy).  In  contrast,  metrics  are  used  to\n",
      "evaluate  a  model,  they  must  be  more  easily  interpretable,  and  they  can  be  non-\n",
      "differentiable or have 0 gradients everywhere (e.g., accuracy).\n",
      "\n",
      "That  said,  in  most  cases,  defining  a  custom  metric  function  is  exactly  the  same  as\n",
      "defining a custom loss function. In fact, we could even use the Huber loss function we\n",
      "created earlier as a metric6, it would work just fine (and persistence would also work\n",
      "the same way, in this case only saving the name of the function, \"huber_fn\"):\n",
      "\n",
      "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[create_huber(2.0)])\n",
      "\n",
      "For each batch during training, Keras will compute this metric and keep track of its\n",
      "mean  since  the  beginning  of  the  epoch.  Most  of  the  time,  this  is  exactly  what  you\n",
      "want. But not always! Consider a binary classifier’s precision, for example. As we saw\n",
      "in Chapter 3, precision is the number of true positives divided by the number of posi‐\n",
      "tive predictions (including both true positives and false positives). Suppose the model\n",
      "made 5 positive predictions in the first batch, 4 of which were correct: that’s 80% pre‐\n",
      "cision. Then suppose the model made 3 positive predictions in the second batch, but\n",
      "they were all incorrect: that’s 0% precision for the second batch. If you just compute\n",
      "the mean of these two precisions, you get 40%. But wait a second, this is not the mod‐\n",
      "el’s precision over these two batches! Indeed, there were a total of 4 true positives (4 +\n",
      "0) out of 8 positive predictions (5 + 3), so the overall precision is 50%, not 40%. What\n",
      "we need is an object that can keep track of the number of true positives and the num‐\n",
      "\n",
      "6 However, the Huber loss is seldom used as a metric (the MAE or MSE are preferred).\n",
      "\n",
      "380 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      "\fber of false positives, and compute their ratio when requested. This is precisely what\n",
      "the keras.metrics.Precision class does:\n",
      "\n",
      ">>> precision = keras.metrics.Precision()\n",
      ">>> precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])\n",
      "<tf.Tensor: id=581729, shape=(), dtype=float32, numpy=0.8>\n",
      ">>> precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])\n",
      "<tf.Tensor: id=581780, shape=(), dtype=float32, numpy=0.5>\n",
      "\n",
      "In this example, we created a Precision object, then we used it like a function, pass‐\n",
      "ing  it  the  labels  and  predictions  for  the  first  batch,  then  for  the  second  batch  (note\n",
      "that  we  could  also  have  passed  sample  weights).  We  used  the  same  number  of  true\n",
      "and false positives as in the example we just discussed. After the first batch, it returns\n",
      "the precision of 80%, then after the second batch it returns 50% (which is the overall\n",
      "precision so far, not the second batch’s precision). This is called a streaming metric (or\n",
      "stateful metric), as it is gradually updated, batch after batch.\n",
      "\n",
      "At any point, we can call the result() method to get the current value of the metric.\n",
      "We  can  also  look  at  its  variables  (tracking  the  number  of  true  and  false  positives)\n",
      "using  the  variables  attribute,  and  reset  these  variables  using  the  reset_states()\n",
      "method:\n",
      "\n",
      ">>> p.result()\n",
      "<tf.Tensor: id=581794, shape=(), dtype=float32, numpy=0.5>\n",
      ">>> p.variables\n",
      "[<tf.Variable 'true_positives:0' [...] numpy=array([4.], dtype=float32)>,\n",
      " <tf.Variable 'false_positives:0' [...] numpy=array([4.], dtype=float32)>]\n",
      ">>> p.reset_states() # both variables get reset to 0.0\n",
      "\n",
      "If  you  need  to  create  such  a  streaming  metric,  you  can  just  create  a  subclass  of  the\n",
      "keras.metrics.Metric  class.  Here  is  a  simple  example  that  keeps  track  of  the  total\n",
      "Huber  loss  and  the  number  of  instances  seen  so  far.  When  asked  for  the  result,  it\n",
      "returns the ratio, which is simply the mean Huber loss:\n",
      "\n",
      "class HuberMetric(keras.metrics.Metric):\n",
      "    def __init__(self, threshold=1.0, **kwargs):\n",
      "        super().__init__(**kwargs) # handles base args (e.g., dtype)\n",
      "        self.threshold = threshold\n",
      "        self.huber_fn = create_huber(threshold)\n",
      "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
      "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
      "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
      "        metric = self.huber_fn(y_true, y_pred)\n",
      "        self.total.assign_add(tf.reduce_sum(metric))\n",
      "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
      "    def result(self):\n",
      "        return self.total / self.count\n",
      "    def get_config(self):\n",
      "        base_config = super().get_config()\n",
      "        return {**base_config, \"threshold\": self.threshold}\n",
      "\n",
      "Customizing Models and Training Algorithms \n",
      "\n",
      "| \n",
      "\n",
      "381\n",
      "\n",
      "\fLet’s walk through this code:7:\n",
      "\n",
      "• The constructor uses the add_weight() method to create the variables needed to\n",
      "keep track of the metric’s state over multiple batches, in this case the sum of all\n",
      "Huber losses (total) and the number of instances seen so far (count). You could\n",
      "just create variables manually if you preferred. Keras tracks any tf.Variable that\n",
      "is set as an attribute (and more generally, any “trackable” object, such as layers or\n",
      "models).\n",
      "\n",
      "• The update_state() method is called when you use an instance of this class as a\n",
      "function (as we did with the Precision object). It updates the variables given the\n",
      "labels and predictions for one batch (and sample weights, but in this case we just\n",
      "ignore them).\n",
      "\n",
      "• The result() method computes and returns the final result, in this case just the\n",
      "mean Huber metric over all instances. When you use the metric as a function, the\n",
      "update_state()  method  gets  called  first,  then  the  result()  method  is  called,\n",
      "and its output is returned.\n",
      "\n",
      "• We  also  implement  the  get_config()  method  to  ensure  the  threshold  gets\n",
      "\n",
      "saved along with the model.\n",
      "\n",
      "• The  default  implementation  of  the  reset_states()  method  just  resets  all  vari‐\n",
      "\n",
      "ables to 0.0 (but you can override it if needed).\n",
      "\n",
      "Keras will take care of variable persistence seamlessly, no action is\n",
      "required.\n",
      "\n",
      "When  you  define  a  metric  using  a  simple  function,  Keras  automatically  calls  it  for\n",
      "each batch, and it keeps track of the mean during each epoch, just like we did man‐\n",
      "ually. So the only benefit of our HuberMetric class is that the threshold will be saved.\n",
      "But of course, some metrics, like precision, cannot simply be averaged over batches:\n",
      "in thoses cases, there’s no other option than to implement a streaming metric.\n",
      "\n",
      "Now that we have built a streaming metric, building a custom layer will seem like a\n",
      "walk in the park!\n",
      "\n",
      "7 This class is for illustration purposes only. A simpler and better implementation would just subclass the\n",
      "\n",
      "keras.metrics.Mean class, see the notebook for an example.\n",
      "\n",
      "382 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      "\fCustom Layers\n",
      "You may occasionally want to build an architecture that contains an exotic layer for\n",
      "which TensorFlow does not provide a default implementation. In this case, you will\n",
      "need  to  create  a  custom  layer.  Or  sometimes  you  may  simply  want  to  build  a  very\n",
      "repetitive architecture, containing identical blocks of layers repeated many times, and\n",
      "it would be convenient to treat each block of layers as a single layer. For example, if\n",
      "the model is a sequence of layers A, B, C, A, B, C, A, B, C, then you might want to\n",
      "define a custom layer D containing layers A, B, C, and your model would then simply\n",
      "be D, D, D. Let’s see how to build custom layers.\n",
      "\n",
      "First,  some  layers  have  no  weights,  such  as  keras.layers.Flatten  or  keras.lay\n",
      "ers.ReLU.  If  you  want  to  create  a  custom  layer  without  any  weights,  the  simplest\n",
      "option is to write a function and wrap it in a keras.layers.Lambda layer. For exam‐\n",
      "ple, the following layer will apply the exponential function to its inputs:\n",
      "\n",
      "exponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))\n",
      "\n",
      "This custom layer can then be used like any other layer, using the sequential API, the\n",
      "functional API, or the subclassing API. You can also use it as an activation function\n",
      "(or you could just use activation=tf.exp, or activation=keras.activations.expo\n",
      "nential, or simply activation=\"exponential\"). The exponential layer is sometimes\n",
      "used in the output layer of a regression model when the values to predict have very\n",
      "different scales (e.g., 0.001, 10., 1000.).\n",
      "\n",
      "As  you  probably  guessed  by  now,  to  build  a  custom  stateful  layer  (i.e.,  a  layer  with\n",
      "weights), you need to create a subclass of the keras.layers.Layer class. For exam‐\n",
      "ple, the following class implements a simplified version of the Dense layer:\n",
      "\n",
      "class MyDense(keras.layers.Layer):\n",
      "    def __init__(self, units, activation=None, **kwargs):\n",
      "        super().__init__(**kwargs)\n",
      "        self.units = units\n",
      "        self.activation = keras.activations.get(activation)\n",
      "\n",
      "    def build(self, batch_input_shape):\n",
      "        self.kernel = self.add_weight(\n",
      "            name=\"kernel\", shape=[batch_input_shape[-1], self.units],\n",
      "            initializer=\"glorot_normal\")\n",
      "        self.bias = self.add_weight(\n",
      "            name=\"bias\", shape=[self.units], initializer=\"zeros\")\n",
      "        super().build(batch_input_shape) # must be at the end\n",
      "\n",
      "    def call(self, X):\n",
      "        return self.activation(X @ self.kernel + self.bias)\n",
      "\n",
      "    def compute_output_shape(self, batch_input_shape):\n",
      "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
      "\n",
      "Customizing Models and Training Algorithms \n",
      "\n",
      "| \n",
      "\n",
      "383\n",
      "\n",
      "\f    def get_config(self):\n",
      "        base_config = super().get_config()\n",
      "        return {**base_config, \"units\": self.units,\n",
      "                \"activation\": keras.activations.serialize(self.activation)}\n",
      "\n",
      "Let’s walk through this code:\n",
      "\n",
      "• The constructor takes all the hyperparameters as arguments (in this example just\n",
      "units  and  activation),  and  importantly  it  also  takes  a  **kwargs  argument.  It\n",
      "calls  the  parent  constructor,  passing  it  the  kwargs:  this  takes  care  of  standard\n",
      "arguments such as  input_shape,  trainable,  name, and so on. Then it saves the\n",
      "hyperparameters  as  attributes,  converting  the  activation  argument  to  the\n",
      "appropriate activation function using the keras.activations.get() function (it\n",
      "accepts functions, standard strings like \"relu\" or \"selu\", or simply None)8.\n",
      "\n",
      "• The  build()  method’s  role  is  to  create  the  layer’s  variables,  by  calling  the\n",
      "add_weight()  method  for  each  weight.  The  build()  method  is  called  the  first\n",
      "time  the  layer  is  used.  At  that  point,  Keras  will  know  the  shape  of  this  layer’s\n",
      "inputs, and it will pass it to the build() method9, which is often necessary to cre‐\n",
      "ate some of the weights. For example, we need to know the number of neurons in\n",
      "the previous layer in order to create the connection weights matrix (i.e., the \"ker\n",
      "nel\"): this corresponds to the size of the last dimension of the inputs. At the end\n",
      "of the build() method (and only at the end), you must call the parent’s build()\n",
      "method: this tells Keras that the layer is built (it just sets self.built = True).\n",
      "• The  call()  method  actually  performs  the  desired  operations.  In  this  case,  we\n",
      "compute the matrix multiplication of the inputs X and the layer’s kernel, we add\n",
      "the bias vector, we apply the activation function to the result, and this gives us the\n",
      "output of the layer.\n",
      "\n",
      "• The  compute_output_shape()  method  simply  returns  the  shape  of  this  layer’s\n",
      "outputs. In this case, it is the same shape as the inputs, except the last dimension\n",
      "is replaced with the number of neurons in the layer. Note that in tf.keras, shapes\n",
      "are instances of the tf.TensorShape class, which you can convert to Python lists\n",
      "using as_list().\n",
      "\n",
      "• The  get_config()  method  is  just  like  earlier.  Note  that  we  save  the  activation\n",
      "\n",
      "function’s full configuration by calling keras.activations.serialize().\n",
      "\n",
      "You can now use a MyDense layer just like any other layer!\n",
      "\n",
      "8 This function is specific to tf.keras. You could use keras.activations.Activation instead.\n",
      "9 The Keras API calls this argument input_shape, but since it also includes the batch dimension, I prefer to call\n",
      "\n",
      "it batch_input_shape. Same for compute_output_shape().\n",
      "\n",
      "384 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      "\fYou  can  generally  omit  the  compute_output_shape()  method,  as\n",
      "tf.keras  automatically  infers  the  output  shape,  except  when  the\n",
      "layer is dynamic (as we will see shortly). In other Keras implemen‐\n",
      "tations, this method is either required or by default it assumes the\n",
      "output shape is the same as the input shape.\n",
      "\n",
      "To create a layer with multiple inputs (e.g., Concatenate), the argument to the call()\n",
      "method should be a tuple containing all the inputs, and similarly the argument to the\n",
      "compute_output_shape()  method  should  be  a  tuple  containing  each  input’s  batch\n",
      "shape. To create a layer with multiple outputs, the call() method should return the\n",
      "list of outputs, and the compute_output_shape() should return the list of batch out‐\n",
      "put  shapes  (one  per  output).  For  example,  the  following  toy  layer  takes  two  inputs\n",
      "and returns three outputs:\n",
      "\n",
      "class MyMultiLayer(keras.layers.Layer):\n",
      "    def call(self, X):\n",
      "        X1, X2 = X\n",
      "        return [X1 + X2, X1 * X2, X1 / X2]\n",
      "\n",
      "    def compute_output_shape(self, batch_input_shape):\n",
      "        b1, b2 = batch_input_shape\n",
      "        return [b1, b1, b1] # should probably handle broadcasting rules\n",
      "\n",
      "This layer may now be used like any other layer, but of course only using the func‐\n",
      "tional and subclassing APIs, not the sequential API (which only accepts layers with\n",
      "one input and one output).\n",
      "\n",
      "If  your  layer  needs  to  have  a  different  behavior  during  training  and  during  testing\n",
      "(e.g., if it uses  Dropout or  BatchNormalization layers), then you must add a  train\n",
      "ing argument to the call() method and use this argument to decide what to do. For\n",
      "example, let’s create a layer that adds Gaussian noise during training (for regulariza‐\n",
      "tion), but does nothing during testing (Keras actually has a layer that does the same\n",
      "thing: keras.layers.GaussianNoise):\n",
      "\n",
      "class MyGaussianNoise(keras.layers.Layer):\n",
      "    def __init__(self, stddev, **kwargs):\n",
      "        super().__init__(**kwargs)\n",
      "        self.stddev = stddev\n",
      "\n",
      "    def call(self, X, training=None):\n",
      "        if training:\n",
      "            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n",
      "            return X + noise\n",
      "        else:\n",
      "            return X\n",
      "\n",
      "    def compute_output_shape(self, batch_input_shape):\n",
      "        return batch_input_shape\n",
      "\n",
      "Customizing Models and Training Algorithms \n",
      "\n",
      "| \n",
      "\n",
      "385\n",
      "\n",
      "\fWith  that,  you  can  now  build  any  custom  layer  you  need!  Now  let’s  create  custom\n",
      "models.\n",
      "\n",
      "Custom Models\n",
      "We already looked at custom model classes in Chapter 10 when we discussed the sub‐\n",
      "classing  API.10  It  is  actually  quite  straightforward,  just  subclass  the  keras.mod\n",
      "els.Model  class,  create  layers  and  variables  in  the  constructor,  and  implement  the\n",
      "call() method to do whatever you want the model to do. For example, suppose you\n",
      "want to build the model represented in Figure 12-3:\n",
      "\n",
      "Figure 12-3. Custom Model Example\n",
      "\n",
      "The inputs go through a first dense layer, then through a residual block composed of\n",
      "two dense layers and an addition operation (as we will see in Chapter 14, a residual\n",
      "block  adds  its  inputs  to  its  outputs),  then  through  this  same  residual  block  3  more\n",
      "times, then through a second residual block, and the final result goes through a dense\n",
      "output layer. Note that this model does not make much sense, it’s just an example to\n",
      "illustrate the fact that you can easily build any kind of model you want, even contain‐\n",
      "\n",
      "10 The name “subclassing API” usually refers only to the creation of custom models by subclassing, although\n",
      "\n",
      "many other things can be created by subclassing, as we saw in this chapter.\n",
      "\n",
      "386 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      "\fing  loops  and  skip  connections.  To  implement  this  model,  it  is  best  to  first  create  a\n",
      "ResidualBlock layer, since we are going to create a couple identical blocks (and we\n",
      "might want to reuse it in another model):\n",
      "\n",
      "class ResidualBlock(keras.layers.Layer):\n",
      "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
      "        super().__init__(**kwargs)\n",
      "        self.hidden = [keras.layers.Dense(n_neurons, activation=\"elu\",\n",
      "                                          kernel_initializer=\"he_normal\")\n",
      "                       for _ in range(n_layers)]\n",
      "\n",
      "    def call(self, inputs):\n",
      "        Z = inputs\n",
      "        for layer in self.hidden:\n",
      "            Z = layer(Z)\n",
      "        return inputs + Z\n",
      "\n",
      "This layer is a bit special since it contains other layers. This is handled transparently\n",
      "by Keras: it automatically detects that the hidden attribute contains trackable objects\n",
      "(layers  in  this  case),  so  their  variables  are  automatically  added  to  this  layer’s  list  of\n",
      "variables. The rest of this class is self-explanatory. Next, let’s use the subclassing API\n",
      "to define the model itself:\n",
      "\n",
      "class ResidualRegressor(keras.models.Model):\n",
      "    def __init__(self, output_dim, **kwargs):\n",
      "        super().__init__(**kwargs)\n",
      "        self.hidden1 = keras.layers.Dense(30, activation=\"elu\",\n",
      "                                          kernel_initializer=\"he_normal\")\n",
      "        self.block1 = ResidualBlock(2, 30)\n",
      "        self.block2 = ResidualBlock(2, 30)\n",
      "        self.out = keras.layers.Dense(output_dim)\n",
      "\n",
      "    def call(self, inputs):\n",
      "        Z = self.hidden1(inputs)\n",
      "        for _ in range(1 + 3):\n",
      "            Z = self.block1(Z)\n",
      "        Z = self.block2(Z)\n",
      "        return self.out(Z)\n",
      "\n",
      "We  create  the  layers  in  the  constructor,  and  use  them  in  the  call()  method.  This\n",
      "model can then be used like any other model (compile it, fit it, evaluate it and use it to\n",
      "make  predictions).  If  you  also  want  to  be  able  to  save  the  model  using  the  save()\n",
      "method,  and  load  it  using  the  keras.models.load_model()  function,  you  must\n",
      "implement the get_config() method (as we did earlier) in both the ResidualBlock\n",
      "class and the ResidualRegressor class. Alternatively, you can just save and load the\n",
      "weights using the save_weights() and load_weights() methods.\n",
      "\n",
      "The Model class is actually a subclass of the Layer class, so models can be defined and\n",
      "used exactly like layers. But a model also has some extra functionalities, including of\n",
      "course  its  compile(),  fit(),  evaluate()  and  predict()  methods  (and  a  few  var‐\n",
      "\n",
      "Customizing Models and Training Algorithms \n",
      "\n",
      "| \n",
      "\n",
      "387\n",
      "\n",
      "\fiants,  such  as  train_on_batch()  or  fit_generator()),  plus  the  get_layers()\n",
      "method  (which  can  return  any  of  the  model’s  layers  by  name  or  by  index),  and  the\n",
      "save()  method  (and  support  for  keras.models.load_model()  and  keras.mod\n",
      "els.clone_model()). So if models provide more functionalities than layers, why not\n",
      "just  define  every  layer  as  a  model?  Well,  technically  you  could,  but  it  is  probably\n",
      "cleaner  to  distinguish  the  internal  components  of  your  model  (layers  or  reusable\n",
      "blocks  of  layers)  from  the  model  itself.  The  former  should  subclass  the  Layer  class,\n",
      "while the latter should subclass the Model class.\n",
      "\n",
      "With that, you can quite naturally and concisely build almost any model that you find\n",
      "in a paper, either using the sequential API, the functional API, the subclassing API, or\n",
      "even a mix of these. “Almost” any model? Yes, there are still a couple things that we\n",
      "need to look at: first, how to define losses or metrics based on model internals, and\n",
      "second how to build a custom training loop.\n",
      "\n",
      "Losses and Metrics Based on Model Internals\n",
      "The custom losses and metrics we defined earlier were all based on the labels and the\n",
      "predictions (and optionally sample weights). However, you will occasionally want to\n",
      "define losses based on other parts of your model, such as the weights or activations of\n",
      "its hidden layers. This may be useful for regularization purposes, or to monitor some\n",
      "internal aspect of your model.\n",
      "\n",
      "To define a custom loss based on model internals, just compute it based on any part\n",
      "of the model you want, then pass the result to the add_loss() method. For example,\n",
      "the following custom model represents a standard MLP regressor with 5 hidden lay‐\n",
      "ers,  except  it  also  implements  a  reconstruction  loss  (see  ???):  we  add  an  extra  Dense\n",
      "layer on top of the last hidden layer, and its role is to try to reconstruct the inputs of\n",
      "the model. Since the reconstruction must have the same shape as the model’s inputs,\n",
      "we need to create this Dense layer in the build() method to have access to the shape\n",
      "of the inputs. In the call() method, we compute both the regular output of the MLP,\n",
      "plus the output of the reconstruction layer. We then compute the mean squared dif‐\n",
      "ference  between  the  reconstructions  and  the  inputs,  and  we  add  this  value  (times\n",
      "0.05) to the model’s list of losses by calling  add_loss(). During training, Keras will\n",
      "add this loss to the main loss (which is why we scaled down the reconstruction loss,\n",
      "to ensure the main loss dominates). As a result, the model will be forced to preserve\n",
      "as much information as possible through the hidden layers, even information that is\n",
      "not  directly  useful  for  the  regression  task  itself.  In  practice,  this  loss  sometimes\n",
      "improves generalization; it is a regularization loss:\n",
      "\n",
      "class ReconstructingRegressor(keras.models.Model):\n",
      "    def __init__(self, output_dim, **kwargs):\n",
      "        super().__init__(**kwargs)\n",
      "        self.hidden = [keras.layers.Dense(30, activation=\"selu\",\n",
      "                                          kernel_initializer=\"lecun_normal\")\n",
      "\n",
      "388 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      "\f                       for _ in range(5)]\n",
      "        self.out = keras.layers.Dense(output_dim)\n",
      "\n",
      "    def build(self, batch_input_shape):\n",
      "        n_inputs = batch_input_shape[-1]\n",
      "        self.reconstruct = keras.layers.Dense(n_inputs)\n",
      "        super().build(batch_input_shape)\n",
      "\n",
      "    def call(self, inputs):\n",
      "        Z = inputs\n",
      "        for layer in self.hidden:\n",
      "            Z = layer(Z)\n",
      "        reconstruction = self.reconstruct(Z)\n",
      "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
      "        self.add_loss(0.05 * recon_loss)\n",
      "        return self.out(Z)\n",
      "\n",
      "Similarly, you can add a custom metric based on model internals by computing it in\n",
      "any way you want, as long at the result is the output of a metric object. For example,\n",
      "you can create a keras.metrics.Mean() object in the constructor, then call it in the\n",
      "call() method, passing it the recon_loss, and finally add it to the model by calling\n",
      "the  model’s  add_metric()  method.  This  way,  when  you  train  the  model,  Keras  will\n",
      "display both the mean loss over each epoch (the loss is the sum of the main loss plus\n",
      "0.05  times  the  reconstruction  loss)  and  the  mean  reconstruction  error  over  each\n",
      "epoch. Both will go down during training:\n",
      "\n",
      "Epoch 1/5\n",
      "11610/11610 [=============] [...] loss: 4.3092 - reconstruction_error: 1.7360\n",
      "Epoch 2/5\n",
      "11610/11610 [=============] [...] loss: 1.1232 - reconstruction_error: 0.8964\n",
      "[...]\n",
      "\n",
      "In  over  99%  of  the  cases,  everything  we  have  discussed  so  far  will  be  sufficient  to\n",
      "implement whatever model you want to build, even with complex architectures, los‐\n",
      "ses, metrics, and so on. However, in some rare cases you may need to customize the\n",
      "training loop itself. However, before we get there, we need to look at how to compute\n",
      "gradients automatically in TensorFlow.\n",
      "\n",
      "Computing Gradients Using Autodiff\n",
      "To  understand  how  to  use  autodiff  (see  Chapter  10  and  ???)  to  compute  gradients\n",
      "automatically, let’s consider a simple toy function:\n",
      "\n",
      "def f(w1, w2):\n",
      "    return 3 * w1 ** 2 + 2 * w1 * w2\n",
      "\n",
      "If you know calculus, you can analytically find that the partial derivative of this func‐\n",
      "tion with regards to w1 is 6 * w1 + 2 * w2. You can also find that its partial derivative\n",
      "with regards to w2 is 2 * w1. For example, at the point (w1, w2) = (5, 3), these par‐\n",
      "\n",
      "Customizing Models and Training Algorithms \n",
      "\n",
      "| \n",
      "\n",
      "389\n",
      "\n",
      "\ftial derivatives are equal to 36 and 10, respectively, so the gradient vector at this point\n",
      "is (36, 10). But if this were a neural network, the function would be much more com‐\n",
      "plex,  typically  with  tens  of  thousands  of  parameters,  and  finding  the  partial  deriva‐\n",
      "tives analytically by hand would be an almost impossible task. One solution could be\n",
      "to compute an approximation of each partial derivative by measuring how much the\n",
      "function’s output changes when you tweak the corresponding parameter:\n",
      "\n",
      ">>> w1, w2 = 5, 3\n",
      ">>> eps = 1e-6\n",
      ">>> (f(w1 + eps, w2) - f(w1, w2)) / eps\n",
      "36.000003007075065\n",
      ">>> (f(w1, w2 + eps) - f(w1, w2)) / eps\n",
      "10.000000003174137\n",
      "\n",
      "Looks about right! This works rather well and it is trivial to implement, but it is just\n",
      "an approximation, and importantly you need to call f() at least once per parameter\n",
      "(not twice, since we could compute f(w1, w2) just once). This makes this approach\n",
      "intractable  for  large  neural  networks.  So  instead  we  should  use  autodiff  (see  Chap‐\n",
      "ter 10 and ???). TensorFlow makes this pretty simple:\n",
      "\n",
      "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
      "with tf.GradientTape() as tape:\n",
      "    z = f(w1, w2)\n",
      "\n",
      "gradients = tape.gradient(z, [w1, w2])\n",
      "\n",
      "We  first  define  two  variables  w1  and  w2,  then  we  create  a  tf.GradientTape  context\n",
      "that will automatically record every operation that involves a variable, and finally we\n",
      "ask this tape to compute the gradients of the result z with regards to both variables\n",
      "[w1, w2]. Let’s take a look at the gradients that TensorFlow computed:\n",
      "\n",
      ">>> gradients\n",
      "[<tf.Tensor: id=828234, shape=(), dtype=float32, numpy=36.0>,\n",
      " <tf.Tensor: id=828229, shape=(), dtype=float32, numpy=10.0>]\n",
      "\n",
      "Perfect! Not only is the result accurate (the precision is only limited by the floating\n",
      "point errors), but the gradient() method only goes through the recorded computa‐\n",
      "tions once (in reverse order), no matter how many variables there are, so it is incredi‐\n",
      "bly efficient. It’s like magic!\n",
      "\n",
      "Only put the strict minimum inside the tf.GradientTape() block,\n",
      "to save memory. Alternatively, you can pause recording by creating\n",
      "a  with  tape.stop_recording()  block  inside  the  tf.Gradient\n",
      "Tape() block.\n",
      "\n",
      "The tape is automatically erased immediately after you call its gradient() method, so\n",
      "you will get an exception if you try to call gradient() twice:\n",
      "\n",
      "390 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      "\fwith tf.GradientTape() as tape:\n",
      "    z = f(w1, w2)\n",
      "\n",
      "dz_dw1 = tape.gradient(z, w1) # => tensor 36.0\n",
      "dz_dw2 = tape.gradient(z, w2) # RuntimeError!\n",
      "\n",
      "If you need to call gradient() more than once, you must make the tape persistent,\n",
      "and delete it when you are done with it to free resources:\n",
      "\n",
      "with tf.GradientTape(persistent=True) as tape:\n",
      "    z = f(w1, w2)\n",
      "\n",
      "dz_dw1 = tape.gradient(z, w1) # => tensor 36.0\n",
      "dz_dw2 = tape.gradient(z, w2) # => tensor 10.0, works fine now!\n",
      "del tape\n",
      "\n",
      "By  default,  the  tape  will  only  track  operations  involving  variables,  so  if  you  try  to\n",
      "compute the gradient of z with regards to anything else than a variable, the result will\n",
      "be None:\n",
      "\n",
      "c1, c2 = tf.constant(5.), tf.constant(3.)\n",
      "with tf.GradientTape() as tape:\n",
      "    z = f(c1, c2)\n",
      "\n",
      "gradients = tape.gradient(z, [c1, c2]) # returns [None, None]\n",
      "\n",
      "However, you can force the tape to watch any tensors you like, to record every opera‐\n",
      "tion  that  involves  them.  You  can  then  compute  gradients  with  regards  to  these  ten‐\n",
      "sors, as if they were variables:\n",
      "\n",
      "with tf.GradientTape() as tape:\n",
      "    tape.watch(c1)\n",
      "    tape.watch(c2)\n",
      "    z = f(c1, c2)\n",
      "\n",
      "gradients = tape.gradient(z, [c1, c2]) # returns [tensor 36., tensor 10.]\n",
      "\n",
      "This can be useful in some cases, for example if you want to implement a regulariza‐\n",
      "tion loss that penalizes activations that vary a lot when the inputs vary little: the loss\n",
      "will be based on the gradient of the activations with regards to the inputs. Since the\n",
      "inputs are not variables, you would need to tell the tape to watch them.\n",
      "\n",
      "If you compute the gradient of a list of tensors (e.g., [z1, z2, z3]) with regards to\n",
      "some variables (e.g., [w1, w2]), TensorFlow actually efficiently computes the sum of\n",
      "the  gradients  of  these  tensors  (i.e.,  gradient(z1,  [w1,  w2]),  plus  gradient(z2,\n",
      "[w1,  w2]),  plus  gradient(z3,  [w1,  w2])).  Due  to  the  way  reverse-mode  autodiff\n",
      "works, it is not possible to compute the individual gradients (z1, z2 and z3) without\n",
      "actually calling gradient() multiple times (once for z1, once for z2 and once for z3),\n",
      "which requires making the tape persistent (and deleting it afterwards).\n",
      "\n",
      "Customizing Models and Training Algorithms \n",
      "\n",
      "| \n",
      "\n",
      "391\n",
      "\n",
      "\fMoreover, it is actually possible to compute second order partial derivatives (the Hes‐\n",
      "sians,  i.e.,  the  partial  derivatives  of  the  partial  derivatives)!  To  do  this,  we  need  to\n",
      "record  the  operations  that  are  performed  when  computing  the  first-order  partial\n",
      "derivatives (the Jacobians): this requires a second tape. Here is how it works:\n",
      "\n",
      "with tf.GradientTape(persistent=True) as hessian_tape:\n",
      "    with tf.GradientTape() as jacobian_tape:\n",
      "        z = f(w1, w2)\n",
      "    jacobians = jacobian_tape.gradient(z, [w1, w2])\n",
      "hessians = [hessian_tape.gradient(jacobian, [w1, w2])\n",
      "            for jacobian in jacobians]\n",
      "del hessian_tape\n",
      "\n",
      "The inner tape is used to compute the Jacobians, as we did earlier. The outer tape is\n",
      "used to compute the partial derivatives of each Jacobian. Since we need to call gradi\n",
      "ent() once for each Jacobian (or else we would get the sum of the partial derivatives\n",
      "over all the Jabobians, as explained earlier), we need the outer tape to be persistent, so\n",
      "we delete it at the end. The Jacobians are obviously the same as earlier (36 and 5), but\n",
      "now we also have the Hessians:\n",
      "\n",
      ">>> hessians # dz_dw1_dw1, dz_dw1_dw2, dz_dw2_dw1, dz_dw2_dw2\n",
      "[[<tf.Tensor: id=830578, shape=(), dtype=float32, numpy=6.0>,\n",
      "  <tf.Tensor: id=830595, shape=(), dtype=float32, numpy=2.0>],\n",
      " [<tf.Tensor: id=830600, shape=(), dtype=float32, numpy=2.0>, None]]\n",
      "\n",
      "Let’s verify these Hessians. The first two are the partial derivatives of 6 * w1 + 2 * w2\n",
      "(which  is,  as  we  saw  earlier,  the  partial  derivative  of  f  with  regards  to  w1),  with\n",
      "regards to w1 and w2. The result is correct: 6 for w1 and 2 for w2. The next two are the\n",
      "partial  derivatives  of  2  *  w1  (the  partial  derivative  of  f  with  regards  to  w2),  with\n",
      "regards to w1 and w2, which are 2 for w1 and 0 for w2. Note that TensorFlow returns\n",
      "None instead of 0 since w2 does not appear at all in 2 * w1. TensorFlow also returns\n",
      "None when you use an operation whose gradients are not defined (e.g., tf.argmax()).\n",
      "\n",
      "In  some  rare  cases  you  may  want  to  stop  gradients  from  backpropagating  through\n",
      "some part of your neural network. To do this, you must use the tf.stop_gradient()\n",
      "function: it just returns its inputs during the forward pass (like tf.identity()), but\n",
      "it does not let gradients through during backpropagation (it acts like a constant). For\n",
      "example:\n",
      "\n",
      "def f(w1, w2):\n",
      "    return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)\n",
      "\n",
      "with tf.GradientTape() as tape:\n",
      "    z = f(w1, w2) # same result as without stop_gradient()\n",
      "\n",
      "gradients = tape.gradient(z, [w1, w2]) # => returns [tensor 30., None]\n",
      "\n",
      "392 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      "\fFinally, you may occasionally run into some numerical issues when computing gradi‐\n",
      "ents.  For  example,  if  you  compute  the  gradients  of  the  my_softplus()  function  for\n",
      "large inputs, the result will be NaN:\n",
      "\n",
      ">>> x = tf.Variable([100.])\n",
      ">>> with tf.GradientTape() as tape:\n",
      "...     z = my_softplus(x)\n",
      "...\n",
      ">>> tape.gradient(z, [x])\n",
      "<tf.Tensor: [...] numpy=array([nan], dtype=float32)>\n",
      "\n",
      "This is because computing the gradients of this function using autodiff leads to some\n",
      "numerical  difficulties:  due  to  floating  point  precision  errors,  autodiff  ends  up  com‐\n",
      "puting infinity divided by infinity (which returns NaN). Fortunately, we can analyti‐\n",
      "cally find that the derivative of the softplus function is just 1 / (1 + 1 / exp(x)), which\n",
      "is numerically stable. Next, we can tell TensorFlow to use this stable function when\n",
      "computing  the  gradients  of  the  my_softplus()  function,  by  decorating  it  with\n",
      "@tf.custom_gradient, and making it return both its normal output and the function\n",
      "that computes the derivatives (note that it will receive as input the gradients that were\n",
      "backpropagated so far, down to the softplus function, and according to the chain rule\n",
      "we should multiply them with this function’s gradients):\n",
      "\n",
      "@tf.custom_gradient\n",
      "def my_better_softplus(z):\n",
      "    exp = tf.exp(z)\n",
      "    def my_softplus_gradients(grad):\n",
      "        return grad / (1 + 1 / exp)\n",
      "    return tf.math.log(exp + 1), my_softplus_gradients\n",
      "\n",
      "Now when we compute the gradients of the my_better_softplus() function, we get\n",
      "the proper result, even for large input values (however, the main output still explodes\n",
      "because of the exponential: one workaround is to use tf.where() to just return the\n",
      "inputs when they are large).\n",
      "\n",
      "Congratulations! You can now compute the gradients of any function (provided it is\n",
      "differentiable  at  the  point  where  you  compute  it),  you  can  even  compute  Hessians,\n",
      "block  backpropagation  when  needed  and  even  write  your  own  gradient  functions!\n",
      "This is probably more flexibility than you will ever need, even if you build your own\n",
      "custom training loops, as we will see now.\n",
      "\n",
      "Custom Training Loops\n",
      "In some rare cases, the fit() method may not be flexible enough for what you need\n",
      "to  do.  For  example,  the  Wide  and  Deep  paper  we  discussed  in  Chapter  10  actually\n",
      "uses two different optimizers: one for the wide path and the other for the deep path.\n",
      "Since  the  fit()  method  only  uses  one  optimizer  (the  one  that  we  specify  when\n",
      "\n",
      "Customizing Models and Training Algorithms \n",
      "\n",
      "| \n",
      "\n",
      "393\n",
      "\n",
      "\fcompiling  the  model),  implementing  this  paper  requires  writing  your  own  custom\n",
      "loop.\n",
      "\n",
      "You may also like to write your own custom training loops simply to feel more confi‐\n",
      "dent  that  it  does  precisely  what  you  intent  it  to  do  (perhaps  you  are  unsure  about\n",
      "some  details  of  the  fit()  method).  It  can  sometimes  feel  safer  to  make  everything\n",
      "explicit. However, remember that writing a custom training loop will make your code\n",
      "longer, more error prone and harder to maintain.\n",
      "\n",
      "Unless you really need the extra flexibility, you should prefer using\n",
      "the  fit()  method  rather  than  implementing  your  own  training\n",
      "loop, especially if you work in a team.\n",
      "\n",
      "First, let’s build a simple model. No need to compile it, since we will handle the train‐\n",
      "ing loop manually:\n",
      "\n",
      "l2_reg = keras.regularizers.l2(0.05)\n",
      "model = keras.models.Sequential([\n",
      "    keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
      "                       kernel_regularizer=l2_reg),\n",
      "    keras.layers.Dense(1, kernel_regularizer=l2_reg)\n",
      "])\n",
      "\n",
      "Next, let’s create a tiny function that will randomly sample a batch of instances from\n",
      "the training set (in Chapter 13 we will discuss the Data API, which offers a much bet‐\n",
      "ter alternative):\n",
      "\n",
      "def random_batch(X, y, batch_size=32):\n",
      "    idx = np.random.randint(len(X), size=batch_size)\n",
      "    return X[idx], y[idx]\n",
      "\n",
      "Let’s also define a function that will display the training status, including the number\n",
      "of steps, the total number of steps, the mean loss since the start of the epoch (i.e., we\n",
      "will use the Mean metric to compute it), and other metrics:\n",
      "\n",
      "def print_status_bar(iteration, total, loss, metrics=None):\n",
      "    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result())\n",
      "                         for m in [loss] + (metrics or [])])\n",
      "    end = \"\" if iteration < total else \"\\n\"\n",
      "    print(\"\\r{}/{} - \".format(iteration, total) + metrics,\n",
      "          end=end)\n",
      "\n",
      "This  code  is  self-explanatory,  unless  you  are  unfamiliar  with  Python  string  format‐\n",
      "ting: {:.4f} will format a float with 4 digits after the decimal point. Moreover, using\n",
      "\\r (carriage return) along with end=\"\" ensures that the status bar always gets printed\n",
      "on the same line. In the notebook, the print_status_bar() function also includes a\n",
      "progress bar, but you could use the handy tqdm library instead.\n",
      "\n",
      "394 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      "\fWith that, let’s get down to business! First, we need to define some hyperparameters,\n",
      "choose the optimizer, the loss function and the metrics (just the MAE in this exam‐\n",
      "ple):\n",
      "\n",
      "n_epochs = 5\n",
      "batch_size = 32\n",
      "n_steps = len(X_train) // batch_size\n",
      "optimizer = keras.optimizers.Nadam(lr=0.01)\n",
      "loss_fn = keras.losses.mean_squared_error\n",
      "mean_loss = keras.metrics.Mean()\n",
      "metrics = [keras.metrics.MeanAbsoluteError()]\n",
      "\n",
      "And now we are ready to build the custom loop!\n",
      "\n",
      "for epoch in range(1, n_epochs + 1):\n",
      "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
      "    for step in range(1, n_steps + 1):\n",
      "        X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
      "        with tf.GradientTape() as tape:\n",
      "            y_pred = model(X_batch, training=True)\n",
      "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
      "            loss = tf.add_n([main_loss] + model.losses)\n",
      "        gradients = tape.gradient(loss, model.trainable_variables)\n",
      "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
      "        mean_loss(loss)\n",
      "        for metric in metrics:\n",
      "            metric(y_batch, y_pred)\n",
      "        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n",
      "    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)\n",
      "    for metric in [mean_loss] + metrics:\n",
      "        metric.reset_states()\n",
      "\n",
      "There’s a lot going on in this code, so let’s walk through it:\n",
      "\n",
      "• We create two nested loops: one for the epochs, the other for the batches within\n",
      "\n",
      "an epoch.\n",
      "\n",
      "• Then we sample a random batch from the training set.\n",
      "• Inside the tf.GradientTape() block, we make a prediction for one batch (using\n",
      "the  model  as  a  function),  and  we  compute  the  loss:  it  is  equal  to  the  main  loss\n",
      "plus  the  other  losses  (in  this  model,  there  is  one  regularization  loss  per  layer).\n",
      "Since  the  mean_squared_error()  function  returns  one  loss  per  instance,  we\n",
      "compute  the  mean  over  the  batch  using  tf.reduce_mean()  (if  you  wanted  to\n",
      "apply different weights to each instance, this is where you would do it). The regu‐\n",
      "larization  losses  are  already  reduced  to  a  single  scalar  each,  so  we  just  need  to\n",
      "sum  them  (using  tf.add_n(),  which  sums  multiple  tensors  of  the  same  shape\n",
      "and data type).\n",
      "\n",
      "Customizing Models and Training Algorithms \n",
      "\n",
      "| \n",
      "\n",
      "395\n",
      "\n",
      "\f• Next,  we  ask  the  tape  to  compute  the  gradient  of  the  loss  with  regards  to  each\n",
      "trainable variable (not all variables!), and we apply them to the optimizer to per‐\n",
      "form a Gradient Descent step.\n",
      "\n",
      "• Next we update the mean loss and the metrics (over the current epoch), and we\n",
      "\n",
      "display the status bar.\n",
      "\n",
      "• At the end of each epoch, we display the status bar again to make it look com‐\n",
      "plete11 and to print a line feed, and we reset the states of the mean loss and the\n",
      "metrics.\n",
      "\n",
      "If you set the optimizer’s clipnorm or clipvalue hyperparameters, it will take care of\n",
      "this for you. If you want to apply any other transformation to the gradients, simply do\n",
      "so before calling the apply_gradients() method.\n",
      "\n",
      "If you add weight constraints to your model (e.g., by setting kernel_constraint or\n",
      "bias_constraint  when  creating  a  layer),  you  should  update  the  training  loop  to\n",
      "apply these constraints just after apply_gradients():\n",
      "\n",
      "for variable in model.variables:\n",
      "    if variable.constraint is not None:\n",
      "        variable.assign(variable.constraint(variable))\n",
      "\n",
      "Most  importantly,  this  training  loop  does  not  handle  layers  that  behave  differently\n",
      "during training and testing (e.g., BatchNormalization or Dropout). To handle these,\n",
      "you need to call the model with training=True and make sure it propagates this to\n",
      "every layer that needs it.12\n",
      "\n",
      "As you can see, there are quite a lot of things you need to get right, it is easy to make a\n",
      "mistake. But on the bright side, you get full control, so it’s your call.\n",
      "\n",
      "Now that you know how to customize any part of your models13 and training algo‐\n",
      "rithms, let’s see how you can use TensorFlow’s automatic graph generation feature: it\n",
      "can speed up your custom code considerably, and it will also make it portable to any\n",
      "platform supported by TensorFlow (see ???).\n",
      "\n",
      "TensorFlow Functions and Graphs\n",
      "In TensorFlow 1, graphs were unavoidable (as were the complexities that came with\n",
      "them):  they  were  a  central  part  of  TensorFlow’s  API.  In  TensorFlow  2,  they  are  still\n",
      "\n",
      "11 The truth is we did not process every single instance in the training set because we sampled instances ran‐\n",
      "\n",
      "domly, so some were processed more than once while others were not processed at all. In practice that’s fine.\n",
      "Moreover, if the training set size is not a multiple of the batch size, we will miss a few instances.\n",
      "\n",
      "12 Alternatively, check out K.learning_phase(), K.set_learning_phase() and K.learning_phase_scope().\n",
      "\n",
      "13 With the exception of optimizers, as very few people ever customize these: see the notebook for an example.\n",
      "\n",
      "396 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      "\fthere, but not as central, and much (much!) simpler to use. To demonstrate this, let’s\n",
      "start with a trivial function that just computes the cube of its input:\n",
      "\n",
      "def cube(x):\n",
      "    return x ** 3\n",
      "\n",
      "We can obviously call this function with a Python value, such as an int or a float, or\n",
      "we can call it with a tensor:\n",
      "\n",
      ">>> cube(2)\n",
      "8\n",
      ">>> cube(tf.constant(2.0))\n",
      "<tf.Tensor: id=18634148, shape=(), dtype=float32, numpy=8.0>\n",
      "\n",
      "Now, let’s use tf.function() to convert this Python function to a TensorFlow Func‐\n",
      "tion:\n",
      "\n",
      ">>> tf_cube = tf.function(cube)\n",
      ">>> tf_cube\n",
      "<tensorflow.python.eager.def_function.Function at 0x1546fc080>\n",
      "\n",
      "This TF Function can then be used exactly like the original Python function, and it\n",
      "will return the same result (but as tensors):\n",
      "\n",
      ">>> tf_cube(2)\n",
      "<tf.Tensor: id=18634201, shape=(), dtype=int32, numpy=8>\n",
      ">>> tf_cube(tf.constant(2.0))\n",
      "<tf.Tensor: id=18634211, shape=(), dtype=float32, numpy=8.0>\n",
      "\n",
      "Under the hood, tf.function() analyzed the computations performed by the cube()\n",
      "function  and  generated  an  equivalent  computation  graph!  As  you  can  see,  it  was\n",
      "rather painless (we will see how this works shortly). Alternatively, we could have used\n",
      "tf.function as a decorator; this is actually more common:\n",
      "\n",
      "@tf.function\n",
      "def tf_cube(x):\n",
      "    return x ** 3\n",
      "\n",
      "The original Python function is still available via the TF Function’s python_function\n",
      "attribute, in case you ever need it:\n",
      "\n",
      ">>> tf_cube.python_function(2)\n",
      "8\n",
      "\n",
      "TensorFlow  optimizes  the  computation  graph,  pruning  unused  nodes,  simplifying\n",
      "expressions  (e.g.,  1  +  2  would  get  replaced  with  3),  and  more.  Once  the  optimized\n",
      "graph is ready, the TF Function efficiently executes the operations in the graph, in the\n",
      "appropriate order (and in parallel when it can). As a result, a TF Function will usually\n",
      "run much faster than the original Python function, especially if it performs complex\n",
      "\n",
      "TensorFlow Functions and Graphs \n",
      "\n",
      "| \n",
      "\n",
      "397\n",
      "\n",
      "\fcomputations.14  Most  of  the  time  you  will  not  really  need  to  know  more  than  that:\n",
      "when  you  want  to  boost  a  Python  function,  just  transform  it  into  a  TF  Function.\n",
      "That’s all!\n",
      "\n",
      "Moreover, when you write a custom loss function, a custom metric, a custom layer or\n",
      "any  other  custom  function,  and  you  use  it  in  a  Keras  model  (as  we  did  throughout\n",
      "this chapter), Keras automatically converts your function into a TF Function, no need\n",
      "to use tf.function(). So most of the time, all this magic is 100% transparent.\n",
      "\n",
      "You  can  tell  Keras  not  to  convert  your  Python  functions  to  TF\n",
      "Functions  by  setting  dynamic=True  when  creating  a  custom  layer\n",
      "or  a  custom  model.  Alternatively,  you  can  set  run_eagerly=True\n",
      "when calling the model’s compile() method.\n",
      "\n",
      "TF  Function  generates  a  new  graph  for  every  unique  set  of  input  shapes  and  data\n",
      "types, and it caches it for subsequent calls. For example, if you call tf_cube(tf.con\n",
      "stant(10)), a graph will be generated for int32 tensors of shape []. Then if you call\n",
      "tf_cube(tf.constant(20)),  the  same  graph  will  be  reused.  But  if  you  then  call\n",
      "tf_cube(tf.constant([10, 20])), a new graph will be generated for int32 tensors\n",
      "of shape [2]. This is how TF Functions handle polymorphism (i.e., varying argument\n",
      "types and shapes). However, this is only true for tensor arguments: if you pass numer‐\n",
      "ical Python values to a TF Function, a new graph will be generated for every distinct\n",
      "value: for example, calling tf_cube(10) and tf_cube(20) will generate two graphs.\n",
      "\n",
      "If  you  call  a  TF  Function  many  times  with  different  numerical\n",
      "Python values, then many graphs will be generated, slowing down\n",
      "your program and using up a lot of RAM. Python values should be\n",
      "reserved  for  arguments  that  will  have  few  unique  values,  such  as\n",
      "hyperparameters like the number of neurons per layer. This allows\n",
      "TensorFlow to better optimize each variant of your model.\n",
      "\n",
      "Autograph and Tracing\n",
      "So how does TensorFlow generate graphs? Well, first it starts by analyzing the Python\n",
      "function’s  source  code  to  capture  all  the  control  flow  statements,  such  as  for  loops\n",
      "and  while  loops,  if  statements,  as  well  as  break,  continue  and  return  statements.\n",
      "This first step is called autograph. The reason TensorFlow has to analyze the source\n",
      "code  is  that  Python  does  not  provide  any  other  way  to  capture  control  flow  state‐\n",
      "ments: it offers magic methods like __add__() or __mul__() to capture operators like\n",
      "\n",
      "14 However, in this trivial example, the computation graph is so small that there is nothing at all to optimize, so\n",
      "\n",
      "tf_cube() actually runs much slower than cube().\n",
      "\n",
      "398 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      "\f+ and *, but there are no __while__() or __if__() magic methods. After analyzing\n",
      "the function’s code, autograph outputs an upgraded version of that function in which\n",
      "all  the  control  flow  statements  are  replaced  by  the  appropriate  TensorFlow  opera‐\n",
      "tions,  such  as  tf.while_loop()  for  loops  and  tf.cond()  for  if  statements.  For\n",
      "example,  in  Figure  12-4,  autograph  analyzes  the  source  code  of  the  sum_squares()\n",
      "Python function, and it generates the tf__sum_squares() function. In this function,\n",
      "the  for  loop  is  replaced  by  the  definition  of  the  loop_body()  function  (containing\n",
      "the body of the original for loop), followed by a call to the for_stmt() function. This\n",
      "call will build the appropriate tf.while_loop() operation in the computation graph.\n",
      "\n",
      "Figure 12-4. How TensorFlow generates graphs using autograph and tracing\n",
      "\n",
      "Next,  TensorFlow  calls  this  “upgraded”  function,  but  instead  of  passing  the  actual\n",
      "argument, it passes a symbolic tensor, meaning a tensor without any actual value, only\n",
      "a  name,  a  data  type,  and  a  shape.  For  example,  if  you  call  sum_squares(tf.con\n",
      "stant(10)), then the tf__sum_squares() function will actually be called with a sym‐\n",
      "bolic tensor of type int32 and shape []. The function will run in graph mode, meaning\n",
      "that each TensorFlow operation will just add a node in the graph to represent itself\n",
      "and  its  output  tensor(s)  (as  opposed  to  the  regular  mode,  called  eager  execution,  or\n",
      "eager mode). In graph mode, TF operations do not perform any actual computations.\n",
      "This  should  feel  familiar  if  you  know  TensorFlow  1,  as  graph  mode  was  the  default\n",
      "mode. In Figure 12-4, you can see the tf__sum_squares() function being called with\n",
      "a symbolic tensor as argument (in this case, an int32 tensor of shape []), and the final\n",
      "graph  generated  during  tracing.  The  ellipses  represent  operations,  and  the  arrows\n",
      "represent tensors (both the generated function and the graph are simplified).\n",
      "\n",
      "TensorFlow Functions and Graphs \n",
      "\n",
      "| \n",
      "\n",
      "399\n",
      "\n",
      "\fTo view the generated function’s source code, you can call tf.auto\n",
      "graph.to_code(sum_squares.python_function).  The  code  is  not\n",
      "meant to be pretty, but it can sometimes help for debugging.\n",
      "\n",
      "TF Function Rules\n",
      "Most of the time, converting a Python function that performs TensorFlow operations\n",
      "into a TF Function is trivial: just decorate it with @tf.function or let Keras take care\n",
      "of it for you. However, there are a few rules to respect:\n",
      "\n",
      "• If  you  call  any  external  library,  including  NumPy  or  even  the  standard  library,\n",
      "this  call  will  run  only  during  tracing,  it  will  not  be  part  of  the  graph.  Indeed,  a\n",
      "TensorFlow graph can only include TensorFlow constructs (tensors, operations,\n",
      "variables, datasets, and so on). So make sure you use tf.reduce_sum() instead of\n",
      "np.sum(),  and  tf.sort()  instead  of  the  built-in  sorted()  function,  and  so  on\n",
      "(unless you really want the code to run only during tracing).\n",
      "— For  example,  if  you  define  a  TF  function  f(x)  that  just  returns  np.ran\n",
      "dom.rand(),  a  random  number  will  only  be  generated  when  the  function  is\n",
      "traced,  so  f(tf.constant(2.))  and  f(tf.constant(3.))  will  return  the\n",
      "same random number, but f(tf.constant([2., 3.])) will return a different\n",
      "one.  If  you  replace  np.random.rand()  with  tf.random.uniform([]),  then  a\n",
      "new  random  number  will  be  generated  upon  every  call,  since  the  operation\n",
      "will be part of the graph.\n",
      "\n",
      "— If  your  non-TensorFlow  code  has  side-effects  (such  as  logging  something  or\n",
      "updating  a  Python  counter),  then  you  should  not  expect  that  side-effect  to\n",
      "occur every time you call the TF Function, as it will only occur when the func‐\n",
      "tion is traced.\n",
      "\n",
      "— You  can  wrap  arbitrary  Python  code  in  a  tf.py_function()  operation,  but\n",
      "this will hinder performance, as TensorFlow will not be able to do any graph\n",
      "optimization on this code, and it will also reduce portability, as the graph will\n",
      "only  run  on  platforms  where  Python  is  available  (and  the  right  libraries\n",
      "installed).\n",
      "\n",
      "• You can call other Python functions or TF Functions, but they should follow the\n",
      "same rules, as TensorFlow will also capture their operations in the computation\n",
      "graph.  Note  that  these  other  functions  do  not  need  to  be  decorated  with\n",
      "@tf.function.\n",
      "\n",
      "• If  the  function  creates  a  TensorFlow  variable  (or  any  other  stateful  TensorFlow\n",
      "object, such as a dataset or a queue), it must do so upon the very first call, and\n",
      "only then, or else you will get an exception. It is usually preferable to create vari‐\n",
      "ables outside of the TF Function (e.g., in the build() method of a custom layer).\n",
      "\n",
      "400 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 12: Custom Models and Training with TensorFlow\n",
      "\n",
      "\f• The  source  code  of  your  Python  function  should  be  available  to  TensorFlow.  If\n",
      "the  source  code  is  unavailable  (for  example,  if  you  define  your  function  in  the\n",
      "Python shell, which does not give access to the source code, or if you deploy only\n",
      "the compiled Python files *.pyc to production), then the graph generation pro‐\n",
      "cess will fail or have limited functionality.\n",
      "\n",
      "• TensorFlow will only capture for loops that iterate over a tensor or a Dataset. So\n",
      "make sure you use for i in tf.range(10) rather than for i in range(10), or\n",
      "else the loop will not be captured in the graph. Instead, it will run during tracing.\n",
      "This may be what you want, if the for loop is meant to build the graph, for exam‐\n",
      "ple to create each layer in a neural network.\n",
      "\n",
      "• And  as  always,  for  performance  reasons,  you  should  prefer  a  vectorized  imple‐\n",
      "\n",
      "mentation whenever you can, rather than using loops.\n",
      "\n",
      "It’s time to sum up! In this chapter we started with a brief overview of TensorFlow,\n",
      "then we looked at TensorFlow’s low-level API, including tensors, operations, variables\n",
      "and special data structures. We then used these tools to customize almost every com‐\n",
      "ponent  in  tf.keras.  Finally,  we  looked  at  how  TF  Functions  can  boost  performance,\n",
      "how graphs are generated using autograph and tracing, and what rules to follow when\n",
      "you  write  TF  Functions  (if  you  would  like  to  open  the  black  box  a  bit  further,  for\n",
      "example  to  explore  the  generated  graphs,  you  will  find  further  technical  details\n",
      "in ???).\n",
      "\n",
      "In the next chapter, we will look at how to efficiently load and preprocess data with\n",
      "TensorFlow.\n",
      "\n",
      "TensorFlow Functions and Graphs \n",
      "\n",
      "| \n",
      "\n",
      "401\n",
      "\n",
      "\f\fCHAPTER 13\n",
      "Loading and Preprocessing Data with\n",
      "TensorFlow\n",
      "\n",
      "With  Early  Release  ebooks,  you  get  books  in  their  earliest  form—\n",
      "the author’s raw and unedited content as he or she writes—so you\n",
      "can  take  advantage  of  these  technologies  long  before  the  official\n",
      "release of these titles. The following will be Chapter 13 in the final\n",
      "release of the book.\n",
      "\n",
      "So far we have used only datasets that fit in memory, but Deep Learning systems are\n",
      "often trained on very large datasets that will not fit in RAM. Ingesting a large dataset\n",
      "and preprocessing it efficiently can be tricky to implement with other Deep Learning\n",
      "libraries, but TensorFlow makes it easy thanks to the Data API: you just create a data‐\n",
      "set  object,  tell  it  where  to  get  the  data,  then  transform  it  in  any  way  you  want,  and\n",
      "TensorFlow  takes  care  of  all  the  implementation  details,  such  as  multithreading,\n",
      "queuing, batching, prefetching, and so on.\n",
      "\n",
      "Off  the  shelf,  the  Data  API  can  read  from  text  files  (such  as  CSV  files),  binary  files\n",
      "with  fixed-size  records,  and  binary  files  that  use  TensorFlow’s  TFRecord  format,\n",
      "which  supports  records  of  varying  sizes.  TFRecord  is  a  flexible  and  efficient  binary\n",
      "format based on Protocol Buffers (an open source binary format). The Data API also\n",
      "has  support  for  reading  from  SQL  databases.  Moreover,  many  Open  Source  exten‐\n",
      "sions are available to read from all sorts of data sources, such as Google’s BigQuery\n",
      "service.\n",
      "\n",
      "However,  reading  huge  datasets  efficiently  is  not  the  only  difficulty:  the  data  also\n",
      "needs  to  be  preprocessed.  Indeed,  it  is  not  always  composed  strictly  of  convenient\n",
      "numerical fields: sometimes there will be text features, categorical features, and so on.\n",
      "To handle this, TensorFlow provides the Features API: it lets you easily convert these\n",
      "features  to  numerical  features  that  can  be  consumed  by  your  neural  network.  For\n",
      "\n",
      "403\n",
      "\n",
      "\fexample,  categorical  features  with  a  large  number  of  categories  (such  as  cities,  or\n",
      "words) can be encoded using embeddings (as we will see, an embedding is a trainable\n",
      "dense vector that represents a category).\n",
      "\n",
      "Both  the  Data  API  and  the  Features  API  work  seamlessly  with\n",
      "tf.keras.\n",
      "\n",
      "In  this  chapter,  we  will  cover  the  Data  API,  the  TFRecord  format  and  the  Features\n",
      "API  in  detail.  We  will  also  take  a  quick  look  at  a  few  related  projects  from  Tensor‐\n",
      "Flow’s ecosystem:\n",
      "\n",
      "• TF  Transform  (tf.Transform)  makes  it  possible  to  write  a  single  preprocessing\n",
      "function  that  can  be  run  both  in  batch  mode  on  your  full  training  set,  before\n",
      "training (to speed it up), and then exported to a TF Function and incorporated\n",
      "into  your  trained  model,  so  that  once  it  is  deployed  in  production,  it  can  take\n",
      "care of preprocessing new instances on the fly.\n",
      "\n",
      "• TF Datasets (TFDS) provides a convenient function to download many common\n",
      "datasets of all kinds, including large ones like ImageNet, and it provides conve‐\n",
      "nient dataset objects to manipulate them using the Data API.\n",
      "\n",
      "So let’s get started!\n",
      "\n",
      "The Data API\n",
      "The whole Data API revolves around the concept of a dataset: as you might suspect,\n",
      "this represents a sequence of data items. Usually you will use datasets that gradually\n",
      "read data from disk, but for simplicity let’s just create a dataset entirely in RAM using\n",
      "tf.data.Dataset.from_tensor_slices():\n",
      "\n",
      ">>> X = tf.range(10)  # any data tensor\n",
      ">>> dataset = tf.data.Dataset.from_tensor_slices(X)\n",
      ">>> dataset\n",
      "<TensorSliceDataset shapes: (), types: tf.int32>\n",
      "\n",
      "The  from_tensor_slices()  function  takes  a  tensor  and  creates  a  tf.data.Dataset\n",
      "whose elements are all the slices of X (along the first dimension), so this dataset con‐\n",
      "tains  10  items:  tensors  0,  1,  2,  …,  9.  In  this  case  we  would  have  obtained  the  same\n",
      "dataset if we had used tf.data.Dataset.range(10).\n",
      "\n",
      "You can simply iterate over a dataset’s items like this:\n",
      "\n",
      ">>> for item in dataset:\n",
      "...     print(item)\n",
      "\n",
      "404 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "\n",
      "\f...\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "[...]\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n",
      "\n",
      "Chaining Transformations\n",
      "Once you have a dataset, you can apply all sorts of transformations to it by calling its\n",
      "transformation methods. Each method returns a new dataset, so you can chain trans‐\n",
      "formations like this (this chain is illustrated in Figure 13-1):\n",
      "\n",
      ">>> dataset = dataset.repeat(3).batch(7)\n",
      ">>> for item in dataset:\n",
      "...     print(item)\n",
      "...\n",
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n",
      "\n",
      "Figure 13-1. Chaining Dataset Transformations\n",
      "\n",
      "In  this  example,  we  first  call  the  repeat()  method  on  the  original  dataset,  and  it\n",
      "returns  a  new  dataset  that  will  repeat  the  items  of  the  original  dataset  3  times.  Of\n",
      "course, this will not copy the whole data in memory 3 times! In fact, if you call this\n",
      "method  with  no  arguments,  the  new  dataset  will  repeat  the  source  dataset  forever.\n",
      "Then we call the batch() method on this new dataset, and again this creates a new\n",
      "dataset. This one will group the items of the previous dataset in batches of 7 items.\n",
      "Finally,  we  iterate  over  the  items  of  this  final  dataset.  As  you  can  see,  the  batch()\n",
      "method  had  to  output  a  final  batch  of  size  2  instead  of  7,  but  you  can  call  it  with\n",
      "drop_remainder=True if you want it to drop this final batch so that all batches have\n",
      "the exact same size.\n",
      "\n",
      "The Data API \n",
      "\n",
      "| \n",
      "\n",
      "405\n",
      "\n",
      "\fThe dataset methods do not modify datasets, they create new ones,\n",
      "so make sure to keep a reference to these new datasets (e.g., data\n",
      "set = ...), or else nothing will happen.\n",
      "\n",
      "You  can  also  apply  any  transformation  you  want  to  the  items  by  calling  the  map()\n",
      "method. For example, this creates a new dataset with all items doubled:\n",
      "\n",
      ">>> dataset = dataset.map(lambda x: x * 2) # Items: [0,2,4,6,8,10,12]\n",
      "\n",
      "This  function  is  the  one  you  will  call  to  apply  any  preprocessing  you  want  to  your\n",
      "data. Sometimes, this will include computations that can be quite intensive, such as\n",
      "reshaping or rotating an image, so you will usually want to spawn multiple threads to\n",
      "speed things up: it’s as simple as setting the num_parallel_calls argument.\n",
      "\n",
      "While the map() applies a transformation to each item, the apply() method applies a\n",
      "transformation to the dataset as a whole. For example, the following code “unbatches”\n",
      "the dataset, by applying the unbatch() function to the dataset (this function is cur‐\n",
      "rently experimental, but it will most likely move to the core API in a future release).\n",
      "Each  item  in  the  new  dataset  will  be  a  single  integer  tensor  instead  of  a  batch  of  7\n",
      "integers:\n",
      "\n",
      ">>> dataset = dataset.apply(tf.data.experimental.unbatch()) # Items: 0,2,4,...\n",
      "\n",
      "It is also possible to simply filter the dataset using the filter() method:\n",
      "\n",
      ">>> dataset = dataset.filter(lambda x: x < 10) # Items: 0 2 4 6 8 0 2 4 6...\n",
      "\n",
      "You will often want to look at just a few items from a dataset. You can use the take()\n",
      "method for that:\n",
      "\n",
      ">>> for item in dataset.take(3):\n",
      "...     print(item)\n",
      "...\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "\n",
      "Shuffling the Data\n",
      "As you know, Gradient Descent works best when the instances in the training set are\n",
      "independent and identically distributed (see Chapter 4). A simple way to ensure this\n",
      "is  to  shuffle  the  instances.  For  this,  you  can  just  use  the  shuffle()  method.  It  will\n",
      "create  a  new  dataset  that  will  start  by  filling  up  a  buffer  with  the  first  items  of  the\n",
      "source  dataset,  then  whenever  it  is  asked  for  an  item,  it  will  pull  one  out  randomly\n",
      "from the buffer, and replace it with a fresh one from the source dataset, until it has\n",
      "iterated  entirely  through  the  source  dataset.  At  this  point  it  continues  to  pull  out\n",
      "items randomly from the buffer until it is empty. You must specify the buffer size, and\n",
      "\n",
      "406 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "\n",
      "\fit  is  important  to  make  it  large  enough  or  else  shuffling  will  not  be  very  efficient.1\n",
      "However,  obviously  do  not  exceed  the  amount  of  RAM  you  have,  and  even  if  you\n",
      "have plenty of it, there’s no need to go well beyond the dataset’s size. You can provide\n",
      "a random seed if you want the same random order every time you run your program.\n",
      "\n",
      ">>> dataset = tf.data.Dataset.range(10).repeat(3) # 0 to 9, three times\n",
      ">>> dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)\n",
      ">>> for item in dataset:\n",
      "...     print(item)\n",
      "...\n",
      "tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\n",
      "tf.Tensor([3 6], shape=(2,), dtype=int64)\n",
      "\n",
      "If you call repeat() on a shuffled dataset, by default it will generate\n",
      "a new order at every iteration. This is generally a good idea, but if\n",
      "you prefer to reuse the same order at each iteration (e.g., for tests\n",
      "or debugging), you can set reshuffle_each_iteration=False.\n",
      "\n",
      "For a large dataset that does not fit in memory, this simple shuffling-buffer approach\n",
      "may not be sufficient, since the buffer will be small compared to the dataset. One sol‐\n",
      "ution  is  to  shuffle  the  source  data  itself  (for  example,  on  Linux  you  can  shuffle  text\n",
      "files using the shuf command). This will definitely improve shuffling a lot! However,\n",
      "even if the source data is shuffled, you will usually want to shuffle it some more, or\n",
      "else the same order will be repeated at each epoch, and the model may end up being\n",
      "biased  (e.g.,  due  to  some  spurious  patterns  present  by  chance  in  the  source  data’s\n",
      "order). To shuffle the instances some more, a common approach is to split the source\n",
      "data into multiple files, then read them in a random order during training. However,\n",
      "instances  located  in  the  same  file  will  still  end  up  close  to  each  other.  To  avoid  this\n",
      "you  can  pick  multiple  files  randomly,  and  read  them  simultaneously,  interleaving\n",
      "their lines. Then on top of that you can add a shuffling buffer using the  shuffle()\n",
      "method. If all this sounds like a lot of work, don’t worry: the Data API actually makes\n",
      "all this possible in just a few lines of code. Let’s see how to do this.\n",
      "\n",
      "1 Imagine a sorted deck of cards on your left: suppose you just take the top 3 cards and shuffle them, then pick\n",
      "one randomly and put it to your right, keeping the other 2 in your hands. Take another card on your left,\n",
      "shuffle the 3 cards in your hands and pick one of them randomly, and put it on your right. When you are\n",
      "done going through all the cards like this, you will have a deck of cards on your right: do you think it will be\n",
      "perfectly shuffled?\n",
      "\n",
      "The Data API \n",
      "\n",
      "| \n",
      "\n",
      "407\n",
      "\n",
      "\fInterleaving Lines From Multiple Files\n",
      "\n",
      "First,  let’s  suppose  that  you  loaded  the  California  housing  dataset,  you  shuffled  it\n",
      "(unless it was already shuffled), you split it into a training set, a validation set and a\n",
      "test set, then you split each set into many CSV files that each look like this (each row\n",
      "contains 8 input features plus the target median house value):\n",
      "\n",
      "MedInc,HouseAge,AveRooms,AveBedrms,Popul,AveOccup,Lat,Long,MedianHouseValue\n",
      "3.5214,15.0,3.0499,1.1065,1447.0,1.6059,37.63,-122.43,1.442\n",
      "5.3275,5.0,6.4900,0.9910,3464.0,3.4433,33.69,-117.39,1.687\n",
      "3.1,29.0,7.5423,1.5915,1328.0,2.2508,38.44,-122.98,1.621\n",
      "[...]\n",
      "\n",
      "Let’s also suppose train_filepaths contains the list of file paths (and you also have\n",
      "valid_filepaths and test_filepaths):\n",
      "\n",
      ">>> train_filepaths\n",
      "['datasets/housing/my_train_00.csv', 'datasets/housing/my_train_01.csv',...]\n",
      "\n",
      "Now let’s create a dataset containing only these file paths:\n",
      "\n",
      "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)\n",
      "\n",
      "By default, the list_files() function returns a dataset that shuffles the file paths. In\n",
      "general this is a good thing, but you can set shuffle=False if you do not want that,\n",
      "for some reason.\n",
      "\n",
      "Next, we can call the interleave() method to read from 5 files at a time and inter‐\n",
      "leave their lines (skipping the first line of each file, which is the header row, using the\n",
      "skip() method):\n",
      "\n",
      "n_readers = 5\n",
      "dataset = filepath_dataset.interleave(\n",
      "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
      "    cycle_length=n_readers)\n",
      "\n",
      "The  interleave()  method  will  create  a  dataset  that  will  pull  5  file  paths  from  the\n",
      "filepath_dataset, and for each one it will call the function we gave it (a lambda in\n",
      "this  example)  to  create  a  new  dataset,  in  this  case  a  TextLineDataset.  It  will  then\n",
      "cycle through these 5 datasets, reading one line at a time from each until all datasets\n",
      "are out of items. Then it will get the next 5 file paths from the filepath_dataset, and\n",
      "interleave them the same way, and so on until it runs out of file paths.\n",
      "\n",
      "For interleaving to work best, it is preferable to have files of identi‐\n",
      "cal length, or else the end of the longest files will not be interleaved.\n",
      "\n",
      "408 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "\n",
      "\fBy  default,  interleave()  does  not  use  parallelism,  it  just  reads  one  line  at  a  time\n",
      "from each file, sequentially. However, if you want it to actually read files in parallel,\n",
      "you can set the num_parallel_calls argument to the number of threads you want.\n",
      "You can even set it to tf.data.experimental.AUTOTUNE to make TensorFlow choose\n",
      "the right number of threads dynamically based on the available CPU (however, this is\n",
      "an experimental feature for now). Let’s look at what the dataset contains now:\n",
      "\n",
      ">>> for line in dataset.take(5):\n",
      "...     print(line.numpy())\n",
      "...\n",
      "b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782'\n",
      "b'4.1812,52.0,5.7013,0.9965,692.0,2.4027,33.73,-118.31,3.215'\n",
      "b'3.6875,44.0,4.5244,0.9930,457.0,3.1958,34.04,-118.15,1.625'\n",
      "b'3.3456,37.0,4.5140,0.9084,458.0,3.2253,36.67,-121.7,2.526'\n",
      "b'3.5214,15.0,3.0499,1.1065,1447.0,1.6059,37.63,-122.43,1.442'\n",
      "\n",
      "These are the first rows (ignoring the header row) of 5 CSV files, chosen randomly.\n",
      "Looks  good!  But  as  you  can  see,  these  are  just  byte  strings,  we  need  to  parse  them,\n",
      "and also scale the data.\n",
      "\n",
      "Preprocessing the Data\n",
      "Let’s implement a small function that will perform this preprocessing:\n",
      "\n",
      "X_mean, X_std = [...] # mean and scale of each feature in the training set\n",
      "n_inputs = 8\n",
      "\n",
      "def preprocess(line):\n",
      "  defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
      "  fields = tf.io.decode_csv(line, record_defaults=defs)\n",
      "  x = tf.stack(fields[:-1])\n",
      "  y = tf.stack(fields[-1:])\n",
      "  return (x - X_mean) / X_std, y\n",
      "\n",
      "Let’s walk through this code:\n",
      "\n",
      "• First, we assume that you have precomputed the mean and standard deviation of\n",
      "each feature in the training set. X_mean and X_std are just 1D tensors (or NumPy\n",
      "arrays) containing 8 floats, one per input feature.\n",
      "\n",
      "• The preprocess() function takes one CSV line, and starts by parsing it. For this,\n",
      "it uses the tf.io.decode_csv() function, which takes two arguments: the first is\n",
      "the line to parse, and the second is an array containing the default value for each\n",
      "column in the CSV file. This tells TensorFlow not only the default value for each\n",
      "column,  but  also  the  number  of  columns  and  the  type  of  each  column.  In  this\n",
      "example, we tell it that all feature columns are floats and missing values should\n",
      "default  to  0,  but  we  provide  an  empty  array  of  type  tf.float32  as  the  default\n",
      "value for the last column (the target): this tells TensorFlow that this column con‐\n",
      "\n",
      "The Data API \n",
      "\n",
      "| \n",
      "\n",
      "409\n",
      "\n",
      "\ftains  floats,  but  that  there  is  no  default  value,  so  it  will  raise  an  exception  if  it\n",
      "encounters a missing value.\n",
      "\n",
      "• The decode_csv() function returns a list of scalar tensors (one per column) but\n",
      "we need to return 1D tensor arrays. So we call tf.stack() on all tensors except\n",
      "for the last one (the target): this will stack these tensors into a 1D array. We then\n",
      "do  the  same  for  the  target  value  (this  makes  it  a  1D  tensor  array  with  a  single\n",
      "value, rather than a scalar tensor).\n",
      "\n",
      "• Finally,  we  scale  the  input  features  by  subtracting  the  feature  means  and  then\n",
      "dividing by the feature standard deviations, and we return a tuple containing the\n",
      "scaled features and the target.\n",
      "\n",
      "Let’s test this preprocessing function:\n",
      "\n",
      ">>> preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')\n",
      "(<tf.Tensor: id=6227, shape=(8,), dtype=float32, numpy=\n",
      " array([ 0.16579159,  1.216324  , -0.05204564, -0.39215982, -0.5277444 ,\n",
      "        -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)>,\n",
      " <tf.Tensor: [...], numpy=array([2.782], dtype=float32)>)\n",
      "\n",
      "We can now apply this preprocessing function to the dataset.\n",
      "\n",
      "Putting Everything Together\n",
      "To make the code reusable, let’s put together everything we have discussed so far into\n",
      "a small helper function: it will create and return a dataset that will efficiently load Cal‐\n",
      "ifornia housing data from multiple CSV files, then shuffle it, preprocess it and batch it\n",
      "(see Figure 13-2):\n",
      "\n",
      "def csv_reader_dataset(filepaths, repeat=None, n_readers=5,\n",
      "                       n_read_threads=None, shuffle_buffer_size=10000,\n",
      "                       n_parse_threads=5, batch_size=32):\n",
      "    dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat)\n",
      "    dataset = dataset.interleave(\n",
      "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
      "        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
      "    dataset = dataset.shuffle(shuffle_buffer_size)\n",
      "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
      "    dataset = dataset.batch(batch_size)\n",
      "    return dataset.prefetch(1)\n",
      "\n",
      "410 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "\n",
      "\fFigure 13-2. Loading and Preprocessing Data From Multiple CSV Files\n",
      "\n",
      "Everything should make sense in this code, except the very last line (prefetch(1)),\n",
      "which is actually quite important for performance.\n",
      "\n",
      "Prefetching\n",
      "By  calling  prefetch(1)  at  the  end,  we  are  creating  a  dataset  that  will  do  its  best  to\n",
      "always be one batch ahead2. In other words, while our training algorithm is working\n",
      "on one batch, the dataset will already be working in parallel on getting the next batch\n",
      "ready. This can improve performance dramatically, as is illustrated on Figure 13-3. If\n",
      "we also ensure that loading and preprocessing are multithreaded (by setting num_par\n",
      "allel_calls  when  calling  interleave()  and  map()),  we  can  exploit  multiple  cores\n",
      "on the CPU and hopefully make preparing one batch of data shorter than running a\n",
      "training step on the GPU: this way the GPU will be almost 100% utilized (except for\n",
      "the data transfer time from the CPU to the GPU), and training will run much faster.\n",
      "\n",
      "2 In general, just prefetching one batch is fine, but in some cases you may need to prefetch a few more. Alterna‐\n",
      "tively, you can let TensorFlow decide automatically by passing tf.data.experimental.AUTOTUNE (this is an\n",
      "experimental feature for now).\n",
      "\n",
      "The Data API \n",
      "\n",
      "| \n",
      "\n",
      "411\n",
      "\n",
      "\fFigure 13-3. Speedup Training Thanks to Prefetching and Multithreading\n",
      "\n",
      "If  you  plan  to  purchase  a  GPU  card,  its  processing  power  and  its\n",
      "memory  size  are  of  course  very  important  (in  particular,  a  large\n",
      "RAM is crucial for computer vision), but its memory bandwidth is\n",
      "just as important as the processing power to get good performance:\n",
      "this  is  the  number  of  gigabytes  of  data  it  can  get  in  or  out  of  its\n",
      "RAM per second.\n",
      "\n",
      "With  that,  you  can  now  build  efficient  input  pipelines  to  load  and  preprocess  data\n",
      "from multiple text files. We have discussed the most common dataset methods, but\n",
      "there  are  a  few  more  you  may  want  to  look  at:  concatenate(),  zip(),  window(),\n",
      "reduce(), cache(), shard(), flat_map() and padded_batch(). There are also a cou‐\n",
      "ple more class methods: from_generator() and from_tensors(), which create a new\n",
      "dataset from a Python generator or a list of tensors respectively. Please check the API\n",
      "documentation for more details. Also note that there are experimental features avail‐\n",
      "able  in  tf.data.experimental,  many  of  which  will  most  likely  make  it  to  the  core\n",
      "API  in  future  releases  (e.g.,  check  out  the  CsvDataset  class  and  the  SqlDataset\n",
      "classes).\n",
      "\n",
      "412 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "\n",
      "\fUsing the Dataset With tf.keras\n",
      "Now we can use the csv_reader_dataset() function to create a dataset for the train‐\n",
      "ing set (ensuring it repeats the data forever), the validation set and the test set:\n",
      "\n",
      "train_set = csv_reader_dataset(train_filepaths, repeat=None)\n",
      "valid_set = csv_reader_dataset(valid_filepaths)\n",
      "test_set = csv_reader_dataset(test_filepaths)\n",
      "\n",
      "And now we can simply build and train a Keras model using these datasets.3 All we\n",
      "need  to  do  is  to  call  the  fit()  method  with  the  datasets  instead  of  X_train  and\n",
      "y_train, and specify the number of steps per epoch for each set:4\n",
      "\n",
      "model = keras.models.Sequential([...])\n",
      "model.compile([...])\n",
      "model.fit(train_set, steps_per_epoch=len(X_train) // batch_size, epochs=10,\n",
      "          validation_data=valid_set,\n",
      "          validation_steps=len(X_valid) // batch_size)\n",
      "\n",
      "Similarly, we can pass a dataset to the evaluate() and predict() methods (and again\n",
      "specify the number of steps per epoch):\n",
      "\n",
      "model.evaluate(test_set, steps=len(X_test) // batch_size)\n",
      "model.predict(new_set, steps=len(X_new) // batch_size)\n",
      "\n",
      "Unlike the other sets, the new_set will usually not contain labels (if it does, Keras will\n",
      "just ignore them). Note that in all these cases, you can still use NumPy arrays instead\n",
      "of datasets if you want (but of course they need to have been loaded and preprocessed\n",
      "first).\n",
      "\n",
      "If you want to build your own custom training loop (as in Chapter 12), you can just\n",
      "iterate over the training set, very naturally:\n",
      "\n",
      "for X_batch, y_batch in train_set:\n",
      "    [...] # perform one gradient descent step\n",
      "\n",
      "In  fact,  it  is  even  possible  to  create  a  tf.function  (see  Chapter  12)  that  performs  the\n",
      "whole training loop!5\n",
      "\n",
      "@tf.function\n",
      "def train(model, optimizer, loss_fn, n_epochs, [...]):\n",
      "    train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs, [...])\n",
      "    for X_batch, y_batch in train_set:\n",
      "        with tf.GradientTape() as tape:\n",
      "\n",
      "3 Support for datasets is specific to tf.keras, it will not work on other implementations of the Keras API.\n",
      "\n",
      "4 The number of steps per epoch is optional if the dataset just goes through the data once, but if you do not\n",
      "\n",
      "specify it, the progress bar will not be displayed during the first epoch.\n",
      "\n",
      "5 Note that for now the dataset must be created within the TF Function. This may be fixed by the time you read\n",
      "\n",
      "these lines (see TensorFlow issue #25414).\n",
      "\n",
      "The Data API \n",
      "\n",
      "| \n",
      "\n",
      "413\n",
      "\n",
      "\f            y_pred = model(X_batch)\n",
      "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
      "            loss = tf.add_n([main_loss] + model.losses)\n",
      "        grads = tape.gradient(loss, model.trainable_variables)\n",
      "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
      "\n",
      "Congratulations, you now know how to build powerful input pipelines using the Data\n",
      "API! However, so far we have used CSV files, which are common, simple and conve‐\n",
      "nient, but they are not really efficient, and they do not support large or complex data\n",
      "structures very well, such as images or audio. So let’s use TFRecords instead.\n",
      "\n",
      "If you are happy with CSV files (or whatever other format you are\n",
      "using), you do not have to use TFRecords. As the saying goes, if it\n",
      "ain’t broke, don’t fix it! TFRecords are useful when the bottleneck\n",
      "during training is loading and parsing the data.\n",
      "\n",
      "The TFRecord Format\n",
      "The TFRecord format is TensorFlow’s preferred format for storing large amounts of\n",
      "data  and  reading  it  efficiently.  It  is  a  very  simple  binary  format  that  just  contains  a\n",
      "sequence  of  binary  records  of  varying  sizes  (each  record  just  has  a  length,  a  CRC\n",
      "checksum to check that the length was not corrupted, then the actual data, and finally\n",
      "a  CRC  checksum  for  the  data).  You  can  easily  create  a  TFRecord  file  using  the\n",
      "tf.io.TFRecordWriter class:\n",
      "\n",
      "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
      "    f.write(b\"This is the first record\")\n",
      "    f.write(b\"And this is the second record\")\n",
      "\n",
      "And  you  can  then  use  a  tf.data.TFRecordDataset  to  read  one  or  more  TFRecord\n",
      "files:\n",
      "\n",
      "filepaths = [\"my_data.tfrecord\"]\n",
      "dataset = tf.data.TFRecordDataset(filepaths)\n",
      "for item in dataset:\n",
      "    print(item)\n",
      "\n",
      "This will output:\n",
      "\n",
      "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
      "tf.Tensor(b'And this is the second record', shape=(), dtype=string)\n",
      "\n",
      "By default, a  TFRecordDataset will read files one by one, but you\n",
      "can  make  it  read  multiple  files  in  parallel  and  interleave  their\n",
      "records  by  setting  num_parallel_reads.  Alternatively,  you  could\n",
      "obtain  the  same  result  by  using  list_files()  and  interleave()\n",
      "as we did earlier to read multiple CSV files.\n",
      "\n",
      "414 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "\n",
      "\fCompressed TFRecord Files\n",
      "It can sometimes be useful to compress your TFRecord files, especially if they need to\n",
      "be loaded via a network connection. You can create a compressed TFRecord file by\n",
      "setting the options argument:\n",
      "\n",
      "options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
      "with tf.io.TFRecordWriter(\"my_compressed.tfrecord\", options) as f:\n",
      "  [...]\n",
      "\n",
      "When reading a compressed TFRecord file, you need to specify the compression type:\n",
      "\n",
      "dataset = tf.data.TFRecordDataset([\"my_compressed.tfrecord\"],\n",
      "                                  compression_type=\"GZIP\")\n",
      "\n",
      "A Brief Introduction to Protocol Buffers\n",
      "Even though each record can use any binary format you want, TFRecord files usually\n",
      "contain serialized Protocol Buffers (also called protobufs). This is a portable, extensi‐\n",
      "ble and efficient binary format developed at Google back in 2001 and Open Sourced\n",
      "in 2008, and they are now widely used, in particular in gRPC, Google’s remote proce‐\n",
      "dure call system. Protocol Buffers are defined using a simple language that looks like\n",
      "this:\n",
      "\n",
      "syntax = \"proto3\";\n",
      "message Person {\n",
      "  string name = 1;\n",
      "  int32 id = 2;\n",
      "  repeated string email = 3;\n",
      "}\n",
      "\n",
      "This definition says we are using the protobuf format version 3, and it specifies that\n",
      "each Person object6 may (optionally) have a name of type string, an id of type int32,\n",
      "and zero or more email fields, each of type string. The numbers 1, 2 and 3 are the\n",
      "field  identifiers:  they  will  be  used  in  each  record’s  binary  representation.  Once  you\n",
      "have a definition in a .proto file, you can compile it. This requires protoc, the proto‐\n",
      "buf compiler, to generate access classes in Python (or some other language). Note that\n",
      "the  protobuf  definitions  we  will  use  have  already  been  compiled  for  you,  and  their\n",
      "Python  classes  are  part  of  TensorFlow,  so  you  will  not  need  to  use  protoc.  All  you\n",
      "need to know is how to use protobuf access classes in Python. To illustrate the basics,\n",
      "let’s  look  at  a  simple  example  that  uses  the  access  classes  generated  for  the  Person\n",
      "protobuf (the code is explained in the comments):\n",
      "\n",
      ">>> from person_pb2 import Person  # import the generated access class\n",
      ">>> person = Person(name=\"Al\", id=123, email=[\"a@b.com\"])  # create a Person\n",
      ">>> print(person)  # display the Person\n",
      "\n",
      "6 Since protobuf objects are meant to be serialized and transmitted, they are called messages.\n",
      "\n",
      "The TFRecord Format \n",
      "\n",
      "| \n",
      "\n",
      "415\n",
      "\n",
      "\fname: \"Al\"\n",
      "id: 123\n",
      "email: \"a@b.com\"\n",
      ">>> person.name  # read a field\n",
      "\"Al\"\n",
      ">>> person.name = \"Alice\"  # modify a field\n",
      ">>> person.email[0]  # repeated fields can be accessed like arrays\n",
      "\"a@b.com\"\n",
      ">>> person.email.append(\"c@d.com\")  # add an email address\n",
      ">>> s = person.SerializeToString()  # serialize the object to a byte string\n",
      ">>> s\n",
      "b'\\n\\x05Alice\\x10{\\x1a\\x07a@b.com\\x1a\\x07c@d.com'\n",
      ">>> person2 = Person()  # create a new Person\n",
      ">>> person2.ParseFromString(s)  # parse the byte string (27 bytes long)\n",
      "27\n",
      ">>> person == person2  # now they are equal\n",
      "True\n",
      "\n",
      "In short, we import the Person class generated by protoc, we create an instance and\n",
      "we  play  with  it,  visualizing  it,  reading  and  writing  some  fields,  then  we  serialize  it\n",
      "using  the  SerializeToString()  method.  This  is  the  binary  data  that  is  ready  to  be\n",
      "saved or transmitted over the network. When reading or receiving this binary data,\n",
      "we can parse it using the ParseFromString() method, and we get a copy of the object\n",
      "that was serialized.7\n",
      "\n",
      "We could save the serialized Person object to a TFRecord file, then we could load and\n",
      "parse it: everything would work fine. However, SerializeToString() and ParseFrom\n",
      "String() are not TensorFlow operations (and neither are the other operations in this\n",
      "code),  so  they  cannot  be  included  in  a  TensorFlow  Function  (except  by  wrapping\n",
      "them in a tf.py_function() operation, which would make the code slower and less\n",
      "portable, as we saw in Chapter 12). Fortunately, TensorFlow does include special pro‐\n",
      "tobuf definitions for which it provides parsing operations.\n",
      "\n",
      "TensorFlow Protobufs\n",
      "The main protobuf typically used in a TFRecord file is the Example protobuf, which\n",
      "represents one instance in a dataset. It contains a list of named features, where each\n",
      "feature can either be a list of byte strings, a list of floats or a list of integers. Here is the\n",
      "protobuf definition:\n",
      "\n",
      "syntax = \"proto3\";\n",
      "message BytesList { repeated bytes value = 1; }\n",
      "message FloatList { repeated float value = 1 [packed = true]; }\n",
      "message Int64List { repeated int64 value = 1 [packed = true]; }\n",
      "\n",
      "7 This chapter contains the bare minimum you need to know about protobufs to use TFRecords. To learn more\n",
      "\n",
      "about protobufs, please visit https://homl.info/protobuf.\n",
      "\n",
      "416 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "\n",
      "\fmessage Feature {\n",
      "    oneof kind {\n",
      "        BytesList bytes_list = 1;\n",
      "        FloatList float_list = 2;\n",
      "        Int64List int64_list = 3;\n",
      "    }\n",
      "};\n",
      "message Features { map<string, Feature> feature = 1; };\n",
      "message Example { Features features = 1; };\n",
      "\n",
      "The definitions of BytesList, FloatList and Int64List are straightforward enough\n",
      "([packed = true] is used for repeated numerical fields, for a more efficient encod‐\n",
      "ing).  A  Feature  either  contains  a  BytesList,  a  FloatList  or  an  Int64List.  A  Fea\n",
      "tures  (with  an  s)  contains  a  dictionary  that  maps  a  feature  name  to  the\n",
      "corresponding feature value. And finally, an Example just contains a Features object.8\n",
      "Here is how you could create a  tf.train.Example representing the same person as\n",
      "earlier, and write it to TFRecord file:\n",
      "\n",
      "from tensorflow.train import BytesList, FloatList, Int64List\n",
      "from tensorflow.train import Feature, Features, Example\n",
      "\n",
      "person_example = Example(\n",
      "    features=Features(\n",
      "        feature={\n",
      "            \"name\": Feature(bytes_list=BytesList(value=[b\"Alice\"])),\n",
      "            \"id\": Feature(int64_list=Int64List(value=[123])),\n",
      "            \"emails\": Feature(bytes_list=BytesList(value=[b\"a@b.com\",\n",
      "                                                          b\"c@d.com\"]))\n",
      "        }))\n",
      "\n",
      "The code is a bit verbose and repetitive, but it’s rather straightforward (and you could\n",
      "easily wrap it inside a small helper function). Now that we have an Example protobuf,\n",
      "we can serialize it by calling its SerializeToString() method, then write the result‐\n",
      "ing data to a TFRecord file:\n",
      "\n",
      "with tf.io.TFRecordWriter(\"my_contacts.tfrecord\") as f:\n",
      "    f.write(person_example.SerializeToString())\n",
      "\n",
      "Normally you would write much more than just one example! Typically, you would\n",
      "create a conversion script that reads from your current format (say, CSV files), creates\n",
      "an  Example  protobuf  for  each  instance,  serializes  them  and  saves  them  to  several\n",
      "TFRecord files, ideally shuffling them in the process. This requires a bit of work, so\n",
      "once  again  make  sure  it  is  really  necessary  (perhaps  your  pipeline  works  fine  with\n",
      "CSV files).\n",
      "\n",
      "8 Why was Example even defined since it contains no more than a Features object? Well, TensorFlow may one\n",
      "day decide to add more fields to it. As long as the new Example definition still contains the features field,\n",
      "with the same id, it will be backward compatible. This extensibility is one of the great features of protobufs.\n",
      "\n",
      "The TFRecord Format \n",
      "\n",
      "| \n",
      "\n",
      "417\n",
      "\n",
      "\fNow  that  we  have  a  nice  TFRecord  file  containing  a  serialized  Example,  let’s  try  to\n",
      "load it.\n",
      "\n",
      "Loading and Parsing Examples\n",
      "To  load  the  serialized  Example  protobufs,  we  will  use  a  tf.data.TFRecordDataset\n",
      "once again, and we will parse each  Example using  tf.io.parse_single_example().\n",
      "This is a TensorFlow operation so it can be included in a TF Function. It requires at\n",
      "least  two  arguments:  a  string  scalar  tensor  containing  the  serialized  data,  and  a\n",
      "description  of  each  feature.  The  description  is  a  dictionary  that  maps  each  feature\n",
      "name  to  either  a  tf.io.FixedLenFeature  descriptor  indicating  the  feature’s  shape,\n",
      "type and default value, or a tf.io.VarLenFeature descriptor indicating only the type\n",
      "(if the length may vary, such as for the \"emails\" feature). For example:\n",
      "\n",
      "feature_description = {\n",
      "    \"name\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
      "    \"id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
      "    \"emails\": tf.io.VarLenFeature(tf.string),\n",
      "}\n",
      "\n",
      "for serialized_example in tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]):\n",
      "    parsed_example = tf.io.parse_single_example(serialized_example,\n",
      "                                                feature_description)\n",
      "\n",
      "The  fixed  length  features  are  parsed  as  regular  tensors,  but  the  variable  length  fea‐\n",
      "tures are parsed as sparse tensors. You can convert a sparse tensor to a dense tensor\n",
      "using tf.sparse.to_dense(), but in this case it is simpler to just access its values:\n",
      "\n",
      ">>> tf.sparse.to_dense(parsed_example[\"emails\"], default_value=b\"\")\n",
      "<tf.Tensor: [...] dtype=string, numpy=array([b'a@b.com', b'c@d.com'], [...])>\n",
      ">>> parsed_example[\"emails\"].values\n",
      "<tf.Tensor: [...] dtype=string, numpy=array([b'a@b.com', b'c@d.com'], [...])>\n",
      "\n",
      "A BytesList can contain any binary data you want, including any serialized object.\n",
      "For example, you can use tf.io.encode_jpeg() to encode an image using the JPEG\n",
      "format,  and  put  this  binary  data  in  a  BytesList.  Later,  when  your  code  reads  the\n",
      "TFRecord,  it  will  start  by  parsing  the  Example,  then  you  will  need  to  call\n",
      "tf.io.decode_jpeg()  to  parse  the  data  and  get  the  original  image  (or  you  can  use\n",
      "tf.io.decode_image(), which can decode any BMP, GIF, JPEG or PNG image). You\n",
      "can  also  store  any  tensor  you  want  in  a  BytesList  by  serializing  the  tensor  using\n",
      "tf.io.serialize_tensor(),  then  putting  the  resulting  byte  string  in  a  BytesList\n",
      "feature.  Later,  when  you  parse  the  TFRecord,  you  can  parse  this  data  using\n",
      "tf.io.parse_tensor().\n",
      "\n",
      "Instead of parsing examples one by one using tf.io.parse_single_example(), you\n",
      "may want to parse them batch by batch using tf.io.parse_example():\n",
      "\n",
      "418 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "\n",
      "\fdataset = tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]).batch(10)\n",
      "for serialized_examples in dataset:\n",
      "    parsed_examples = tf.io.parse_example(serialized_examples,\n",
      "                                          feature_description)\n",
      "\n",
      "As  you  can  see,  the  Example  proto  will  probably  be  sufficient  for  most  use  cases.\n",
      "However, it may be a bit cumbersome to use when you are dealing with lists of lists.\n",
      "For  example,  suppose  you  want  to  classify  text  documents.  Each  document  may  be\n",
      "represented  as  a  list  of  sentences,  where  each  sentence  is  represented  as  a  list  of\n",
      "words.  And  perhaps  each  document  also  has  a  list  of  comments,  where  each  com‐\n",
      "ment is also represented as a list of words. Moreover, there may be some contextual\n",
      "data as well, such as the document’s author, title and publication date. TensorFlow’s\n",
      "SequenceExample protobuf is designed for such use cases.\n",
      "\n",
      "Handling Lists of Lists Using the SequenceExample Protobuf\n",
      "Here is the definition of the SequenceExample protobuf:\n",
      "\n",
      "message FeatureList { repeated Feature feature = 1; };\n",
      "message FeatureLists { map<string, FeatureList> feature_list = 1; };\n",
      "message SequenceExample {\n",
      "    Features context = 1;\n",
      "    FeatureLists feature_lists = 2;\n",
      "};\n",
      "\n",
      "A  SequenceExample  contains  a  Features  object  for  the  contextual  data  and  a  Fea\n",
      "tureLists  object  which  contains  one  or  more  named  FeatureList  objects  (e.g.,  a\n",
      "FeatureList named \"content\" and another named \"comments\"). Each FeatureList\n",
      "just contains a list of Feature objects, each of which may be a list of byte strings, a list\n",
      "of 64-bit integers or a list of floats (in this example, each Feature would represent a\n",
      "sentence or a comment, perhaps in the form of a list of word identifiers). Building a\n",
      "SequenceExample, serializing it and parsing it is very similar to building, serializing\n",
      "and  parsing  an  Example,  but  you  must  use  tf.io.parse_single_sequence_exam\n",
      "ple()  to  parse  a  single  SequenceExample  or  tf.io.parse_sequence_example()  to\n",
      "parse a batch, and both functions return a tuple containing the context features (as a\n",
      "dictionary)  and  the  feature  lists  (also  as  a  dictionary).  If  the  feature  lists  contain\n",
      "sequences of varying sizes (as in the example above), you may want to convert them\n",
      "to ragged tensors using tf.RaggedTensor.from_sparse() (see the notebook for the\n",
      "full code):\n",
      "\n",
      "parsed_context, parsed_feature_lists = tf.io.parse_single_sequence_example(\n",
      "    serialized_sequence_example, context_feature_descriptions,\n",
      "    sequence_feature_descriptions)\n",
      "parsed_content = tf.RaggedTensor.from_sparse(parsed_feature_lists[\"content\"])\n",
      "\n",
      "Now that you know how to efficiently store, load and parse data, the next step is to\n",
      "prepare it so that it can be fed to a neural network. This means converting all features\n",
      "\n",
      "The TFRecord Format \n",
      "\n",
      "| \n",
      "\n",
      "419\n",
      "\n",
      "\finto numerical features (ideally not too sparse), scaling them, and more. In particular,\n",
      "if your data contains categorical features or text features, they need to be converted to\n",
      "numbers. For this, the Features API can help.\n",
      "\n",
      "The Features API\n",
      "Preprocessing  your  data  can  be  performed  in  many  ways:  it  can  be  done  ahead  of\n",
      "time when preparing your data files, using any tool you like. Or you can preprocess\n",
      "your data on the fly when loading it with the Data API (e.g., using the dataset’s map()\n",
      "method, as we saw earlier). Or you can include a preprocessing layer directly in your\n",
      "model.  Whichever  solution  you  prefer,  the  Features  API  can  help  you:  it  is  a  set  of\n",
      "functions  available  in  the  tf.feature_column  package,  which  let  you  define  how\n",
      "each feature (or group of features) in your data should be preprocessed (therefore you\n",
      "can  think  of  this  API  as  the  analog  of  Scikit-Learn’s  ColumnTransformer  class).  We\n",
      "will start by looking at the different types of columns available, and then we will look\n",
      "at how to use them.\n",
      "\n",
      "Let’s go back to the variant of the California housing dataset that we used in Chap‐\n",
      "ter 2, since it includes a categorical feature and missing data. Here is a simple numeri‐\n",
      "cal column named \"housing_median_age\":\n",
      "\n",
      "housing_median_age = tf.feature_column.numeric_column(\"housing_median_age\")\n",
      "\n",
      "Numeric columns let you specify a normalization function using the normalizer_fn\n",
      "argument. For example, let’s tweak the \"housing_median_age\" column to define how\n",
      "it  should  be  scaled.  Note  that  this  requires  computing  ahead  of  time  the  mean  and\n",
      "standard deviation of this feature in the training set:\n",
      "\n",
      "age_mean, age_std = X_mean[1], X_std[1]  # The median age is column in 1\n",
      "housing_median_age = tf.feature_column.numeric_column(\n",
      "    \"housing_median_age\", normalizer_fn=lambda x: (x - age_mean) / age_std)\n",
      "\n",
      "In some cases, it might improve performance to bucketize some numerical features,\n",
      "effectively  transforming  a  numerical  feature  into  a  categorical  feature.  For  example,\n",
      "let’s create a bucketized column based on the median_income column, with 5 buckets:\n",
      "less than 1.5 ($15,000), then 1.5 to 3, 3 to 4.5, 4.5 to 6., and above 6. (notice that when\n",
      "you specify 4 boundaries, there are actually 5 buckets):\n",
      "\n",
      "median_income = tf.feature_column.numeric_column(\"median_income\")\n",
      "bucketized_income = tf.feature_column.bucketized_column(\n",
      "    median_income, boundaries=[1.5, 3., 4.5, 6.])\n",
      "\n",
      "If the median_income feature is equal to, say, 3.2, then the bucketized_income feature\n",
      "will automatically be equal to 2 (i.e., the index of the corresponding income bucket).\n",
      "Choosing the right boundaries can be somewhat of an art, but one approach is to just\n",
      "use percentiles of the data (e.g., the 10th percentile, the 20th percentile, and so on). If\n",
      "a  feature  is  multimodal,  meaning  it  has  separate  peaks  in  its  distribution,  you  may\n",
      "\n",
      "420 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "\n",
      "\fwant to define a bucket for each mode, placing the boundaries in between the peaks.\n",
      "Whether you use the percentiles or the modes, you need to analyze the distribution of\n",
      "your data ahead of time, just like we had to measure the mean and standard deviation\n",
      "ahead of time to normalize the housing_median_age column.\n",
      "\n",
      "Categorical Features\n",
      "For  categorical  features  such  as  ocean_proximity,  there  are  several  options.  If  it  is\n",
      "already represented as a category ID (i.e., an integer from 0 to the max ID), then you\n",
      "can  use  the  categorical_column_with_identity()  function  (specifying  the  max\n",
      "ID). If not, and you know the list of all possible categories, then you can use categori\n",
      "cal_column_with_vocabulary_list():\n",
      "\n",
      "ocean_prox_vocab = ['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN']\n",
      "ocean_proximity = tf.feature_column.categorical_column_with_vocabulary_list(\n",
      "    \"ocean_proximity\", ocean_prox_vocab)\n",
      "\n",
      "If you prefer to have TensorFlow load the vocabulary from a file, you can call catego\n",
      "rical_column_with_vocabulary_file()  instead.  As  you  might  expect,  these  two\n",
      "functions  will  simply  map  each  category  to  its  index  in  the  vocabulary  (e.g.,  NEAR\n",
      "BAY will be mapped to 3), and unknown categories will be mapped to -1.\n",
      "\n",
      "For  categorical  columns  with  a  large  vocabulary  (e.g.,  for  zipcodes,  cities,  words,\n",
      "products,  users,  etc.),  it  may  not  be  convenient  to  get  the  full  list  of  possible  cate‐\n",
      "gories, or perhaps categories may be added or removed so frequently that using cate‐\n",
      "gory  indices  would  be  too  unreliable.  In  this  case,  you  may  prefer  to  use  a\n",
      "categorical_column_with_hash_bucket(). If we had a \"city\" feature in the dataset,\n",
      "we could encode it like this:\n",
      "\n",
      "city_hash = tf.feature_column.categorical_column_with_hash_bucket(\n",
      "    \"city\", hash_bucket_size=1000)\n",
      "\n",
      "This  feature  will  compute  a  hash  for  each  category  (i.e.,  for  each  city),  modulo  the\n",
      "number  of  hash  buckets  (hash_bucket_size).  You  must  set  the  number  of  buckets\n",
      "high enough to avoid getting too many collisions (i.e., different categories ending up\n",
      "in  the  same  bucket),  but  the  higher  you  set  it,  the  more  RAM  will  be  used  (by  the\n",
      "embedding table, as we will see shortly).\n",
      "\n",
      "Crossed Categorical Features\n",
      "If you suspect that two (or more) categorical features are more meaningful when used\n",
      "jointly, then you can create a crossed column. For example, suppose people are partic‐\n",
      "ularly fond of old houses inland and new houses near the ocean, then it might help to\n",
      "\n",
      "The Features API \n",
      "\n",
      "| \n",
      "\n",
      "421\n",
      "\n",
      "\fcreate  a  bucketized  column  for  the  housing_median_age  feature9,  and  cross  it  with\n",
      "the ocean_proximity column. The crossed column will compute a hash of every age\n",
      "& ocean proximity combination it comes across, modulo the hash_bucket_size, and\n",
      "this will give it the cross category ID. You may then choose to use only this crossed\n",
      "column in your model, or also include the individual columns.\n",
      "\n",
      "bucketized_age = tf.feature_column.bucketized_column(\n",
      "    housing_median_age, boundaries=[-1., -0.5, 0., 0.5, 1.]) # age was scaled\n",
      "age_and_ocean_proximity = tf.feature_column.crossed_column(\n",
      "    [bucketized_age, ocean_proximity], hash_bucket_size=100)\n",
      "\n",
      "Another common use case for crossed columns is to cross latitude and longitude into\n",
      "a  single  categorical  feature:  you  start  by  bucketizing  the  latitude  and  longitude,  for\n",
      "example  into  20  buckets  each,  then  you  cross  these  bucketized  features  into  a  loca\n",
      "tion column. This will create a 20×20 grid over California, and each cell in the grid\n",
      "will correspond to one category:\n",
      "\n",
      "latitude = tf.feature_column.numeric_column(\"latitude\")\n",
      "longitude = tf.feature_column.numeric_column(\"longitude\")\n",
      "bucketized_latitude = tf.feature_column.bucketized_column(\n",
      "    latitude, boundaries=list(np.linspace(32., 42., 20 - 1)))\n",
      "bucketized_longitude = tf.feature_column.bucketized_column(\n",
      "    longitude, boundaries=list(np.linspace(-125., -114., 20 - 1)))\n",
      "location = tf.feature_column.crossed_column(\n",
      "    [bucketized_latitude, bucketized_longitude], hash_bucket_size=1000)\n",
      "\n",
      "Encoding Categorical Features Using One-Hot Vectors\n",
      "No  matter  which  option  you  choose  to  build  a  categorical  feature  (categorical  col‐\n",
      "umns, bucketized columns or crossed columns), it must be encoded before you can\n",
      "feed  it  to  a  neural  network.  There  are  two  options  to  encode  a  categorical  feature:\n",
      "one-hot  vectors  or  embeddings.  For  the  first  option,  simply  use  the  indicator_col\n",
      "umn() function:\n",
      "\n",
      "ocean_proximity_one_hot = tf.feature_column.indicator_column(ocean_proximity)\n",
      "\n",
      "A one-hot vector encoding has the size of the vocabulary length, which is fine if there\n",
      "are just a few possible categories, but if the vocabulary is large, you will end up with\n",
      "too many inputs fed to your neural network: it will have too many weights to learn\n",
      "and it will probably not perform very well. In particular, this will typically be the case\n",
      "when  you  use  hash  buckets.  In  this  case,  you  should  probably  encode  them  using\n",
      "embeddings instead.\n",
      "\n",
      "9 Since the housing_median_age feature was normalized, the boundaries are for normalized ages.\n",
      "\n",
      "422 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "\n",
      "\fAs a rule of thumb (but your mileage may vary!), if the number of\n",
      "categories is lower than 10, then one-hot encoding is generally the\n",
      "way to go. If the number of categories is greater than 50 (which is\n",
      "often  the  case  when  you  use  hash  buckets),  then  embeddings  are\n",
      "usually preferable. In between 10 and 50 categories, you may want\n",
      "to experiment with both options and see which one works best for\n",
      "your  use  case.  Also,  embeddings  typically  require  more  training\n",
      "data, unless you can reuse pretrained embeddings.\n",
      "\n",
      "Encoding Categorical Features Using Embeddings\n",
      "An  embedding  is  a  trainable  dense  vector  that  represents  a  category.  By  default,\n",
      "embeddings are initialized randomly, so for example the \"NEAR BAY\" category could\n",
      "be represented initially by a random vector such as [0.131, 0.890], while the \"NEAR\n",
      "OCEAN\"  category  may  be  represented  by  another  random  vector  such  as  [0.631,\n",
      "0.791] (in this example, we are using 2D embeddings, but the number of dimensions\n",
      "is a hyperparameter you can tweak). Since these embeddings are trainable, they will\n",
      "gradually  improve  during  training,  and  as  they  represent  fairly  similar  categories,\n",
      "Gradient  Descent  will  certainly  end  up  pushing  them  closer  together,  while  it  will\n",
      "tend to move them away from the \"INLAND\" category’s embedding (see Figure 13-4).\n",
      "Indeed,  the  better  the  representation,  the  easier  it  will  be  for  the  neural  network  to\n",
      "make accurate predictions, so training tends to make embeddings useful representa‐\n",
      "tions of the categories. This is called representation learning (we will see other types of\n",
      "representation learning in ???).\n",
      "\n",
      "The Features API \n",
      "\n",
      "| \n",
      "\n",
      "423\n",
      "\n",
      "\fFigure 13-4. Embeddings Will Gradually Improve During Training\n",
      "\n",
      "Word Embeddings\n",
      "Not only will embeddings generally be useful representations for the task at hand, but\n",
      "quite often these same embeddings can be reused successfully for other tasks as well.\n",
      "The most common example of this is word embeddings (i.e., embeddings of individual\n",
      "words): when you are working on a natural language processing task, you are often\n",
      "better off reusing pretrained word embeddings than training your own. The idea of\n",
      "using  vectors  to  represent  words  dates  back  to  the  1960s,  and  many  sophisticated\n",
      "techniques  have  been  used  to  generate  useful  vectors,  including  using  neural  net‐\n",
      "works,  but  things  really  took  off  in  2013,  when  Tomáš  Mikolov  and  other  Google\n",
      "researchers published a paper10 describing how to learn word embeddings using deep\n",
      "neural  networks,  much  faster  than  previous  attempts.  This  allowed  them  to  learn\n",
      "embeddings on a very large corpus of text: they trained a deep neural network to pre‐\n",
      "dict  the  words  near  any  given  word.  This  allowed  them  to  obtain  astounding  word\n",
      "embeddings.  For  example,  synonyms  had  very  close  embeddings,  and  semantically\n",
      "related words such as France, Spain, Italy, and so on, ended up clustered together. But\n",
      "it’s not just about proximity: word embeddings were also organized along meaningful\n",
      "axes in the embedding space. Here is a famous example: if you compute King – Man\n",
      "+ Woman (adding and subtracting the embedding vectors of these words), then the\n",
      "result  will  be  very  close  to  the  embedding  of  the  word  Queen  (see  Figure  13-5).  In\n",
      "other words, the word embeddings encode the concept of gender! Similarly, you can\n",
      "compute  Madrid  –  Spain  +  France,  and  of  course  the  result  is  close  to  Paris,  which\n",
      "seems to show that the notion of capital city was also encoded in the embeddings.\n",
      "\n",
      "10 “Distributed Representations of Words and Phrases and their Compositionality”, T. Mikolov et al. (2013).\n",
      "\n",
      "424 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "\n",
      "\fFigure 13-5. Word Embeddings\n",
      "\n",
      "Let’s go back to the Features API. Here is how you could encode the ocean_proxim\n",
      "ity categories as 2D embeddings:\n",
      "\n",
      "ocean_proximity_embed = tf.feature_column.embedding_column(ocean_proximity,\n",
      "                                                           dimension=2)\n",
      "\n",
      "Each of the five ocean_proximity categories will now be represented as a 2D vector.\n",
      "These vectors are stored in an embedding matrix with one row per category, and one\n",
      "column  per  embedding  dimension,  so  in  this  example  it  is  a  5×2  matrix.  When  an\n",
      "embedding column is given a category index as input (say, 3, which corresponds to\n",
      "the  category  \"NEAR  BAY\"),  it  just  performs  a  lookup  in  the  embedding  matrix  and\n",
      "returns the corresponding row (say, [0.331, 0.190]). Unfortunately, the embedding\n",
      "matrix can be quite large, especially when you have a large vocabulary: if this is the\n",
      "case, the model can only learn good representations for the categories for which it has\n",
      "sufficient  training  data.  To  reduce  the  size  of  the  embedding  matrix,  you  can  of\n",
      "course try lowering the dimension hyperparameter, but if you reduce this parameter\n",
      "too much, the representations may not be as good. Another option is to reduce the\n",
      "vocabulary size (e.g., if you are dealing with text, you can try dropping the rare words\n",
      "from the vocabulary, and replace them all with a token like \"<unknown>\" or \"<UNK>\").\n",
      "If you are using hash buckets, you can also try reducing the hash_bucket_size (but\n",
      "not too much, or else you will get collisions).\n",
      "\n",
      "The Features API \n",
      "\n",
      "| \n",
      "\n",
      "425\n",
      "\n",
      "\fIf  there  are  no  pretrained  embeddings  that  you  can  reuse  for  the\n",
      "task you are trying to tackle, and if you do not have enough train‐\n",
      "ing  data  to  learn  them,  then  you  can  try  to  learn  them  on  some\n",
      "auxiliary task for which it is easier to obtain plenty of training data.\n",
      "After  that,  you  can  reuse  the  trained  embeddings  for  your  main\n",
      "task.\n",
      "\n",
      "Using Feature Columns for Parsing\n",
      "Let’s suppose you have created feature columns for each of your input features, as well\n",
      "as for the target. What can you do with them? Well, for one you can pass them to the\n",
      "make_parse_example_spec() function to generate feature descriptions (so you don’t\n",
      "have to do it manually, as we did earlier):\n",
      "\n",
      "columns = [bucketized_age, ....., median_house_value] # all features + target\n",
      "feature_descriptions = tf.feature_column.make_parse_example_spec(columns)\n",
      "\n",
      "You don’t always have to create a separate feature column for each\n",
      "and every feature. For example, instead of having 2 numerical fea‐\n",
      "ture  columns,  you  could  choose  to  have  a  single  2D  column:  just\n",
      "set shape=[2] when calling numerical_column().\n",
      "\n",
      "You  can  then  create  a  function  that  parses  serialized  examples  using  these  feature\n",
      "descriptions, and separates the target column from the input features:\n",
      "\n",
      "def parse_examples(serialized_examples):\n",
      "    examples = tf.io.parse_example(serialized_examples, feature_descriptions)\n",
      "    targets = examples.pop(\"median_house_value\") # separate the targets\n",
      "    return examples, targets\n",
      "\n",
      "Next, you can create a TFRecordDataset that will read batches of serialized examples\n",
      "(assuming the TFRecord file contains serialized Example protobufs with the appropri‐\n",
      "ate features):\n",
      "\n",
      "batch_size = 32\n",
      "dataset = tf.data.TFRecordDataset([\"my_data_with_features.tfrecords\"])\n",
      "dataset = dataset.repeat().shuffle(10000).batch(batch_size).map(parse_examples)\n",
      "\n",
      "Using Feature Columns in Your Models\n",
      "Feature  columns  can  also  be  used  directly  in  your  model,  to  convert  all  your  input\n",
      "features  into  a  single  dense  vector  which  the  neural  network  can  then  process.  For\n",
      "this, all you need to do is add a keras.layers.DenseFeatures layer as the first layer\n",
      "in your model, passing it the list of feature columns (excluding the target column):\n",
      "\n",
      "columns_without_target = columns[:-1]\n",
      "model = keras.models.Sequential([\n",
      "    keras.layers.DenseFeatures(feature_columns=columns_without_target),\n",
      "\n",
      "426 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "\n",
      "\f    keras.layers.Dense(1)\n",
      "])\n",
      "model.compile(loss=\"mse\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
      "steps_per_epoch = len(X_train) // batch_size\n",
      "history = model.fit(dataset, steps_per_epoch=steps_per_epoch, epochs=5)\n",
      "\n",
      "The DenseFeatures layer will take care of converting every input feature to a dense\n",
      "representation,  and  it  will  also  apply  any  extra  transformation  we  specified,  such  as\n",
      "scaling the housing_median_age using the normalizer_fn function we provided. You\n",
      "can take a closer look at what the DenseFeatures layer does by calling it directly:\n",
      "\n",
      ">>> some_columns = [ocean_proximity_embed, bucketized_income]\n",
      ">>> dense_features = keras.layers.DenseFeatures(some_columns)\n",
      ">>> dense_features({\n",
      "...     \"ocean_proximity\": [[\"NEAR OCEAN\"], [\"INLAND\"], [\"INLAND\"]],\n",
      "...     \"median_income\": [[3.], [7.2], [1.]]\n",
      "... })\n",
      "...\n",
      "<tf.Tensor: id=559790, shape=(3, 7), dtype=float32, numpy=\n",
      "array([[ 0. , 0. , 1. , 0. , 0. ,-0.36277947 , 0.30109018],\n",
      "       [ 0. , 0. , 0. , 0. , 1. , 0.22548223 , 0.33142096],\n",
      "       [ 1. , 0. , 0. , 0. , 0. , 0.22548223 , 0.33142096]], dtype=float32)>\n",
      "\n",
      "In this example, we create a DenseFeatures layer with just two columns, and we call\n",
      "it with some data, in the form of a dictionary of features. In this case, since the bucke\n",
      "tized_income  column  relies  on  the  median_income  column,  the  dictionary  must\n",
      "include  the  \"median_income\"  key,  and  similarly  since  the  ocean_proximity_embed\n",
      "column  is  based  on  the  ocean_proximity  column,  the  dictionary  must  include  the\n",
      "\"ocean_proximity\" key. Columns are handled in alphabetical order, so first we look\n",
      "at the bucketized income column (its name is the same as the median_income column\n",
      "name, plus \"_bucketized\"). The incomes 3, 7.2 and 1 get mapped respectively to cat‐\n",
      "egory 2 (for incomes between 1.5 and 3), category 0 (for incomes below 1.5), and cat‐\n",
      "egory 4 (for incomes greater than 6). Then these category IDs get one-hot encoded:\n",
      "category 2 gets encoded as [0., 0., 1., 0., 0.] and so on (note that bucketized\n",
      "columns get one-hot encoded by default, no need to call indicator_column()). Now\n",
      "on  to  the  ocean_proximity_embed  column.  The  \"NEAR  OCEAN\"  and  \"INLAND\"  cate‐\n",
      "gories  just  get  mapped  to  their  respective  embeddings  (which  were  initialized  ran‐\n",
      "domly).  The  resulting  tensor  is  the  concatenation  of  the  one-hot  vectors  and  the\n",
      "embeddings.\n",
      "\n",
      "Now you can feed all kinds of features to a neural network, including numerical fea‐\n",
      "tures, categorical features, and even text (by splitting the text into words, then using\n",
      "word  embedding)!  However,  performing  all  the  preprocessing  on  the  fly  can  slow\n",
      "down training. Let’s see how this can be improved.\n",
      "\n",
      "The Features API \n",
      "\n",
      "| \n",
      "\n",
      "427\n",
      "\n",
      "\fTF Transform\n",
      "If preprocessing is computationally expensive, then handling it before training rather\n",
      "than on the fly may give you a significant speedup: the data will be preprocessed just\n",
      "once per instance before training, rather than once per instance and per epoch during\n",
      "training. Tools like Apache Beam let you run efficient data processing pipelines over\n",
      "large amounts of data, even distributed across multiple servers, so why not use it to\n",
      "preprocess all the training data? This works great and indeed can speed up training,\n",
      "but there is one problem: once your model is trained, suppose you want to deploy it\n",
      "to a mobile app: you will need to write some code in your app to take care of prepro‐\n",
      "cessing the data before it is fed to the model. And suppose you also want to deploy\n",
      "the model to TensorFlow.js so it runs in a web browser? Once again, you will need to\n",
      "write  some  preprocessing  code.  This  can  become  a  maintenance  nightmare:  when‐\n",
      "ever you want to change the preprocessing logic, you will need to update your Apache\n",
      "Beam code, your mobile app code and your Javascript code. It is not only time con‐\n",
      "suming,  but  also  error  prone:  you  may  end  up  with  subtle  differences  between  the\n",
      "preprocessing operations performed before training and the ones performed in your\n",
      "app or in the browser. This training/serving skew will lead to bugs or degraded perfor‐\n",
      "mance.\n",
      "\n",
      "One improvement would be to take the trained model (trained on data that was pre‐\n",
      "processed  by  your  Apache  Beam  code),  and  before  deploying  it  to  your  app  or  the\n",
      "browser, add an extra input layer to take care of preprocessing on the fly (either by\n",
      "writing  a  custom  layer  or  by  using  a  DenseFeatures  layer).  That’s  definitely  better,\n",
      "since now you just have two versions of your preprocessing code: the Apache Beam\n",
      "code and the preprocessing layer’s code.\n",
      "\n",
      "But  what  if  you  could  define  your  preprocessing  operations  just  once?  This  is  what\n",
      "TF Transform was designed for. It is part of TensorFlow Extended (TFX), an end-to-\n",
      "end platform for productionizing TensorFlow models. First, to use a TFX component,\n",
      "such as TF Transform, you must install it, it does not come bundled with TensorFlow.\n",
      "You define your preprocessing function just once (in Python), by using TF Transform\n",
      "functions for scaling, bucketizing, crossing features, and more. You can also use any\n",
      "TensorFlow operation you need. Here is what this preprocessing function might look\n",
      "like if we just had two features:\n",
      "\n",
      "import tensorflow_transform as tft\n",
      "\n",
      "def preprocess(inputs):  # inputs is a batch of input features\n",
      "    median_age = inputs[\"housing_median_age\"]\n",
      "    ocean_proximity = inputs[\"ocean_proximity\"]\n",
      "    standardized_age = tft.scale_to_z_score(median_age - tft.mean(median_age))\n",
      "    ocean_proximity_id = tft.compute_and_apply_vocabulary(ocean_proximity)\n",
      "    return {\n",
      "        \"standardized_median_age\": standardized_age,\n",
      "\n",
      "428 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "\n",
      "\f        \"ocean_proximity_id\": ocean_proximity_id\n",
      "    }\n",
      "\n",
      "Next, TF Transform lets you apply this preprocess() function to the whole training\n",
      "set using Apache Beam (it provides an AnalyzeAndTransformDataset class that you\n",
      "can use for this purpose in your Apache Beam pipeline). In the process, it will also\n",
      "compute all the necessary statistics over the whole training set: in this example, the\n",
      "mean and standard deviation of the housing_median_age feature, and the vocabulary\n",
      "for  the  ocean_proximity  feature.  The  components  that  compute  these  statistics  are\n",
      "called analyzers.\n",
      "\n",
      "Importantly, TF Transform will also generate an equivalent TensorFlow Function that\n",
      "you can plug into the model you deploy. This TF Function contains all the necessary\n",
      "statistics computed by Apache Beam (the mean, standard deviation, and vocabulary),\n",
      "simply included as constants.\n",
      "\n",
      "At  the  time  of  this  writing,  TF  Transform  only  supports  Tensor‐\n",
      "Flow  1.  Moreover,  Apache  Beam  only  has  partial  support  for\n",
      "Python  3.  That  said,  both  these  limitations  will  likely  be  fixed  by\n",
      "the time your read this.\n",
      "\n",
      "With  the  Data  API,  TFRecords,  the  Features  API  and  TF  Transform,  you  can  build\n",
      "highly  scalable  input  pipelines  for  training,  and  also  benefit  from  fast  and  portable\n",
      "data preprocessing in production.\n",
      "\n",
      "But  what  if  you  just  wanted  to  use  a  standard  dataset?  Well  in  that  case,  things  are\n",
      "much simpler: just use TFDS!\n",
      "\n",
      "The TensorFlow Datasets (TFDS) Project\n",
      "The TensorFlow Datasets project makes it trivial to download common datasets, from\n",
      "small ones like MNIST or Fashion MNIST, to huge datasets like ImageNet11 (you will\n",
      "need quite a bit of disk space!). The list includes image datasets, text datasets (includ‐\n",
      "ing  translation  datasets),  audio  and  video  datasets,  and  more.  You  can  visit  https://\n",
      "homl.info/tfds to view the full list, along with a description of each dataset.\n",
      "\n",
      "TFDS  is  not  bundled  with  TensorFlow,  so  you  need  to  install  the  tensorflow-\n",
      "datasets  library  (e.g.,  using  pip).  Then  all  you  need  to  do  is  call  the  tfds.load()\n",
      "function, and it will download the data you want (unless it was already downloaded\n",
      "earlier),  and  return  the  data  as  a  dictionary  of  Datasets  (typically  one  for  training,\n",
      "\n",
      "11 At the time of writing, TFDS requires you to download a few files manually for ImageNet (for legal reasons),\n",
      "\n",
      "but this will hopefully get resolved soon.\n",
      "\n",
      "The TensorFlow Datasets (TFDS) Project \n",
      "\n",
      "| \n",
      "\n",
      "429\n",
      "\n",
      "\fand  one  for  testing,  but  this  depends  on  the  dataset  you  choose).  For  example,  let’s\n",
      "download MNIST:\n",
      "\n",
      "import tensorflow_datasets as tfds\n",
      "\n",
      "dataset = tfds.load(name=\"mnist\")\n",
      "mnist_train, mnist_test = dataset[\"train\"], dataset[\"test\"]\n",
      "\n",
      "You  can  then  apply  any  transformation  you  want  (typically  repeating,  batching  and\n",
      "prefetching), and you’re ready to train your model. Here is a simple example:\n",
      "\n",
      "mnist_train = mnist_train.repeat(5).batch(32).prefetch(1)\n",
      "for item in mnist_train:\n",
      "    images = item[\"image\"]\n",
      "    labels = item[\"label\"]\n",
      "    [...]\n",
      "\n",
      "In general, load() returns a shuffled training set, so there’s no need\n",
      "to shuffle it some more.\n",
      "\n",
      "Note that each item in the dataset is a dictionary containing both the features and the\n",
      "labels. But Keras expects each item to be a tuple containing 2 elements (again, the fea‐\n",
      "tures and the labels). You could transform the dataset using the  map() method, like\n",
      "this:\n",
      "\n",
      "mnist_train = mnist_train.repeat(5).batch(32)\n",
      "mnist_train = mnist_train.map(lambda items: (items[\"image\"], items[\"label\"]))\n",
      "mnist_train = mnist_train.prefetch(1)\n",
      "\n",
      "Or  you  can  just  ask  the  load()  function  to  do  this  for  you  by  setting  as_super\n",
      "vised=True (obviously this works only for labeled datasets). You can also specify the\n",
      "batch size if you want. Then the dataset can be passed directly to your tf.keras model:\n",
      "\n",
      "dataset = tfds.load(name=\"mnist\", batch_size=32, as_supervised=True)\n",
      "mnist_train = dataset[\"train\"].repeat().prefetch(1)\n",
      "model = keras.models.Sequential([...])\n",
      "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\")\n",
      "model.fit(mnist_train, steps_per_epoch=60000 // 32, epochs=5)\n",
      "\n",
      "This  was  quite  a  technical  chapter,  and  you  may  feel  that  it  is  a  bit  far  from  the\n",
      "abstract beauty of neural networks, but the fact is deep learning often involves large\n",
      "amounts  of  data,  and  knowing  how  to  load,  parse  and  preprocess  it  efficiently  is  a\n",
      "crucial  skill  to  have.  In  the  next  chapter,  we  will  look  at  Convolutional  Neural  Net‐\n",
      "works, which are among the most successful neural net architectures for image pro‐\n",
      "cessing, and many other applications.\n",
      "\n",
      "430 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "\n",
      "\fCHAPTER 14\n",
      "Deep Computer Vision Using Convolutional\n",
      "Neural Networks\n",
      "\n",
      "With  Early  Release  ebooks,  you  get  books  in  their  earliest  form—\n",
      "the author’s raw and unedited content as he or she writes—so you\n",
      "can  take  advantage  of  these  technologies  long  before  the  official\n",
      "release of these titles. The following will be Chapter 14 in the final\n",
      "release of the book.\n",
      "\n",
      "Although IBM’s Deep Blue supercomputer beat the chess world champion Garry Kas‐\n",
      "parov back in 1996, it wasn’t until fairly recently that computers were able to reliably\n",
      "perform seemingly trivial tasks such as detecting a puppy in a picture or recognizing\n",
      "spoken words. Why are these tasks so effortless to us humans? The answer lies in the\n",
      "fact that perception largely takes place outside the realm of our consciousness, within\n",
      "specialized  visual,  auditory,  and  other  sensory  modules  in  our  brains.  By  the  time\n",
      "sensory information reaches our consciousness, it is already adorned with high-level\n",
      "features; for example, when you look at a picture of a cute puppy, you cannot choose\n",
      "not to see the puppy, or not to notice its cuteness. Nor can you explain how you rec‐\n",
      "ognize  a  cute  puppy;  it’s  just  obvious  to  you.  Thus,  we  cannot  trust  our  subjective\n",
      "experience: perception is not trivial at all, and to understand it we must look at how\n",
      "the sensory modules work.\n",
      "\n",
      "Convolutional neural networks (CNNs) emerged from the study of the brain’s visual\n",
      "cortex, and they have been used in image recognition since the 1980s. In the last few\n",
      "years, thanks to the increase in computational power, the amount of available training\n",
      "data, and the tricks presented in Chapter 11 for training deep nets, CNNs have man‐\n",
      "aged to achieve superhuman performance on some complex visual tasks. They power\n",
      "image  search  services,  self-driving  cars,  automatic  video  classification  systems,  and\n",
      "more. Moreover, CNNs are not restricted to visual perception: they are also successful\n",
      "\n",
      "431\n",
      "\n",
      "\fat  many  other  tasks,  such  as  voice  recognition  or  natural  language  processing  (NLP);\n",
      "however, we will focus on visual applications for now.\n",
      "\n",
      "In  this  chapter  we  will  present  where  CNNs  came  from,  what  their  building  blocks\n",
      "look like, and how to implement them using TensorFlow and Keras. Then we will dis‐\n",
      "cuss  some  of  the  best  CNN  architectures,  and  discuss  other  visual  tasks,  including\n",
      "object detection (classifying multiple objects in an image and placing bounding boxes\n",
      "around them) and semantic segmentation (classifying each pixel according to the class\n",
      "of the object it belongs to).\n",
      "\n",
      "The Architecture of the Visual Cortex\n",
      "David  H.  Hubel  and  Torsten  Wiesel  performed  a  series  of  experiments  on  cats  in\n",
      "19581  and  19592  (and  a  few  years  later  on  monkeys3),  giving  crucial  insights  on  the\n",
      "structure of the visual cortex (the authors received the Nobel Prize in Physiology or\n",
      "Medicine  in  1981  for  their  work).  In  particular,  they  showed  that  many  neurons  in\n",
      "the visual cortex have a small local receptive field, meaning they react only to visual\n",
      "stimuli  located  in  a  limited  region  of  the  visual  field  (see  Figure  14-1,  in  which  the\n",
      "local receptive fields of five neurons are represented by dashed circles). The receptive\n",
      "fields of different neurons may overlap, and together they tile the whole visual field.\n",
      "Moreover, the authors showed that some neurons react only to images of horizontal\n",
      "lines,  while  others  react  only  to  lines  with  different  orientations  (two  neurons  may\n",
      "have  the  same  receptive  field  but  react  to  different  line  orientations).  They  also\n",
      "noticed that some neurons have larger receptive fields, and they react to more com‐\n",
      "plex  patterns  that  are  combinations  of  the  lower-level  patterns.  These  observations\n",
      "led to the idea that the higher-level neurons are based on the outputs of neighboring\n",
      "lower-level neurons (in Figure 14-1, notice that each neuron is connected only to a\n",
      "few neurons from the previous layer). This powerful architecture is able to detect all\n",
      "sorts of complex patterns in any area of the visual field.\n",
      "\n",
      "1 “Single Unit Activity in Striate Cortex of Unrestrained Cats,” D. Hubel and T. Wiesel (1958).\n",
      "\n",
      "2 “Receptive Fields of Single Neurones in the Cat’s Striate Cortex,” D. Hubel and T. Wiesel (1959).\n",
      "\n",
      "3 “Receptive Fields and Functional Architecture of Monkey Striate Cortex,” D. Hubel and T. Wiesel (1968).\n",
      "\n",
      "432 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fFigure 14-1. Local receptive fields in the visual cortex\n",
      "\n",
      "These  studies  of  the  visual  cortex  inspired  the  neocognitron,  introduced  in  1980,4\n",
      "which  gradually  evolved  into  what  we  now  call  convolutional  neural  networks.  An\n",
      "important milestone was a 1998 paper5 by Yann LeCun, Léon Bottou, Yoshua Bengio,\n",
      "and Patrick Haffner, which introduced the famous LeNet-5 architecture, widely used\n",
      "to recognize handwritten check numbers. This architecture has some building blocks\n",
      "that  you  already  know,  such  as  fully  connected  layers  and  sigmoid  activation  func‐\n",
      "tions, but it also introduces two new building blocks: convolutional layers and pooling\n",
      "layers. Let’s look at them now.\n",
      "\n",
      "Why not simply use a regular deep neural network with fully con‐\n",
      "nected layers for image recognition tasks? Unfortunately, although\n",
      "this works fine for small images (e.g., MNIST), it breaks down for\n",
      "larger  images  because  of  the  huge  number  of  parameters  it\n",
      "requires. For example, a 100 × 100 image has 10,000 pixels, and if\n",
      "the  first  layer  has  just  1,000  neurons  (which  already  severely\n",
      "restricts the amount of information transmitted to the next layer),\n",
      "this means a total of 10 million connections. And that’s just the first\n",
      "layer. CNNs solve this problem using partially connected layers and\n",
      "weight sharing.\n",
      "\n",
      "4 “Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected\n",
      "\n",
      "by Shift in Position,” K. Fukushima (1980).\n",
      "\n",
      "5 “Gradient-Based Learning Applied to Document Recognition,” Y. LeCun et al. (1998).\n",
      "\n",
      "The Architecture of the Visual Cortex \n",
      "\n",
      "| \n",
      "\n",
      "433\n",
      "\n",
      "\fConvolutional Layer\n",
      "The most important building block of a CNN is the convolutional layer:6 neurons in\n",
      "the first convolutional layer are not connected to every single pixel in the input image\n",
      "(like they were in previous chapters), but only to pixels in their receptive fields (see\n",
      "Figure  14-2).  In  turn,  each  neuron  in  the  second  convolutional  layer  is  connected\n",
      "only  to  neurons  located  within  a  small  rectangle  in  the  first  layer.  This  architecture\n",
      "allows the network to concentrate on small low-level features in the first hidden layer,\n",
      "then assemble them into larger higher-level features in the next hidden layer, and so\n",
      "on. This hierarchical structure is common in real-world images, which is one of the\n",
      "reasons why CNNs work so well for image recognition.\n",
      "\n",
      "Figure 14-2. CNN layers with rectangular local receptive fields\n",
      "\n",
      "Until  now,  all  multilayer  neural  networks  we  looked  at  had  layers\n",
      "composed  of  a  long  line  of  neurons,  and  we  had  to  flatten  input\n",
      "images to 1D before feeding them to the neural network. Now each\n",
      "layer is represented in 2D, which makes it easier to match neurons\n",
      "with their corresponding inputs.\n",
      "\n",
      "A neuron located in row i, column j of a given layer is connected to the outputs of the\n",
      "neurons in the previous layer located in rows i to i + fh – 1, columns j to j + fw – 1,\n",
      "where  fh  and  fw  are  the  height  and  width  of  the  receptive  field  (see  Figure  14-3).  In\n",
      "order for a layer to have the same height and width as the previous layer, it is com‐\n",
      "\n",
      "6 A convolution is a mathematical operation that slides one function over another and measures the integral of\n",
      "their pointwise multiplication. It has deep connections with the Fourier transform and the Laplace transform,\n",
      "and is heavily used in signal processing. Convolutional layers actually use cross-correlations, which are very\n",
      "similar to convolutions (see https://homl.info/76 for more details).\n",
      "\n",
      "434 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fmon to add zeros around the inputs, as shown in the diagram. This is called zero pad‐\n",
      "ding.\n",
      "\n",
      "Figure 14-3. Connections between layers and zero padding\n",
      "\n",
      "It is also possible to connect a large input layer to a much smaller layer by spacing out\n",
      "the receptive fields, as shown in Figure 14-4. The shift from one receptive field to the\n",
      "next is called the stride. In the diagram, a 5 × 7 input layer (plus zero padding) is con‐\n",
      "nected to a 3 × 4 layer, using 3 × 3 receptive fields and a stride of 2 (in this example\n",
      "the stride is the same in both directions, but it does not have to be so). A neuron loca‐\n",
      "ted in row i, column j in the upper layer is connected to the outputs of the neurons in\n",
      "the previous layer located in rows i × sh to i × sh + fh – 1, columns j × sw to j × sw + fw –\n",
      "1, where sh and sw are the vertical and horizontal strides.\n",
      "\n",
      "Convolutional Layer \n",
      "\n",
      "| \n",
      "\n",
      "435\n",
      "\n",
      "\fFigure 14-4. Reducing dimensionality using a stride of 2\n",
      "\n",
      "Filters\n",
      "A neuron’s weights can be represented as a small image the size of the receptive field.\n",
      "For example, Figure 14-5 shows two possible sets of weights, called filters (or convolu‐\n",
      "tion kernels). The first one is represented as a black square with a vertical white line in\n",
      "the middle (it is a 7 × 7 matrix full of 0s except for the central column, which is full of\n",
      "1s); neurons using these weights will ignore everything in their receptive field except\n",
      "for  the  central  vertical  line  (since  all  inputs  will  get  multiplied  by  0,  except  for  the\n",
      "ones  located  in  the  central  vertical  line).  The  second  filter  is  a  black  square  with  a\n",
      "horizontal  white  line  in  the  middle.  Once  again,  neurons  using  these  weights  will\n",
      "ignore everything in their receptive field except for the central horizontal line.\n",
      "\n",
      "Now if all neurons in a layer use the same vertical line filter (and the same bias term),\n",
      "and you feed the network the input image shown in Figure 14-5 (bottom image), the\n",
      "layer will output the top-left image. Notice that the vertical white lines get enhanced\n",
      "while the rest gets blurred. Similarly, the upper-right image is what you get if all neu‐\n",
      "rons  use  the  same  horizontal  line  filter;  notice  that  the  horizontal  white  lines  get\n",
      "enhanced while the rest is blurred out. Thus, a layer full of neurons using the same\n",
      "filter outputs a feature map, which highlights the areas in an image that activate the\n",
      "filter the most. Of course you do not have to define the filters manually: instead, dur‐\n",
      "ing training the convolutional layer will automatically learn the most useful filters for\n",
      "its task, and the layers above will learn to combine them into more complex patterns.\n",
      "\n",
      "436 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fFigure 14-5. Applying two different filters to get two feature maps\n",
      "\n",
      "Stacking Multiple Feature Maps\n",
      "Up to now, for simplicity, I have represented the output of each convolutional layer as\n",
      "a  thin  2D  layer,  but  in  reality  a  convolutional  layer  has  multiple  filters  (you  decide\n",
      "how many), and it outputs one feature map per filter, so it is more accurately repre‐\n",
      "sented in 3D (see Figure 14-6). To do so, it has one neuron per pixel in each feature\n",
      "map, and all neurons within a given feature map share the same parameters (i.e., the\n",
      "same weights and bias term). However, neurons in different feature maps use differ‐\n",
      "ent  parameters.  A  neuron’s  receptive  field  is  the  same  as  described  earlier,  but  it\n",
      "extends  across  all  the  previous  layers’  feature  maps.  In  short,  a  convolutional  layer\n",
      "simultaneously  applies  multiple  trainable  filters  to  its  inputs,  making  it  capable  of\n",
      "detecting multiple features anywhere in its inputs.\n",
      "\n",
      "The fact that all neurons in a feature map share the same parame‐\n",
      "ters dramatically reduces the number of parameters in the model.\n",
      "Moreover, once the CNN has learned to recognize a pattern in one\n",
      "location, it can recognize it in any other location. In contrast, once\n",
      "a regular DNN has learned to recognize a pattern in one location, it\n",
      "can recognize it only in that particular location.\n",
      "\n",
      "Moreover, input images are also composed of multiple sublayers: one per color chan‐\n",
      "nel. There are typically three: red, green, and blue (RGB). Grayscale images have just\n",
      "\n",
      "Convolutional Layer \n",
      "\n",
      "| \n",
      "\n",
      "437\n",
      "\n",
      "\fone  channel,  but  some  images  may  have  much  more—for  example,  satellite  images\n",
      "that capture extra light frequencies (such as infrared).\n",
      "\n",
      "Figure 14-6. Convolution layers with multiple feature maps, and images with three color\n",
      "channels\n",
      "\n",
      "Specifically, a neuron located in row i, column j of the feature map k in a given convo‐\n",
      "lutional layer l is connected to the outputs of the neurons in the previous layer l – 1,\n",
      "located in rows i × sh to i × sh + fh – 1 and columns j × sw to j × sw + fw – 1, across all\n",
      "feature maps (in layer l – 1). Note that all neurons located in the same row i and col‐\n",
      "umn  j  but  in  different  feature  maps  are  connected  to  the  outputs  of  the  exact  same\n",
      "neurons in the previous layer.\n",
      "\n",
      "Equation 14-1 summarizes the preceding explanations in one big mathematical equa‐\n",
      "tion: it shows how to compute the output of a given neuron in a convolutional layer.\n",
      "\n",
      "438 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fIt is a bit ugly due to all the different indices, but all it does is calculate the weighted\n",
      "sum of all the inputs, plus the bias term.\n",
      "\n",
      "Equation 14-1. Computing the output of a neuron in a convolutional layer\n",
      "\n",
      "f\n",
      "\n",
      "− 1\n",
      "h\n",
      "zi, j, k = bk + ∑\n",
      "u = 0\n",
      "\n",
      "f\n",
      "\n",
      "− 1\n",
      "w\n",
      "∑\n",
      "v = 0\n",
      "\n",
      "f\n",
      "\n",
      "n′ − 1\n",
      "∑\n",
      "k′ = 0\n",
      "\n",
      "xi′, j′, k′ . wu, v, k′, k with\n",
      "\n",
      "i′ = i × sh + u\n",
      "j′ = j × sw + v\n",
      "\n",
      "• zi, j, k is the output of the neuron located in row i, column j in feature map k of the\n",
      "\n",
      "convolutional layer (layer l).\n",
      "\n",
      "• As explained earlier, sh and sw are the vertical and horizontal strides, fh and fw are\n",
      "the height and width of the receptive field, and fn′ is the number of feature maps\n",
      "in the previous layer (layer l – 1).\n",
      "\n",
      "• xi′, j′, k′ is the output of the neuron located in layer l – 1, row i′, column j′, feature\n",
      "\n",
      "map k′ (or channel k′ if the previous layer is the input layer).\n",
      "\n",
      "• bk is the bias term for feature map k (in layer l). You can think of it as a knob that\n",
      "\n",
      "tweaks the overall brightness of the feature map k.\n",
      "\n",
      "• wu, v, k′ ,k is the connection weight between any neuron in feature map k of the layer\n",
      "l and its input located at row u, column v (relative to the neuron’s receptive field),\n",
      "and feature map k′.\n",
      "\n",
      "TensorFlow Implementation\n",
      "In  TensorFlow,  each  input  image  is  typically  represented  as  a  3D  tensor  of  shape\n",
      "[height,  width,  channels].  A  mini-batch  is  represented  as  a  4D  tensor  of  shape\n",
      "[mini-batch  size,  height,  width,  channels].  The  weights  of  a  convolutional\n",
      "layer are represented as a 4D tensor of shape [fh, fw, fn′, fn]. The bias terms of a convo‐\n",
      "lutional layer are simply represented as a 1D tensor of shape [fn].\n",
      "\n",
      "Let’s  look  at  a  simple  example.  The  following  code  loads  two  sample  images,  using\n",
      "Scikit-Learn’s  load_sample_images()  (which  loads  two  color  images,  one  of  a  Chi‐\n",
      "nese temple, and the other of a flower). The pixel intensities (for each color channel)\n",
      "is represented as a byte from 0 to 255, so we scale these features simply by dividing by\n",
      "255,  to  get  floats  ranging  from  0  to  1.  Then  we  create  two  7  ×  7  filters  (one  with  a\n",
      "vertical  white  line  in  the  middle,  and  the  other  with  a  horizontal  white  line  in  the\n",
      "middle),  and  we  apply  them  to  both  images  using  the  tf.nn.conv2d()  function,\n",
      "which is part of TensorFlow’s low-level Deep Learning API. In this example, we use\n",
      "zero padding (padding=\"SAME\") and a stride of 2. Finally, we plot one of the resulting\n",
      "feature maps (similar to the top-right image in Figure 14-5).\n",
      "\n",
      "Convolutional Layer \n",
      "\n",
      "| \n",
      "\n",
      "439\n",
      "\n",
      "\ffrom sklearn.datasets import load_sample_image\n",
      "\n",
      "# Load sample images\n",
      "china = load_sample_image(\"china.jpg\") / 255\n",
      "flower = load_sample_image(\"flower.jpg\") / 255\n",
      "images = np.array([china, flower])\n",
      "batch_size, height, width, channels = images.shape\n",
      "\n",
      "# Create 2 filters\n",
      "filters = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)\n",
      "filters[:, 3, :, 0] = 1  # vertical line\n",
      "filters[3, :, :, 1] = 1  # horizontal line\n",
      "\n",
      "outputs = tf.nn.conv2d(images, filters, strides=1, padding=\"SAME\")\n",
      "\n",
      "plt.imshow(outputs[0, :, :, 1], cmap=\"gray\") # plot 1st image's 2nd feature map\n",
      "plt.show()\n",
      "\n",
      "Most  of  this  code  is  self-explanatory,  but  the  tf.nn.conv2d()  line  deserves  a  bit  of\n",
      "explanation:\n",
      "\n",
      "• images is the input mini-batch (a 4D tensor, as explained earlier).\n",
      "• filters is the set of filters to apply (also a 4D tensor, as explained earlier).\n",
      "• strides is equal to 1, but it could also be a 1D array with 4 elements, where the\n",
      "two central elements are the vertical and horizontal strides (sh and sw). The first\n",
      "and  last  elements  must  currently  be  equal  to  1.  They  may  one  day  be  used  to\n",
      "specify a batch stride (to skip some instances) and a channel stride (to skip some\n",
      "of the previous layer’s feature maps or channels).\n",
      "\n",
      "• padding must be either \"VALID\" or \"SAME\":\n",
      "\n",
      "— If set to \"VALID\", the convolutional layer does not use zero padding, and may\n",
      "ignore  some  rows  and  columns  at  the  bottom  and  right  of  the  input  image,\n",
      "depending on the stride, as shown in Figure 14-7 (for simplicity, only the hor‐\n",
      "izontal dimension is shown here, but of course the same logic applies to the\n",
      "vertical dimension).\n",
      "\n",
      "— If set to \"SAME\", the convolutional layer uses zero padding if necessary. In this\n",
      "case, the number of output neurons is equal to the number of input neurons\n",
      "divided by the stride, rounded up (in this example, 13 / 5 = 2.6, rounded up to\n",
      "3). Then zeros are added as evenly as possible around the inputs.\n",
      "\n",
      "440 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fFigure 14-7. Padding options—input width: 13, filter width: 6, stride: 5\n",
      "\n",
      "In this example, we manually defined the filters, but in a real CNN you would nor‐\n",
      "mally  define  filters  as  trainable  variables,  so  the  neural  net  can  learn  which  filters\n",
      "work  best,  as  explained  earlier.  Instead  of  manually  creating  the  variables,  however,\n",
      "you can simply use the keras.layers.Conv2D layer:\n",
      "\n",
      "conv = keras.layers.Conv2D(filters=32, kernel_size=3, strides=1,\n",
      "                           padding=\"SAME\", activation=\"relu\")\n",
      "\n",
      "This code creates a Conv2D layer with 32 filters, each 3 × 3, using a stride of 1 (both\n",
      "horizontally and vertically), SAME padding, and applying the ReLU activation func‐\n",
      "tion to its outputs. As you can see, convolutional layers have quite a few hyperpara‐\n",
      "meters: you must choose the number of filters, their height and width, the strides, and\n",
      "the padding type. As always, you can use cross-validation to find the right hyperpara‐\n",
      "meter values, but this is very time-consuming. We will discuss common CNN archi‐\n",
      "tectures  later,  to  give  you  some  idea  of  what  hyperparameter  values  work  best  in \n",
      "practice.\n",
      "\n",
      "Memory Requirements\n",
      "Another problem with CNNs is that the convolutional layers require a huge amount\n",
      "of RAM. This is especially true during training, because the reverse pass of backpro‐\n",
      "pagation requires all the intermediate values computed during the forward pass.\n",
      "\n",
      "For example, consider a convolutional layer with 5 × 5 filters, outputting 200 feature\n",
      "maps of size 150 × 100, with stride 1 and SAME padding. If the input is a 150 × 100\n",
      "\n",
      "Convolutional Layer \n",
      "\n",
      "| \n",
      "\n",
      "441\n",
      "\n",
      "\fRGB image (three channels), then the number of parameters is (5 × 5 × 3 + 1) × 200\n",
      "= 15,200 (the +1 corresponds to the bias terms), which is fairly small compared to a\n",
      "fully connected layer.7 However, each of the 200 feature maps contains 150 × 100 neu‐\n",
      "rons, and each of these neurons needs to compute a weighted sum of its 5 × 5 × 3 =\n",
      "75 inputs: that’s a total of 225 million float multiplications. Not as bad as a fully con‐\n",
      "nected layer, but still quite computationally intensive. Moreover, if the feature maps\n",
      "are represented using 32-bit floats, then the convolutional layer’s output will occupy\n",
      "200  ×  150  ×  100  ×  32  =  96  million  bits  (12  MB)  of  RAM.8  And  that’s  just  for  one\n",
      "instance! If a training batch contains 100 instances, then this layer will use up 1.2 GB\n",
      "of RAM!\n",
      "\n",
      "During inference (i.e., when making a prediction for a new instance) the RAM occu‐\n",
      "pied by one layer can be released as soon as the next layer has been computed, so you\n",
      "only need as much RAM as required by two consecutive layers. But during training\n",
      "everything computed during the forward pass needs to be preserved for the reverse\n",
      "pass, so the amount of RAM needed is (at least) the total amount of RAM required by\n",
      "all layers.\n",
      "\n",
      "If training crashes because of an out-of-memory error, you can try\n",
      "reducing  the  mini-batch  size.  Alternatively,  you  can  try  reducing\n",
      "dimensionality using a stride, or removing a few layers. Or you can\n",
      "try using 16-bit floats instead of 32-bit floats. Or you could distrib‐\n",
      "ute the CNN across multiple devices.\n",
      "\n",
      "Now let’s look at the second common building block of CNNs: the pooling layer.\n",
      "\n",
      "Pooling Layer\n",
      "Once  you  understand  how  convolutional  layers  work,  the  pooling  layers  are  quite\n",
      "easy  to  grasp.  Their  goal  is  to  subsample  (i.e.,  shrink)  the  input  image  in  order  to\n",
      "reduce  the  computational  load,  the  memory  usage,  and  the  number  of  parameters\n",
      "(thereby limiting the risk of overfitting).\n",
      "\n",
      "Just  like  in  convolutional  layers,  each  neuron  in  a  pooling  layer  is  connected  to  the\n",
      "outputs of a limited number of neurons in the previous layer, located within a small\n",
      "rectangular receptive field. You must define its size, the stride, and the padding type,\n",
      "just like before. However, a pooling neuron has no weights; all it does is aggregate the\n",
      "inputs using an aggregation function such as the max or mean. Figure 14-8 shows a\n",
      "max pooling layer, which is the most common type of pooling layer. In this example,\n",
      "\n",
      "7 A fully connected layer with 150 × 100 neurons, each connected to all 150 × 100 × 3 inputs, would have 1502\n",
      "\n",
      "× 1002 × 3 = 675 million parameters!\n",
      "\n",
      "8 In the international system of units (SI), 1 MB = 1,000 kB = 1,000 × 1,000 bytes = 1,000 × 1,000 × 8 bits.\n",
      "\n",
      "442 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fwe use a 2 × 2 _pooling kernel_9, with a stride of 2, and no padding. Only the max\n",
      "input value in each receptive field makes it to the next layer, while the other inputs\n",
      "are  dropped.  For  example,  in  the  lower  left  receptive  field  in  Figure  14-8,  the  input\n",
      "values are 1, 5, 3, 2, so only the max value, 5, is propagated to the next layer. Because\n",
      "of the stride of 2, the output image has half the height and half the width of the input\n",
      "image (rounded down since we use no padding).\n",
      "\n",
      "Figure 14-8. Max pooling layer (2 × 2 pooling kernel, stride 2, no padding)\n",
      "\n",
      "A pooling layer typically works on every input channel independ‐\n",
      "ently, so the output depth is the same as the input depth.\n",
      "\n",
      "Other than reducing computations, memory usage and the number of parameters, a\n",
      "max  pooling  layer  also  introduces  some  level  of  invariance  to  small  translations,  as\n",
      "shown in Figure 14-9. Here we assume that the bright pixels have a lower value than\n",
      "dark pixels, and we consider 3 images (A, B, C) going through a max pooling layer\n",
      "with a 2 × 2 kernel and stride 2. Images B and C are the same as image A, but shifted\n",
      "by  one  and  two  pixels  to  the  right.  As  you  can  see,  the  outputs  of  the  max  pooling\n",
      "layer  for  images  A  and  B  are  identical.  This  is  what  translation  invariance  means.\n",
      "However,  for  image  C,  the  output  is  different:  it  is  shifted  by  one  pixel  to  the  right\n",
      "(but there is still 75% invariance). By inserting a max pooling layer every few layers in\n",
      "a  CNN,  it  is  possible  to  get  some  level  of  translation  invariance  at  a  larger  scale.\n",
      "Moreover,  max  pooling  also  offers  a  small  amount  of  rotational  invariance  and  a\n",
      "slight  scale  invariance.  Such  invariance  (even  if  it  is  limited)  can  be  useful  in  cases\n",
      "where  the  prediction  should  not  depend  on  these  details,  such  as  in  classification\n",
      "tasks.\n",
      "\n",
      "9 Other kernels we discussed so far had weights, but pooling kernels do not: they are just stateless sliding win‐\n",
      "\n",
      "dows.\n",
      "\n",
      "Pooling Layer \n",
      "\n",
      "| \n",
      "\n",
      "443\n",
      "\n",
      "\fFigure 14-9. Invariance to small translations\n",
      "\n",
      "But  max  pooling  has  some  downsides:  firstly,  it  is  obviously  very  destructive:  even\n",
      "with a tiny 2 × 2 kernel and a stride of 2, the output will be two times smaller in both\n",
      "directions (so its area will be four times smaller), simply dropping 75% of the input\n",
      "values. And in some applications, invariance is not desirable, for example for seman‐\n",
      "tic segmentation: this is the task of classifying each pixel in an image depending on the\n",
      "object that pixel belongs to: obviously, if the input image is translated by 1 pixel to the\n",
      "right, the output should also be translated by 1 pixel to the right. The goal in this case\n",
      "is  equivariance,  not  invariance:  a  small  change  to  the  inputs  should  lead  to  a  corre‐\n",
      "sponding small change in the output.\n",
      "\n",
      "TensorFlow Implementation\n",
      "Implementing  a  max  pooling  layer  in  TensorFlow  is  quite  easy.  The  following  code\n",
      "creates a max pooling layer using a 2 × 2 kernel. The strides default to the kernel size,\n",
      "so this layer will use a stride of 2 (both horizontally and vertically). By default, it uses\n",
      "VALID padding (i.e., no padding at all):\n",
      "\n",
      "max_pool = keras.layers.MaxPool2D(pool_size=2)\n",
      "\n",
      "To create an average pooling layer, just use AvgPool2D instead of MaxPool2D. As you\n",
      "might expect, it works exactly like a max pooling layer, except it computes the mean\n",
      "rather  than  the  max.  Average  pooling  layers  used  to  be  very  popular,  but  people\n",
      "\n",
      "444 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fmostly use max pooling layers now, as they generally perform better. This may seem\n",
      "surprising, since computing the mean generally loses less information than comput‐\n",
      "ing the max. But on the other hand, max pooling preserves only the strongest feature,\n",
      "getting rid of all the meaningless ones, so the next layers get a cleaner signal to work\n",
      "with.  Moreover,  max  pooling  offers  stronger  translation  invariance  than  average\n",
      "pooling.\n",
      "\n",
      "Note that max pooling and average pooling can be performed along the depth dimen‐\n",
      "sion  rather  than  the  spatial  dimensions,  although  this  is  not  as  common.  This  can\n",
      "allow the CNN to learn to be invariant to various features. For example, it could learn\n",
      "multiple filters, each detecting a different rotation of the same pattern, such as hand-\n",
      "written digits (see Figure 14-10), and the depth-wise max pooling layer would ensure\n",
      "that the output is the same regardless of the rotation. The CNN could similarly learn\n",
      "to be invariant to anything else: thickness, brightness, skew, color, and so on.\n",
      "\n",
      "Figure 14-10. Depth-wise max pooling can help the CNN learn any invariance\n",
      "\n",
      "Pooling Layer \n",
      "\n",
      "| \n",
      "\n",
      "445\n",
      "\n",
      "\fKeras  does  not  include  a  depth-wise  max  pooling  layer,  but  TensorFlow’s  low-level\n",
      "Deep  Learning  API  does:  just  use  the  tf.nn.max_pool()  function,  and  specify  the\n",
      "kernel size and strides as 4-tuples. The first three values of each should be 1: this indi‐\n",
      "cates  that  the  kernel  size  and  stride  along  the  batch,  height  and  width  dimensions\n",
      "shoud be 1. The last value should be whatever kernel size and stride you want along\n",
      "the  depth  dimension,  for  example  3  (this  must  be  a  divisor  of  the  input  depth;  for\n",
      "example, it will not work if the previous layer outputs 20 feature maps, since 20 is not\n",
      "a multiple of 3):\n",
      "\n",
      "output = tf.nn.max_pool(images,\n",
      "                        ksize=(1, 1, 1, 3),\n",
      "                        strides=(1, 1, 1, 3),\n",
      "                        padding=\"VALID\")\n",
      "\n",
      "If you want to include this as a layer in your Keras models, you can simply wrap it in\n",
      "a Lambda layer (or create a custom Keras layer):\n",
      "\n",
      "depth_pool = keras.layers.Lambda(\n",
      "    lambda X: tf.nn.max_pool(X, ksize=(1, 1, 1, 3), strides=(1, 1, 1, 3),\n",
      "                             padding=\"VALID\"))\n",
      "\n",
      "One last type of pooling layer that you will often see in modern architectures is the\n",
      "global average pooling layer. It works very differently: all it does is compute the mean\n",
      "of  each  entire  feature  map  (it’s  like  an  average  pooling  layer  using  a  pooling  kernel\n",
      "with the same spatial dimensions as the inputs). This means that it just outputs a sin‐\n",
      "gle  number  per  feature  map  and  per  instance.  Although  this  is  of  course  extremely\n",
      "destructive (most of the information in the feature map is lost), it can be useful as the\n",
      "output layer, as we will see later in this chapter. To create such a layer, simply use the\n",
      "keras.layers.GlobalAvgPool2D class:\n",
      "\n",
      "global_avg_pool = keras.layers.GlobalAvgPool2D()\n",
      "\n",
      "It is actually equivalent to this simple Lamba layer, which computes the mean over the\n",
      "spatial dimensions (height and width):\n",
      "\n",
      "global_avg_pool = keras.layers.Lambda(lambda X: tf.reduce_mean(X, axis=[1, 2]))\n",
      "\n",
      "Now you know all the building blocks to create a convolutional neural network. Let’s\n",
      "see how to assemble them.\n",
      "\n",
      "CNN Architectures\n",
      "Typical  CNN  architectures  stack  a  few  convolutional  layers  (each  one  generally  fol‐\n",
      "lowed by a ReLU layer), then a pooling layer, then another few convolutional layers\n",
      "(+ReLU), then another pooling layer, and so on. The image gets smaller and smaller\n",
      "as it progresses through the network, but it also typically gets deeper and deeper (i.e.,\n",
      "with more feature maps) thanks to the convolutional layers (see Figure 14-11). At the\n",
      "top of the stack, a regular feedforward neural network is added, composed of a few\n",
      "\n",
      "446 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\ffully  connected  layers  (+ReLUs),  and  the  final  layer  outputs  the  prediction  (e.g.,  a\n",
      "softmax layer that outputs estimated class probabilities).\n",
      "\n",
      "Figure 14-11. Typical CNN architecture\n",
      "\n",
      "A common mistake is to use convolution kernels that are too large.\n",
      "For  example,  instead  of  using  a  convolutional  layer  with  a  5  ×  5\n",
      "kernel, it is generally preferable to stack two layers with 3 × 3 ker‐\n",
      "nels: it will use less parameters and require less computations, and\n",
      "it will usually perform better. One exception to this recommenda‐\n",
      "tion is for the first convolutional layer: it can typically have a large\n",
      "kernel (e.g., 5 × 5), usually with stride of 2 or more: this will reduce\n",
      "the spatial dimension of the image without losing too much infor‐\n",
      "mation, and since the input image only has 3 channels in general, it\n",
      "will not be too costly.\n",
      "\n",
      "Here is how you can implement a simple CNN to tackle the fashion MNIST dataset\n",
      "(introduced in Chapter 10):\n",
      "\n",
      "from functools import partial\n",
      "\n",
      "DefaultConv2D = partial(keras.layers.Conv2D,\n",
      "                        kernel_size=3, activation='relu', padding=\"SAME\")\n",
      "\n",
      "model = keras.models.Sequential([\n",
      "    DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]),\n",
      "    keras.layers.MaxPooling2D(pool_size=2),\n",
      "    DefaultConv2D(filters=128),\n",
      "    DefaultConv2D(filters=128),\n",
      "    keras.layers.MaxPooling2D(pool_size=2),\n",
      "    DefaultConv2D(filters=256),\n",
      "    DefaultConv2D(filters=256),\n",
      "    keras.layers.MaxPooling2D(pool_size=2),\n",
      "    keras.layers.Flatten(),\n",
      "    keras.layers.Dense(units=128, activation='relu'),\n",
      "    keras.layers.Dropout(0.5),\n",
      "    keras.layers.Dense(units=64, activation='relu'),\n",
      "    keras.layers.Dropout(0.5),\n",
      "    keras.layers.Dense(units=10, activation='softmax'),\n",
      "])\n",
      "\n",
      "CNN Architectures \n",
      "\n",
      "| \n",
      "\n",
      "447\n",
      "\n",
      "\f• In this code, we start by using the partial() function to define a thin wrapper\n",
      "around the Conv2D class, called DefaultConv2D: it simply avoids having to repeat\n",
      "the same hyperparameter values over and over again.\n",
      "\n",
      "• The first layer uses a large kernel size, but no stride because the input images are\n",
      "not very large. It also sets input_shape=[28, 28, 1], which means the images\n",
      "are 28 × 28 pixels, with a single color channel (i.e., grayscale).\n",
      "\n",
      "• Next, we have a max pooling layer, which divides each spatial dimension by a fac‐\n",
      "\n",
      "tor of two (since pool_size=2).\n",
      "\n",
      "• Then we repeat the same structure twice: two convolutional layers followed by a\n",
      "max pooling layer. For larger images, we could repeat this structure several times\n",
      "(the number of repetitions is a hyperparameter you can tune).\n",
      "\n",
      "• Note that the number of filters grows as we climb up the CNN towards the out‐\n",
      "put layer (it is initially 64, then 128, then 256): it makes sense for it to grow, since\n",
      "the number of low level features is often fairly low (e.g., small circles, horizontal\n",
      "lines, etc.), but there are many different ways to combine them into higher level\n",
      "features. It is a common practice to double the number of filters after each pool‐\n",
      "ing layer: since a pooling layer divides each spatial dimension by a factor of 2, we\n",
      "can afford doubling the number of feature maps in the next layer, without fear of\n",
      "exploding the number of parameters, memory usage, or computational load.\n",
      "\n",
      "• Next  is  the  fully  connected  network,  composed  of  2  hidden  dense  layers  and  a\n",
      "dense  output  layer.  Note  that  we  must  flatten  its  inputs,  since  a  dense  network\n",
      "expects a 1D array of features for each instance. We also add two dropout layers,\n",
      "with a dropout rate of 50% each, to reduce overfitting.\n",
      "\n",
      "This CNN reaches over 92% accuracy on the test set. It’s not the state of the art, but it\n",
      "is pretty good, and clearly much better than what we achieved with dense networks in\n",
      "Chapter 10.\n",
      "\n",
      "Over the years, variants of this fundamental architecture have been developed, lead‐\n",
      "ing to amazing advances in the field. A good measure of this progress is the error rate\n",
      "in  competitions  such  as  the  ILSVRC  ImageNet  challenge.  In  this  competition  the\n",
      "top-5 error rate for image classification fell from over 26% to less than 2.3% in just six\n",
      "years. The top-five error rate is the number of test images for which the system’s top 5\n",
      "predictions did not include the correct answer. The images are large (256 pixels high)\n",
      "and  there  are  1,000  classes,  some  of  which  are  really  subtle  (try  distinguishing  120\n",
      "dog breeds). Looking at the evolution of the winning entries is a good way to under‐\n",
      "stand how CNNs work.\n",
      "\n",
      "We will first look at the classical LeNet-5 architecture (1998), then three of the win‐\n",
      "ners  of  the  ILSVRC  challenge:  AlexNet  (2012),  GoogLeNet  (2014),  and  ResNet\n",
      "(2015).\n",
      "\n",
      "448 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fLeNet-5\n",
      "The LeNet-5 architecture10 is perhaps the most widely known CNN architecture. As\n",
      "mentioned earlier, it was created by Yann LeCun in 1998 and widely used for hand‐\n",
      "written digit recognition (MNIST). It is composed of the layers shown in Table 14-1.\n",
      "\n",
      "Table 14-1. LeNet-5 architecture\n",
      "\n",
      "Kernel size\n",
      "–\n",
      "\n",
      "Stride Activation\n",
      "RBF\n",
      "–\n",
      "\n",
      "Layer\n",
      "Out\n",
      "\n",
      "Type\n",
      "Fully Connected –\n",
      "\n",
      "Maps\n",
      "\n",
      "F6\n",
      "\n",
      "C5\n",
      "\n",
      "S4\n",
      "\n",
      "C3\n",
      "\n",
      "S2\n",
      "\n",
      "C1\n",
      "\n",
      "In\n",
      "\n",
      "Fully Connected –\n",
      "\n",
      "Convolution\n",
      "\n",
      "Avg Pooling\n",
      "\n",
      "Convolution\n",
      "\n",
      "Avg Pooling\n",
      "\n",
      "Convolution\n",
      "\n",
      "Input\n",
      "\n",
      "120\n",
      "\n",
      "16\n",
      "\n",
      "16\n",
      "\n",
      "6\n",
      "\n",
      "6\n",
      "\n",
      "1\n",
      "\n",
      "Size\n",
      "10\n",
      "\n",
      "84\n",
      "\n",
      "1 × 1\n",
      "\n",
      "5 × 5\n",
      "\n",
      "–\n",
      "\n",
      "5 × 5\n",
      "\n",
      "2 × 2\n",
      "\n",
      "10 × 10\n",
      "\n",
      "5 × 5\n",
      "\n",
      "14 × 14\n",
      "\n",
      "2 × 2\n",
      "\n",
      "28 × 28\n",
      "\n",
      "5 × 5\n",
      "\n",
      "32 × 32 –\n",
      "\n",
      "–\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "–\n",
      "\n",
      "tanh\n",
      "\n",
      "tanh\n",
      "\n",
      "tanh\n",
      "\n",
      "tanh\n",
      "\n",
      "tanh\n",
      "\n",
      "tanh\n",
      "\n",
      "–\n",
      "\n",
      "There are a few extra details to be noted:\n",
      "\n",
      "• MNIST images are 28 × 28 pixels, but they are zero-padded to 32 × 32 pixels and\n",
      "normalized before being fed to the network. The rest of the network does not use\n",
      "any  padding,  which  is  why  the  size  keeps  shrinking  as  the  image  progresses\n",
      "through the network.\n",
      "\n",
      "• The  average  pooling  layers  are  slightly  more  complex  than  usual:  each  neuron\n",
      "computes the mean of its inputs, then multiplies the result by a learnable coeffi‐\n",
      "cient  (one  per  map)  and  adds  a  learnable  bias  term  (again,  one  per  map),  then\n",
      "finally applies the activation function.\n",
      "\n",
      "• Most  neurons  in  C3  maps  are  connected  to  neurons  in  only  three  or  four  S2\n",
      "maps (instead of all six S2 maps). See table 1 (page 8) in the original paper10 for\n",
      "details.\n",
      "\n",
      "• The output layer is a bit special: instead of computing the matrix multiplication\n",
      "of the inputs and the weight vector, each neuron outputs the square of the Eucli‐\n",
      "dian distance between its input vector and its weight vector. Each output meas‐\n",
      "ures how much the image belongs to a particular digit class. The cross entropy \n",
      "cost function is now preferred, as it penalizes bad predictions much more, pro‐\n",
      "ducing larger gradients and converging faster.\n",
      "\n",
      "10 “Gradient-Based Learning Applied to Document Recognition”, Y. LeCun, L. Bottou, Y. Bengio and P. Haffner\n",
      "\n",
      "(1998).\n",
      "\n",
      "CNN Architectures \n",
      "\n",
      "| \n",
      "\n",
      "449\n",
      "\n",
      "\fYann LeCun’s website (“LENET” section) features great demos of LeNet-5 classifying \n",
      "digits.\n",
      "\n",
      "AlexNet\n",
      "The  AlexNet  CNN  architecture11  won  the  2012  ImageNet  ILSVRC  challenge  by  a\n",
      "large  margin:  it  achieved  17%  top-5  error  rate  while  the  second  best  achieved  only\n",
      "26%!  It  was  developed  by  Alex  Krizhevsky  (hence  the  name),  Ilya  Sutskever,  and\n",
      "Geoffrey Hinton. It is quite similar to LeNet-5, only much larger and deeper, and it\n",
      "was  the  first  to  stack  convolutional  layers  directly  on  top  of  each  other,  instead  of\n",
      "stacking a pooling layer on top of each convolutional layer. Table 14-2 presents this\n",
      "architecture.\n",
      "\n",
      "Table 14-2. AlexNet architecture\n",
      "\n",
      "Layer\n",
      "Out\n",
      "\n",
      "Type\n",
      "Fully Connected –\n",
      "\n",
      "Maps\n",
      "\n",
      "Fully Connected –\n",
      "\n",
      "Fully Connected –\n",
      "\n",
      "Convolution\n",
      "\n",
      "Convolution\n",
      "\n",
      "Convolution\n",
      "\n",
      "Max Pooling\n",
      "\n",
      "Convolution\n",
      "\n",
      "Max Pooling\n",
      "\n",
      "Convolution\n",
      "\n",
      "256\n",
      "\n",
      "384\n",
      "\n",
      "384\n",
      "\n",
      "256\n",
      "\n",
      "256\n",
      "\n",
      "96\n",
      "\n",
      "96\n",
      "\n",
      "F9\n",
      "\n",
      "F8\n",
      "\n",
      "C7\n",
      "\n",
      "C6\n",
      "\n",
      "C5\n",
      "\n",
      "S4\n",
      "\n",
      "C3\n",
      "\n",
      "S2\n",
      "\n",
      "C1\n",
      "\n",
      "In\n",
      "\n",
      "Size\n",
      "1,000\n",
      "\n",
      "4,096\n",
      "\n",
      "4,096\n",
      "\n",
      "13 × 13\n",
      "\n",
      "13 × 13\n",
      "\n",
      "13 × 13\n",
      "\n",
      "13 × 13\n",
      "\n",
      "27 × 27\n",
      "\n",
      "27 × 27\n",
      "\n",
      "55 × 55\n",
      "\n",
      "Kernel size\n",
      "–\n",
      "\n",
      "–\n",
      "\n",
      "–\n",
      "\n",
      "3 × 3\n",
      "\n",
      "3 × 3\n",
      "\n",
      "3 × 3\n",
      "\n",
      "3 × 3\n",
      "\n",
      "5 × 5\n",
      "\n",
      "3 × 3\n",
      "\n",
      "11 × 11\n",
      "\n",
      "Stride Padding Activation\n",
      "–\n",
      "\n",
      "Softmax\n",
      "\n",
      "–\n",
      "\n",
      "–\n",
      "\n",
      "–\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "–\n",
      "\n",
      "–\n",
      "\n",
      "–\n",
      "\n",
      "SAME\n",
      "\n",
      "SAME\n",
      "\n",
      "SAME\n",
      "\n",
      "VALID\n",
      "\n",
      "SAME\n",
      "\n",
      "VALID\n",
      "\n",
      "VALID\n",
      "\n",
      "–\n",
      "\n",
      "ReLU\n",
      "\n",
      "ReLU\n",
      "\n",
      "ReLU\n",
      "\n",
      "ReLU\n",
      "\n",
      "ReLU\n",
      "\n",
      "–\n",
      "\n",
      "ReLU\n",
      "\n",
      "–\n",
      "\n",
      "ReLU\n",
      "\n",
      "–\n",
      "\n",
      "Input\n",
      "\n",
      "3 (RGB)\n",
      "\n",
      "227 × 227 –\n",
      "\n",
      "To  reduce  overfitting,  the  authors  used  two  regularization  techniques:  first  they\n",
      "applied dropout (introduced in Chapter 11) with a 50% dropout rate during training\n",
      "to the outputs of layers F8 and F9. Second, they performed data augmentation by ran‐\n",
      "domly shifting the training images by various offsets, flipping them horizontally, and\n",
      "changing the lighting conditions.\n",
      "\n",
      "Data Augmentation\n",
      "Data  augmentation  artificially  increases  the  size  of  the  training  set  by  generating\n",
      "many realistic variants of each training instance. This reduces overfitting, making this\n",
      "a regularization technique. The generated instances should be as realistic as possible:\n",
      "\n",
      "11 “ImageNet Classification with Deep Convolutional Neural Networks,” A. Krizhevsky et al. (2012).\n",
      "\n",
      "450 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fideally, given an image from the augmented training set, a human should not be able\n",
      "to tell whether it was augmented or not. Moreover, simply adding white noise will not\n",
      "help; the modifications should be learnable (white noise is not).\n",
      "\n",
      "For example, you can slightly shift, rotate, and resize every picture in the training set\n",
      "by  various  amounts  and  add  the  resulting  pictures  to  the  training  set  (see\n",
      "Figure 14-12). This forces the model to be more tolerant to variations in the position,\n",
      "orientation, and size of the objects in the pictures. If you want the model to be more\n",
      "tolerant to different lighting conditions, you can similarly generate many images with\n",
      "various  contrasts.  In  general,  you  can  also  flip  the  pictures  horizontally  (except  for\n",
      "text,  and  other  non-symmetrical  objects).  By  combining  these  transformations  you\n",
      "can greatly increase the size of your training set.\n",
      "\n",
      "Figure 14-12. Generating new training instances from existing ones\n",
      "\n",
      "AlexNet also uses a competitive normalization step immediately after the ReLU step\n",
      "of layers C1 and C3, called local response normalization. The most strongly activated\n",
      "neurons  inhibit  other  neurons  located  at  the  same  position  in  neighboring  feature\n",
      "maps  (such  competitive  activation  has  been  observed  in  biological  neurons).  This\n",
      "encourages different feature maps to specialize, pushing them apart and forcing them\n",
      "\n",
      "CNN Architectures \n",
      "\n",
      "| \n",
      "\n",
      "451\n",
      "\n",
      "\fto  explore  a  wider  range  of  features,  ultimately  improving  generalization.  Equation\n",
      "14-2 shows how to apply LRN.\n",
      "\n",
      "Equation 14-2. Local response normalization\n",
      "\n",
      "j\n",
      "\n",
      "high\n",
      "bi = ai k + α ∑\n",
      "j = j\n",
      "\n",
      "low\n",
      "\n",
      "−β\n",
      "\n",
      "2\n",
      "\n",
      "a j\n",
      "\n",
      "with\n",
      "\n",
      "jhigh = min i +\n",
      "\n",
      "r\n",
      "2\n",
      "\n",
      ", f n − 1\n",
      "\n",
      "jlow = max 0, i −\n",
      "\n",
      "r\n",
      "2\n",
      "\n",
      "• bi is the normalized output of the neuron located in feature map i, at some row u\n",
      "and column v (note that in this equation we consider only neurons located at this\n",
      "row and column, so u and v are not shown).\n",
      "\n",
      "• ai is the activation of that neuron after the ReLU step, but before normalization.\n",
      "• k, α, β, and r are hyperparameters. k is called the bias, and r is called the depth\n",
      "\n",
      "radius.\n",
      "\n",
      "• fn is the number of feature maps.\n",
      "\n",
      "For example, if r = 2 and a neuron has a strong activation, it will inhibit the activation\n",
      "of the neurons located in the feature maps immediately above and below its own.\n",
      "\n",
      "In AlexNet, the hyperparameters are set as follows: r = 2, α = 0.00002, β = 0.75, and k\n",
      "=  1.  This  step  can  be  implemented  using  the  tf.nn.local_response_normaliza\n",
      "tion()  function  (which  you  can  wrap  in  a  Lambda  layer  if  you  want  to  use  it  in  a\n",
      "Keras model).\n",
      "\n",
      "A variant of AlexNet called ZF Net was developed by Matthew Zeiler and Rob Fergus\n",
      "and  won  the  2013  ILSVRC  challenge.  It  is  essentially  AlexNet  with  a  few  tweaked \n",
      "hyperparameters (number of feature maps, kernel size, stride, etc.).\n",
      "\n",
      "GoogLeNet\n",
      "The GoogLeNet architecture was developed by Christian Szegedy et al. from Google\n",
      "Research,12  and  it  won  the  ILSVRC  2014  challenge  by  pushing  the  top-5  error  rate\n",
      "below 7%. This great performance came in large part from the fact that the network\n",
      "was much deeper than previous CNNs (see Figure 14-14). This was made possible by\n",
      "sub-networks  called  inception  modules,13  which  allow  GoogLeNet  to  use  parameters\n",
      "\n",
      "12 “Going Deeper with Convolutions,” C. Szegedy et al. (2015).\n",
      "\n",
      "13 In the 2010 movie Inception, the characters keep going deeper and deeper into multiple layers of dreams,\n",
      "\n",
      "hence the name of these modules.\n",
      "\n",
      "452 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fmuch more efficiently than previous architectures: GoogLeNet actually has 10 times\n",
      "fewer parameters than AlexNet (roughly 6 million instead of 60 million).\n",
      "\n",
      "Figure 14-13 shows the architecture of an inception module. The notation “3 × 3 +\n",
      "1(S)” means that the layer uses a 3 × 3 kernel, stride 1, and SAME padding. The input\n",
      "signal is first copied and fed to four different layers. All convolutional layers use the\n",
      "ReLU activation function. Note that the second set of convolutional layers uses differ‐\n",
      "ent kernel sizes (1 × 1, 3 × 3, and 5 × 5), allowing them to capture patterns at different\n",
      "scales. Also note that every single layer uses a stride of 1 and SAME padding (even\n",
      "the max pooling layer), so their outputs all have the same height and width as their\n",
      "inputs. This makes it possible to concatenate all the outputs along the depth dimen‐\n",
      "sion in the final depth concat layer (i.e., stack the feature maps from all four top con‐\n",
      "volutional layers). This concatenation layer can be implemented in TensorFlow using\n",
      "the tf.concat() operation, with axis=3 (axis 3 is the depth).\n",
      "\n",
      "Figure 14-13. Inception module\n",
      "\n",
      "You  may  wonder  why  inception  modules  have  convolutional  layers  with  1  ×  1  ker‐\n",
      "nels. Surely these layers cannot capture any features since they look at only one pixel\n",
      "at a time? In fact, these layers serve three purposes:\n",
      "\n",
      "• First,  although  they  cannot  capture  spatial  patterns,  they  can  capture  patterns\n",
      "\n",
      "along the depth dimension.\n",
      "\n",
      "• Second,  they  are  configured  to  output  fewer  feature  maps  than  their  inputs,  so\n",
      "they serve as bottleneck layers, meaning they reduce dimensionality. This cuts the\n",
      "computational  cost  and  the  number  of  parameters,  speeding  up  training  and\n",
      "improving generalization.\n",
      "\n",
      "• Lastly, each pair of convolutional layers ([1 × 1, 3 × 3] and [1 × 1, 5 × 5]) acts like\n",
      "a  single,  powerful  convolutional  layer,  capable  of  capturing  more  complex  pat‐\n",
      "terns. Indeed, instead of sweeping a simple linear classifier across the image (as a\n",
      "\n",
      "CNN Architectures \n",
      "\n",
      "| \n",
      "\n",
      "453\n",
      "\n",
      "\fsingle convolutional layer does), this pair of convolutional layers sweeps a two-\n",
      "layer neural network across the image.\n",
      "\n",
      "In  short,  you  can  think  of  the  whole  inception  module  as  a  convolutional  layer  on\n",
      "steroids, able to output feature maps that capture complex patterns at various scales.\n",
      "\n",
      "The number of convolutional kernels for each convolutional layer\n",
      "is  a  hyperparameter.  Unfortunately,  this  means  that  you  have  six\n",
      "more hyperparameters to tweak for every inception layer you add.\n",
      "\n",
      "Now  let’s  look  at  the  architecture  of  the  GoogLeNet  CNN  (see  Figure  14-14).  The\n",
      "number of feature maps output by each convolutional layer and each pooling layer is\n",
      "shown before the kernel size. The architecture is so deep that it has to be represented\n",
      "in three columns, but GoogLeNet is actually one tall stack, including nine inception\n",
      "modules (the boxes with the spinning tops). The six numbers in the inception mod‐\n",
      "ules represent the number of feature maps output by each convolutional layer in the\n",
      "module (in the same order as in Figure 14-13). Note that all the convolutional layers\n",
      "use the ReLU activation function.\n",
      "\n",
      "454 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fFigure 14-14. GoogLeNet architecture\n",
      "\n",
      "Let’s go through this network:\n",
      "\n",
      "• The first two layers divide the image’s height and width by 4 (so its area is divided\n",
      "by 16), to reduce the computational load. The first layer uses a large kernel size,\n",
      "so that much of the information is still preserved.\n",
      "\n",
      "• Then the local response normalization layer ensures that the previous layers learn\n",
      "\n",
      "a wide variety of features (as discussed earlier).\n",
      "\n",
      "• Two  convolutional  layers  follow,  where  the  first  acts  like  a  bottleneck  layer.  As\n",
      "explained  earlier,  you  can  think  of  this  pair  as  a  single  smarter  convolutional\n",
      "layer.\n",
      "\n",
      "• Again, a local response normalization layer ensures that the previous layers cap‐\n",
      "\n",
      "ture a wide variety of patterns.\n",
      "\n",
      "CNN Architectures \n",
      "\n",
      "| \n",
      "\n",
      "455\n",
      "\n",
      "\f• Next a max pooling layer reduces the image height and width by 2, again to speed\n",
      "\n",
      "up computations.\n",
      "\n",
      "• Then  comes  the  tall  stack  of  nine  inception  modules,  interleaved  with  a  couple\n",
      "\n",
      "max pooling layers to reduce dimensionality and speed up the net.\n",
      "\n",
      "• Next,  the  global  average  pooling  layer  simply  outputs  the  mean  of  each  feature\n",
      "map: this drops any remaining spatial information, which is fine since there was\n",
      "not much spatial information left at that point. Indeed, GoogLeNet input images\n",
      "are typically expected to be 224 × 224 pixels, so after 5 max pooling layers, each\n",
      "dividing the height and width by 2, the feature maps are down to 7 × 7. More‐\n",
      "over,  it  is  a  classification  task,  not  localization,  so  it  does  not  matter  where  the\n",
      "object is. Thanks to the dimensionality reduction brought by this layer, there is\n",
      "no  need  to  have  several  fully  connected  layers  at  the  top  of  the  CNN  (like  in\n",
      "AlexNet),  and  this  considerably  reduces  the  number  of  parameters  in  the  net‐\n",
      "work and limits the risk of overfitting.\n",
      "\n",
      "• The last layers are self-explanatory: dropout for regularization, then a fully con‐\n",
      "nected layer with 1,000 units, since there are a 1,000 classes, and a softmax acti‐\n",
      "vation function to output estimated class probabilities.\n",
      "\n",
      "This diagram is slightly simplified: the original GoogLeNet architecture also included\n",
      "two  auxiliary  classifiers  plugged  on  top  of  the  third  and  sixth  inception  modules.\n",
      "They were both composed of one average pooling layer, one convolutional layer, two\n",
      "fully  connected  layers,  and  a  softmax  activation  layer.  During  training,  their  loss\n",
      "(scaled down by 70%) was added to the overall loss. The goal was to fight the vanish‐\n",
      "ing gradients problem and regularize the network. However, it was later shown that\n",
      "their effect was relatively minor.\n",
      "\n",
      "Several  variants  of  the  GoogLeNet  architecture  were  later  proposed  by  Google\n",
      "researchers, including Inception-v3 and Inception-v4, using slightly different incep‐\n",
      "tion modules, and reaching even better performance.\n",
      "\n",
      "VGGNet\n",
      "The runner up in the ILSVRC 2014 challenge was VGGNet14, developed by K. Simon‐\n",
      "yan and A. Zisserman. It had a very simple and classical architecture, with 2 or 3 con‐\n",
      "volutional  layers,  a  pooling  layer,  then  again  2  or  3  convolutional  layers,  a  pooling\n",
      "layer, and so on (with a total of just 16 convolutional layers), plus a final dense net‐\n",
      "work with 2 hidden layers and the output layer. It used only 3 × 3 filters, but many\n",
      "filters.\n",
      "\n",
      "14 “Very Deep Convolutional Networks for Large-Scale Image Recognition,” K. Simonyan and A. Zisserman\n",
      "\n",
      "(2015).\n",
      "\n",
      "456 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fResNet\n",
      "The  ILSVRC  2015  challenge  was  won  using  a  Residual  Network  (or  ResNet),  devel‐\n",
      "oped  by  Kaiming  He  et  al.,15  which  delivered  an  astounding  top-5  error  rate  under\n",
      "3.6%, using an extremely deep CNN composed of 152 layers. It confirmed the general\n",
      "trend: models are getting deeper and deeper, with fewer and fewer parameters. The\n",
      "key to being able to train such a deep network is to use skip connections (also called\n",
      "shortcut connections): the signal feeding into a layer is also added to the output of a\n",
      "layer located a bit higher up the stack. Let’s see why this is useful.\n",
      "\n",
      "When training a neural network, the goal is to make it model a target function h(x).\n",
      "If you add the input x to the output of the network (i.e., you add a skip connection),\n",
      "then  the  network  will  be  forced  to  model  f(x)  =  h(x)  –  x  rather  than  h(x).  This  is\n",
      "called residual learning (see Figure 14-15).\n",
      "\n",
      "Figure 14-15. Residual learning\n",
      "\n",
      "When you initialize a regular neural network, its weights are close to zero, so the net‐\n",
      "work just outputs values close to zero. If you add a skip connection, the resulting net‐\n",
      "work just outputs a copy of its inputs; in other words, it initially models the identity\n",
      "function. If the target function is fairly close to the identity function (which is often\n",
      "the case), this will speed up training considerably.\n",
      "\n",
      "Moreover, if you add many skip connections, the network can start making progress\n",
      "even if several layers have not started learning yet (see Figure 14-16). Thanks to skip\n",
      "connections, the signal can easily make its way across the whole network. The deep\n",
      "residual network can be seen as a stack of residual units, where each residual unit is a\n",
      "small neural network with a skip connection.\n",
      "\n",
      "15 “Deep Residual Learning for Image Recognition,” K. He (2015).\n",
      "\n",
      "CNN Architectures \n",
      "\n",
      "| \n",
      "\n",
      "457\n",
      "\n",
      "\fFigure 14-16. Regular deep neural network (left) and deep residual network (right)\n",
      "\n",
      "Now  let’s  look  at  ResNet’s  architecture  (see  Figure  14-17).  It  is  actually  surprisingly\n",
      "simple.  It  starts  and  ends  exactly  like  GoogLeNet  (except  without  a  dropout  layer),\n",
      "and in between is just a very deep stack of simple residual units. Each residual unit is\n",
      "composed of two convolutional layers (and no pooling layer!), with Batch Normaliza‐\n",
      "tion (BN) and ReLU activation, using 3 × 3 kernels and preserving spatial dimensions\n",
      "(stride 1, SAME padding).\n",
      "\n",
      "Figure 14-17. ResNet architecture\n",
      "\n",
      "Note that the number of feature maps is doubled every few residual units, at the same\n",
      "time as their height and width are halved (using a convolutional layer with stride 2).\n",
      "When this happens the inputs cannot be added directly to the outputs of the residual\n",
      "unit since they don’t have the same shape (for example, this problem affects the skip\n",
      "\n",
      "458 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fconnection represented by the dashed arrow in Figure 14-17). To solve this problem,\n",
      "the inputs are passed through a 1 × 1 convolutional layer with stride 2 and the right\n",
      "number of output feature maps (see Figure 14-18).\n",
      "\n",
      "Figure 14-18. Skip connection when changing feature map size and depth\n",
      "\n",
      "ResNet-34  is  the  ResNet  with  34  layers  (only  counting  the  convolutional  layers  and\n",
      "the fully connected layer) containing three residual units that output 64 feature maps,\n",
      "4 RUs with 128 maps, 6 RUs with 256 maps, and 3 RUs with 512 maps. We will imple‐\n",
      "ment this architecture later in this chapter.\n",
      "\n",
      "ResNets  deeper  than  that,  such  as  ResNet-152,  use  slightly  different  residual  units.\n",
      "Instead of two 3 × 3 convolutional layers with (say) 256 feature maps, they use three\n",
      "convolutional  layers:  first  a  1  ×  1  convolutional  layer  with  just  64  feature  maps  (4\n",
      "times less), which acts as a bottleneck layer (as discussed already), then a 3 × 3 layer\n",
      "with 64 feature maps, and finally another 1 × 1 convolutional layer with 256 feature\n",
      "maps  (4  times  64)  that  restores  the  original  depth.  ResNet-152  contains  three  such\n",
      "RUs that output 256 maps, then 8 RUs with 512 maps, a whopping 36 RUs with 1,024\n",
      "maps, and finally 3 RUs with 2,048 maps.\n",
      "\n",
      "Google’s  Inception-v416  architecture  merged  the  ideas  of  GoogLe‐\n",
      "Net  and  ResNet  and  achieved  close  to  3%  top-5  error  rate  on\n",
      "ImageNet classification.\n",
      "\n",
      "Xception\n",
      "Another  variant  of  the  GoogLeNet  architecture  is  also  worth  noting:  Xception17\n",
      "(which stands for Extreme Inception) was proposed in 2016 by François Chollet (the\n",
      "\n",
      "16 “Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning,” C. Szegedy et al.\n",
      "\n",
      "(2016).\n",
      "\n",
      "17 “Xception: Deep Learning with Depthwise Separable Convolutions,” François Chollet (2016)\n",
      "\n",
      "CNN Architectures \n",
      "\n",
      "| \n",
      "\n",
      "459\n",
      "\n",
      "\fauthor of Keras), and it significantly outperformed Inception-v3 on a huge vision task\n",
      "(350  million  images  and  17,000  classes).  Just  like  Inception-v4,  it  also  merges  the\n",
      "ideas of GoogLeNet and ResNet, but it replaces the inception modules with a special\n",
      "type  of  layer  called  a  depthwise  separable  convolution  (or  separable  convolution  for\n",
      "short18). These layers had been used before in some CNN architectures, but they were\n",
      "not  as  central  as  in  the  Xception  architecture.  While  a  regular  convolutional  layer\n",
      "uses filters that try to simultaneously capture spatial patterns (e.g., an oval) and cross-\n",
      "channel patterns (e.g., mouth + nose + eyes = face), a separable convolutional layer\n",
      "makes the strong assumption that spatial patterns and cross-channel patterns can be\n",
      "modeled separately (see Figure 14-19). Thus, it is composed of two parts: the first part\n",
      "applies  a  single  spatial  filter  for  each  input  feature  map,  then  the  second  part  looks\n",
      "exclusively for cross-channel patterns—it is just a regular convolutional layer with 1 ×\n",
      "1 filters.\n",
      "\n",
      "Figure 14-19. Depthwise Separable Convolutional Layer\n",
      "\n",
      "Since  separable  convolutional  layers  only  have  one  spatial  filter  per  input  channel,\n",
      "you should avoid using them after layers that have too few channels, such as the input\n",
      "layer (granted, that’s what Figure 14-19 represents, but it is just for illustration pur‐\n",
      "poses). For this reason, the Xception architecture starts with 2 regular convolutional\n",
      "layers,  but  then  the  rest  of  the  architecture  uses  only  separable  convolutions  (34  in\n",
      "\n",
      "18 This name can sometimes be ambiguous, since spatially separable convolutions are often called “separable\n",
      "\n",
      "convolutions” as well.\n",
      "\n",
      "460 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fall), plus a few max pooling layers and the usual final layers (a global average pooling\n",
      "layer, and a dense output layer).\n",
      "\n",
      "You might wonder why Xception is considered a variant of GoogLeNet, since it con‐\n",
      "tains no inception module at all? Well, as we discussed earlier, an Inception module\n",
      "contains  convolutional  layers  with  1  ×  1  filters:  these  look  exclusively  for  cross-\n",
      "channel patterns. However, the convolution layers that sit on top of them are regular\n",
      "convolutional layers that look both for spatial and cross-channel patterns. So you can\n",
      "think  of  an  Inception  module  as  an  intermediate  between  a  regular  convolutional\n",
      "layer (which considers spatial patterns and cross-channel patterns jointly) and a sepa‐\n",
      "rable convolutional layer (which considers them separately). In practice, it seems that\n",
      "separable convolutions generally perform better.\n",
      "\n",
      "Separable  convolutions  use  less  parameters,  less  memory  and  less\n",
      "computations  than  regular  convolutional  layers,  and  in  general\n",
      "they  even  perform  better,  so  you  should  consider  using  them  by\n",
      "default (except after layers with few channels).\n",
      "\n",
      "The ILSVRC 2016 challenge was won by the CUImage team from the Chinese Uni‐\n",
      "versity of Hong Kong. They used an ensemble of many different techniques, includ‐\n",
      "ing a sophisticated object-detection system called GBD-Net19, to achieve a top-5 error\n",
      "rate below 3%. Although this result is unquestionably impressive, the complexity of\n",
      "the  solution  contrasted  with  the  simplicity  of  ResNets.  Moreover,  one  year  later\n",
      "another fairly simple architecture performed even better, as we will see now.\n",
      "\n",
      "SENet\n",
      "The  winning  architecture  in  the  ILSVRC  2017  challenge  was  the  Squeeze-and-\n",
      "Excitation Network (SENet)20. This architecture extends existing architectures such as\n",
      "inception networks or ResNets, and boosts their performance. This allowed SENet to\n",
      "win  the  competition  with  an  astonishing  2.25%  top-5  error  rate!  The  extended  ver‐\n",
      "sions of inception networks and ResNet are called SE-Inception and SE-ResNet respec‐\n",
      "tively. The boost comes from the fact that a SENet adds a small neural network, called\n",
      "a SE Block, to every unit in the original architecture (i.e., every inception module or\n",
      "every residual unit), as shown in Figure 14-20.\n",
      "\n",
      "19 “Crafting GBD-Net for Object Detection,” X. Zeng et al. (2016).\n",
      "\n",
      "20 “Squeeze-and-Excitation Networks,” Jie Hu et al. (2017)\n",
      "\n",
      "CNN Architectures \n",
      "\n",
      "| \n",
      "\n",
      "461\n",
      "\n",
      "\fFigure 14-20. SE-Inception Module (left) and SE-ResNet Unit (right)\n",
      "\n",
      "A SE Block analyzes the output of the unit it is attached to, focusing exclusively on\n",
      "the depth dimension (it does not look for any spatial pattern), and it learns which fea‐\n",
      "tures are usually most active together. It then uses this information to recalibrate the\n",
      "feature  maps,  as  shown  in  Figure  14-21.  For  example,  a  SE  Block  may  learn  that\n",
      "mouths, noses and eyes usually appear together in pictures: if you see a mouth and a\n",
      "nose, you should expect to see eyes as well. So if a SE Block sees a strong activation in\n",
      "the mouth and nose feature maps, but only mild activation in the eye feature map, it\n",
      "will  boost  the  eye  feature  map  (more  accurately,  it  will  reduce  irrelevant  feature\n",
      "maps).  If  the  eyes  were  somewhat  confused  with  something  else,  this  feature  map\n",
      "recalibration will help resolve the ambiguity.\n",
      "\n",
      "462 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fFigure 14-21. An SE Block Performs Feature Map Recalibration\n",
      "\n",
      "A SE Block is composed of just 3 layers: a global average pooling layer, a hidden dense\n",
      "layer using the ReLU activation function, and a dense output layer using the sigmoid\n",
      "activation function (see Figure 14-22):\n",
      "\n",
      "Figure 14-22. SE Block Architecture\n",
      "\n",
      "CNN Architectures \n",
      "\n",
      "| \n",
      "\n",
      "463\n",
      "\n",
      "\fAs earlier, the global average pooling layer computes the mean activation for each fea‐\n",
      "ture map: for example, if its input contains 256 feature maps, it will output 256 num‐\n",
      "bers representing the overall level of response for each filter. The next layer is where\n",
      "the “squeeze” happens: this layer has much less than 256 neurons, typically 16 times\n",
      "less than the number of feature maps (e.g., 16 neurons), so the 256 numbers get com‐\n",
      "pressed  into  a  small  vector  (e.g.,  16  dimensional).  This  is  a  low-dimensional  vector\n",
      "representation (i.e., an embedding) of the distribution of feature responses. This bot‐\n",
      "tleneck step forces the SE Block to learn a general representation of the feature com‐\n",
      "binations  (we  will  see  this  principle  in  action  again  when  we  discuss  autoencoders\n",
      "in ???). Finally, the output layer takes the embedding and outputs a recalibration vec‐\n",
      "tor  containing  one  number  per  feature  map  (e.g.,  256),  each  between  0  and  1.  The\n",
      "feature  maps  are  then  multiplied  by  this  recalibration  vector,  so  irrelevant  features\n",
      "(with a low recalibration score) get scaled down while relevant features (with a recali‐\n",
      "bration score close to 1) are left alone.\n",
      "\n",
      "Implementing a ResNet-34 CNN Using Keras\n",
      "Most  CNN  architectures  described  so  far  are  fairly  straightforward  to  implement\n",
      "(although generally you would load a pretrained network instead, as we will see). To\n",
      "illustrate the process, let’s implement a ResNet-34 from scratch using Keras. First, let’s\n",
      "create a ResidualUnit layer:\n",
      "\n",
      "DefaultConv2D = partial(keras.layers.Conv2D, kernel_size=3, strides=1,\n",
      "                        padding=\"SAME\", use_bias=False)\n",
      "\n",
      "class ResidualUnit(keras.layers.Layer):\n",
      "    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n",
      "        super().__init__(**kwargs)\n",
      "        self.activation = keras.activations.get(activation)\n",
      "        self.main_layers = [\n",
      "            DefaultConv2D(filters, strides=strides),\n",
      "            keras.layers.BatchNormalization(),\n",
      "            self.activation,\n",
      "            DefaultConv2D(filters),\n",
      "            keras.layers.BatchNormalization()]\n",
      "        self.skip_layers = []\n",
      "        if strides > 1:\n",
      "            self.skip_layers = [\n",
      "                DefaultConv2D(filters, kernel_size=1, strides=strides),\n",
      "                keras.layers.BatchNormalization()]\n",
      "\n",
      "    def call(self, inputs):\n",
      "        Z = inputs\n",
      "        for layer in self.main_layers:\n",
      "            Z = layer(Z)\n",
      "        skip_Z = inputs\n",
      "        for layer in self.skip_layers:\n",
      "\n",
      "464 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\f            skip_Z = layer(skip_Z)\n",
      "        return self.activation(Z + skip_Z)\n",
      "\n",
      "As you can see, this code matches Figure 14-18 pretty closely. In the constructor, we\n",
      "create all the layers we will need: the main layers are the ones on the right side of the\n",
      "diagram,  and  the  skip  layers  are  the  ones  on  the  left  (only  needed  if  the  stride  is\n",
      "greater than 1). Then in the call() method, we simply make the inputs go through\n",
      "the main layers, and the skip layers (if any), then we add both outputs and we apply\n",
      "the activation function.\n",
      "\n",
      "Next, we can build the ResNet-34 simply using a Sequential model, since it is really\n",
      "just  a  long  sequence  of  layers  (we  can  treat  each  residual  unit  as  a  single  layer  now\n",
      "that we have the ResidualUnit class):\n",
      "\n",
      "model = keras.models.Sequential()\n",
      "model.add(DefaultConv2D(64, kernel_size=7, strides=2,\n",
      "                        input_shape=[224, 224, 3]))\n",
      "model.add(keras.layers.BatchNormalization())\n",
      "model.add(keras.layers.Activation(\"relu\"))\n",
      "model.add(keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"SAME\"))\n",
      "prev_filters = 64\n",
      "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
      "    strides = 1 if filters == prev_filters else 2\n",
      "    model.add(ResidualUnit(filters, strides=strides))\n",
      "    prev_filters = filters\n",
      "model.add(keras.layers.GlobalAvgPool2D())\n",
      "model.add(keras.layers.Flatten())\n",
      "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
      "\n",
      "The only slightly tricky part in this code is the loop that adds the ResidualUnit layers\n",
      "to the model: as explained earlier, the first 3 RUs have 64 filters, then the next 4 RUs\n",
      "have 128 filters, and so on. We then set the strides to 1 when the number of filters is\n",
      "the same as in the previous RU, or else we set it to 2. Then we add the ResidualUnit,\n",
      "and finally we update prev_filters.\n",
      "\n",
      "It is quite amazing that in less than 40 lines of code, we can build the model that won\n",
      "the ILSVRC 2015 challenge! It demonstrates both the elegance of the ResNet model,\n",
      "and the expressiveness of the Keras API. Implementing the other CNN architectures\n",
      "is not much harder. However, Keras comes with several of these architectures built in,\n",
      "so why not use them instead?\n",
      "\n",
      "Using Pretrained Models From Keras\n",
      "In general, you won’t have to implement standard models like GoogLeNet or ResNet\n",
      "manually, since pretrained networks are readily available with a single line of code, in\n",
      "the keras.applications package. For example:\n",
      "\n",
      "model = keras.applications.resnet50.ResNet50(weights=\"imagenet\")\n",
      "\n",
      "Using Pretrained Models From Keras \n",
      "\n",
      "| \n",
      "\n",
      "465\n",
      "\n",
      "\fThat’s  all!  This  will  create  a  ResNet-50  model  and  download  weights  pretrained  on\n",
      "the ImageNet dataset. To use it, you first need to ensure that the images have the right\n",
      "size. A ResNet-50 model expects 224 × 224 images (other models may expect other\n",
      "sizes,  such  as  299  ×  299),  so  let’s  use  TensorFlow’s  tf.image.resize()  function  to\n",
      "resize the images we loaded earlier:\n",
      "\n",
      "images_resized = tf.image.resize(images, [224, 224])\n",
      "\n",
      "The tf.image.resize() will not preserve the aspect ratio. If this is\n",
      "a  problem,  you  can  try  cropping  the  images  to  the  appropriate\n",
      "aspect  ratio  before  resizing.  Both  operations  can  be  done  in  one\n",
      "shot with tf.image.crop_and_resize().\n",
      "\n",
      "The pretrained models assume that the images are preprocessed in a specific way. In\n",
      "some cases they may expect the inputs to be scaled from 0 to 1, or -1 to 1, and so on.\n",
      "Each model provides a preprocess_input() function that you can use to preprocess\n",
      "your images. These functions assume that the pixel values range from 0 to 255, so we\n",
      "must multiply them by 255 (since earlier we scaled them to the 0–1 range):\n",
      "\n",
      "inputs = keras.applications.resnet50.preprocess_input(images_resized * 255)\n",
      "\n",
      "Now we can use the pretrained model to make predictions:\n",
      "\n",
      "Y_proba = model.predict(inputs)\n",
      "\n",
      "As usual, the output Y_proba is a matrix with one row per image and one column per\n",
      "class  (in  this  case,  there  are  1,000  classes).  If  you  want  to  display  the  top  K  predic‐\n",
      "tions, including the class name and the estimated probability of each predicted class,\n",
      "you can use the decode_predictions() function. For each image, it returns an array\n",
      "containing  the  top  K  predictions,  where  each  prediction  is  represented  as  an  array\n",
      "containing the class identifier21, its name and the corresponding confidence score:\n",
      "\n",
      "top_K = keras.applications.resnet50.decode_predictions(Y_proba, top=3)\n",
      "for image_index in range(len(images)):\n",
      "    print(\"Image #{}\".format(image_index))\n",
      "    for class_id, name, y_proba in top_K[image_index]:\n",
      "        print(\"  {} - {:12s} {:.2f}%\".format(class_id, name, y_proba * 100))\n",
      "    print()\n",
      "\n",
      "The output looks like this:\n",
      "\n",
      "Image #0\n",
      "  n03877845 - palace       42.87%\n",
      "  n02825657 - bell_cote    40.57%\n",
      "  n03781244 - monastery    14.56%\n",
      "\n",
      "21 In the ImageNet dataset, each image is associated to a word in the WordNet dataset: the class ID is just a\n",
      "\n",
      "WordNet ID.\n",
      "\n",
      "466 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fImage #1\n",
      "  n04522168 - vase         46.83%\n",
      "  n07930864 - cup          7.78%\n",
      "  n11939491 - daisy        4.87%\n",
      "\n",
      "The correct classes (monastery and daisy) appear in the top 3 results for both images.\n",
      "That’s pretty good considering that the model had to choose among 1,000 classes.\n",
      "\n",
      "As  you  can  see,  it  is  very  easy  to  create  a  pretty  good  image  classifier  using  a  pre‐\n",
      "trained model. Other vision models are available in keras.applications, including\n",
      "several  ResNet  variants,  GoogLeNet  variants  like  InceptionV3  and  Xception,\n",
      "VGGNet  variants,  MobileNet  and  MobileNetV2  (lightweight  models  for  use  in\n",
      "mobile applications), and more.\n",
      "\n",
      "But what if you want to use an image classifier for classes of images that are not part\n",
      "of ImageNet? In that case, you may still benefit from the pretrained models to per‐\n",
      "form transfer learning.\n",
      "\n",
      "Pretrained Models for Transfer Learning\n",
      "If  you  want  to  build  an  image  classifier,  but  you  do  not  have  enough  training  data,\n",
      "then it is often a good idea to reuse the lower layers of a pretrained model, as we dis‐\n",
      "cussed in Chapter 11. For example, let’s train a model to classify pictures of flowers,\n",
      "reusing  a  pretrained  Xception  model.  First,  let’s  load  the  dataset  using  TensorFlow\n",
      "Datasets (see Chapter 13):\n",
      "\n",
      "import tensorflow_datasets as tfds\n",
      "\n",
      "dataset, info = tfds.load(\"tf_flowers\", as_supervised=True, with_info=True)\n",
      "dataset_size = info.splits[\"train\"].num_examples # 3670\n",
      "class_names = info.features[\"label\"].names # [\"dandelion\", \"daisy\", ...]\n",
      "n_classes = info.features[\"label\"].num_classes # 5\n",
      "\n",
      "Note that you can get information about the dataset by setting with_info=True. Here,\n",
      "we  get  the  dataset  size  and  the  names  of  the  classes.  Unfortunately,  there  is  only  a\n",
      "\"train\" dataset, no test set or validation set, so we need to split the training set. The\n",
      "TF Datasets project provides an API for this. For example, let’s take the first 10% of\n",
      "the dataset for testing, the next 15% for validation, and the remaining 75% for train‐\n",
      "ing:\n",
      "\n",
      "test_split, valid_split, train_split = tfds.Split.TRAIN.subsplit([10, 15, 75])\n",
      "\n",
      "test_set = tfds.load(\"tf_flowers\", split=test_split, as_supervised=True)\n",
      "valid_set = tfds.load(\"tf_flowers\", split=valid_split, as_supervised=True)\n",
      "train_set = tfds.load(\"tf_flowers\", split=train_split, as_supervised=True)\n",
      "\n",
      "Pretrained Models for Transfer Learning \n",
      "\n",
      "| \n",
      "\n",
      "467\n",
      "\n",
      "\fNext we must preprocess the images. The CNN expects 224 × 224 images, so we need\n",
      "to  resize  them.  We  also  need  to  run  the  image  through  Xception’s  prepro\n",
      "cess_input() function:\n",
      "\n",
      "def preprocess(image, label):\n",
      "    resized_image = tf.image.resize(image, [224, 224])\n",
      "    final_image = keras.applications.xception.preprocess_input(resized_image)\n",
      "    return final_image, label\n",
      "\n",
      "Let’s apply this preprocessing function to all 3 datasets, and let’s also shuffle & repeat\n",
      "the training set, and add batching & prefetching to all datasets:\n",
      "\n",
      "batch_size = 32\n",
      "train_set = train_set.shuffle(1000).repeat()\n",
      "train_set = train_set.map(preprocess).batch(batch_size).prefetch(1)\n",
      "valid_set = valid_set.map(preprocess).batch(batch_size).prefetch(1)\n",
      "test_set = test_set.map(preprocess).batch(batch_size).prefetch(1)\n",
      "\n",
      "If you want to perform some data augmentation, you can just change the preprocess‐\n",
      "ing function for the training set, adding some random transformations to the training\n",
      "images. For example, use tf.image.random_crop() to randomly crop the images, use\n",
      "tf.image.random_flip_left_right() to randomly flip the images horizontally, and\n",
      "so on (see the notebook for an example).\n",
      "\n",
      "Next let’s load an Xception model, pretrained on ImageNet. We exclude the top of the\n",
      "network  (by  setting  include_top=False):  this  excludes  the  global  average  pooling\n",
      "layer and the dense output layer. We then add our own global average pooling layer,\n",
      "based on the output of the base model, followed by a dense output layer with 1 unit\n",
      "per class, using the softmax activation function. Finally, we create the Keras Model:\n",
      "\n",
      "base_model = keras.applications.xception.Xception(weights=\"imagenet\",\n",
      "                                                  include_top=False)\n",
      "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
      "output = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
      "model = keras.models.Model(inputs=base_model.input, outputs=output)\n",
      "\n",
      "As explained in Chapter 11, it’s usually a good idea to freeze the weights of the pre‐\n",
      "trained layers, at least at the beginning of training:\n",
      "\n",
      "for layer in base_model.layers:\n",
      "    layer.trainable = False\n",
      "\n",
      "Since  our  model  uses  the  base  model’s  layers  directly,  rather  than\n",
      "the base_model object itself, setting base_model.trainable=False\n",
      "would have no effect.\n",
      "\n",
      "Finally, we can compile the model and start training:\n",
      "\n",
      "468 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\foptimizer = keras.optimizers.SGD(lr=0.2, momentum=0.9, decay=0.01)\n",
      "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
      "              metrics=[\"accuracy\"])\n",
      "history = model.fit(train_set,\n",
      "                    steps_per_epoch=int(0.75 * dataset_size / batch_size),\n",
      "                    validation_data=valid_set,\n",
      "                    validation_steps=int(0.15 * dataset_size / batch_size),\n",
      "                    epochs=5)\n",
      "\n",
      "This will be very slow, unless you have a GPU. If you do not, then\n",
      "you should run this chapter’s notebook in Colab, using a GPU run‐\n",
      "time  (it’s  free!).  See  the  instructions  at  https://github.com/ageron/\n",
      "handson-ml2.\n",
      "\n",
      "After training the model for a few epochs, its validation accuracy should reach about\n",
      "75-80%,  and  stop  making  much  progress.  This  means  that  the  top  layers  are  now\n",
      "pretty well trained, so we are ready to unfreeze all layers (or you could try unfreezing\n",
      "just the top ones), and continue training (don’t forget to compile the model when you\n",
      "freeze or unfreeze layers). This time we use a much lower learning rate to avoid dam‐\n",
      "aging the pretrained weights:\n",
      "\n",
      "for layer in base_model.layers:\n",
      "    layer.trainable = True\n",
      "\n",
      "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.001)\n",
      "model.compile(...)\n",
      "history = model.fit(...)\n",
      "\n",
      "It will take a while, but this model should reach around 95% accuracy on the test set.\n",
      "With that, you can start training amazing image classifiers! But there’s more to com‐\n",
      "puter vision than just classification. For example, what if you also want to know where\n",
      "the flower is in the picture? Let’s look at this now.\n",
      "\n",
      "Classification and Localization\n",
      "Localizing an object in a picture can be expressed as a regression task, as discussed in\n",
      "Chapter 10: to predict a bounding box around the object, a common approach is to\n",
      "predict  the  horizontal  and  vertical  coordinates  of  the  object’s  center,  as  well  as  its\n",
      "height and width. This means we have 4 numbers to predict. It does not require much\n",
      "change  to  the  model,  we  just  need  to  add  a  second  dense  output  layer  with  4  units\n",
      "(typically on top of the global average pooling layer), and it can be trained using the\n",
      "MSE loss:\n",
      "\n",
      "base_model = keras.applications.xception.Xception(weights=\"imagenet\",\n",
      "                                                  include_top=False)\n",
      "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
      "class_output = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
      "\n",
      "Classification and Localization \n",
      "\n",
      "| \n",
      "\n",
      "469\n",
      "\n",
      "\floc_output = keras.layers.Dense(4)(avg)\n",
      "model = keras.models.Model(inputs=base_model.input,\n",
      "                           outputs=[class_output, loc_output])\n",
      "model.compile(loss=[\"sparse_categorical_crossentropy\", \"mse\"],\n",
      "              loss_weights=[0.8, 0.2], # depends on what you care most about\n",
      "              optimizer=optimizer, metrics=[\"accuracy\"])\n",
      "\n",
      "But  now  we  have  a  problem:  the  flowers  dataset  does  not  have  bounding  boxes\n",
      "around the flowers. So we need to add them ourselves. This is often one of the hard‐\n",
      "est and most costly part of a Machine Learning project: getting the labels. It’s a good\n",
      "idea  to  spend  time  looking  for  the  right  tools.  To  annotate  images  with  bounding\n",
      "boxes,  you  may  want  to  use  an  open  source  image  labeling  tool  like  VGG  Image\n",
      "Annotator,  LabelImg,  OpenLabeler  or  ImgLab,  or  perhaps  a  commercial  tool  like\n",
      "LabelBox  or  Supervisely.  You  may  also  want  to  consider  crowdsourcing  platforms\n",
      "such as Amazon Mechanical Turk or CrowdFlower if you have a very large number of\n",
      "images to annotate. However, it is quite a lot of work to setup a crowdsourcing plat‐\n",
      "form, prepare the form to be sent to the workers, to supervise them and ensure the\n",
      "quality  of  the  bounding  boxes  they  produce  is  good,  so  make  sure  it  is  worth  the\n",
      "effort: if there are just a few thousand images to label, and you don’t plan to do this\n",
      "frequently, it may be preferable to do it yourself. Adriana Kovashka et al. wrote a very\n",
      "practical paper22 about crowdsourcing in Computer Vision, I recommend you check\n",
      "it out, even if you do not plan to use crowdsourcing.\n",
      "\n",
      "So let’s suppose you obtained the bounding boxes for every image in the flowers data‐\n",
      "set (for now we will assume there is a single bounding box per image), you then need\n",
      "to  create  a  dataset  whose  items  will  be  batches  of  preprocessed  images  along  with\n",
      "their class labels and their bounding boxes. Each item should be a tuple of the form:\n",
      "(images,  (class_labels,  bounding_boxes)).  Then  you  are  ready  to  train  your\n",
      "model!\n",
      "\n",
      "The  bounding  boxes  should  be  normalized  so  that  the  horizontal\n",
      "and  vertical  coordinates,  as  well  as  the  height  and  width  all  range\n",
      "from  0  to  1.  Also,  it  is  common  to  predict  the  square  root  of  the\n",
      "height  and  width  rather  than  the  height  and  width  directly:  this\n",
      "way, a 10 pixel error for a large bounding box will not be penalized\n",
      "as much as a 10 pixel error for a small bounding box.\n",
      "\n",
      "The MSE often works fairly well as a cost function to train the model, but it is not a\n",
      "great metric to evaluate how well the model can predict bounding boxes. The most\n",
      "common metric for this is the Intersection over Union (IoU): it is the area of overlap\n",
      "between  the  predicted  bounding  box  and  the  target  bounding  box,  divided  by  the\n",
      "\n",
      "22 “Crowdsourcing in Computer Vision,” A. Kovashka et al. (2016).\n",
      "\n",
      "470 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\farea  of  their  union  (see  Figure  14-23).  In  tf.keras,  it  is  implemented  by  the\n",
      "tf.keras.metrics.MeanIoU class.\n",
      "\n",
      "Figure 14-23. Intersection over Union (IoU) Metric for Bounding Boxes\n",
      "\n",
      "Classifying and localizing a single object is nice, but what if the images contain multi‐\n",
      "ple objects (as is often the case in the flowers dataset)?\n",
      "\n",
      "Object Detection\n",
      "The  task  of  classifying  and  localizing  multiple  objects  in  an  image  is  called  object\n",
      "detection.  Until  a  few  years  ago,  a  common  approach  was  to  take  a  CNN  that  was\n",
      "trained to classify and locate a single object, then slide it across the image, as shown\n",
      "in  Figure  14-24.  In  this  example,  the  image  was  chopped  into  a  6  ×  8  grid,  and  we\n",
      "show  a  CNN  (the  thick  black  rectangle)  sliding  across  all  3  ×  3  regions.  When  the\n",
      "CNN was looking at the top left of the image, it detected part of the left-most rose,\n",
      "and  then  it  detected  that  same  rose  again  when  it  was  first  shifted  one  step  to  the\n",
      "right. At the next step, it started detecting part of the top-most rose, and then it detec‐\n",
      "ted it again once it was shifted one more step to the right. You would then continue to\n",
      "slide the CNN through the whole image, looking at all 3 × 3 regions. Moreover, since\n",
      "objects can have varying sizes, you would also slide the CNN across regions of differ‐\n",
      "ent sizes. For example, once you are done with the 3 × 3 regions, you might want to\n",
      "slide the CNN across all 4 × 4 regions as well.\n",
      "\n",
      "Object Detection \n",
      "\n",
      "| \n",
      "\n",
      "471\n",
      "\n",
      "\fFigure 14-24. Detecting Multiple Objects by Sliding a CNN Across the Image\n",
      "\n",
      "This  technique  is  fairly  straightforward,  but  as  you  can  see  it  will  detect  the  same\n",
      "object  multiple  times,  at  slightly  different  positions.  Some  post-processing  will  then\n",
      "be needed to get rid of all the unnecessary bounding boxes. A common approach for\n",
      "this is called non-max suppression:\n",
      "\n",
      "• First,  you  need  to  add  an  extra  objectness  output  to  your  CNN,  to  estimate  the\n",
      "probability that a flower is indeed present in the image (alternatively, you could\n",
      "add a “no-flower” class, but this usually does not work as well). It must use the\n",
      "sigmoid  activation  function  and  you  can  train  it  using  the  \"binary_crossen\n",
      "tropy\" loss. Then just get rid of all the bounding boxes for which the objectness\n",
      "score  is  below  some  threshold:  this  will  drop  all  the  bounding  boxes  that  don’t\n",
      "actually contain a flower.\n",
      "\n",
      "• Second, find the bounding box with the highest objectness score, and get rid of\n",
      "all the other bounding boxes that overlap a lot with it (e.g., with an IoU greater\n",
      "than 60%). For example, in Figure 14-24, the bounding box with the max object‐\n",
      "ness score is the thick bounding box over the top-most rose (the objectness score\n",
      "is represented by the thickness of the bounding boxes). The other bounding box\n",
      "over that same rose overlaps a lot with the max bounding box, so we will get rid\n",
      "of it.\n",
      "\n",
      "472 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\f• Third, repeat step two until there are no more bounding boxes to get rid of.\n",
      "\n",
      "This  simple  approach  to  object  detection  works  pretty  well,  but  it  requires  running\n",
      "the  CNN  many  times,  so  it  is  quite  slow.  Fortunately,  there  is  a  much  faster  way  to\n",
      "slide a CNN across an image: using a Fully Convolutional Network.\n",
      "\n",
      "Fully Convolutional Networks (FCNs)\n",
      "The idea of FCNs was first introduced in a 2015 paper23 by Jonathan Long et al., for\n",
      "semantic  segmentation  (the  task  of  classifying  every  pixel  in  an  image  according  to\n",
      "the  class  of  the  object  it  belongs  to).  They  pointed  out  that  you  could  replace  the\n",
      "dense layers at the top of a CNN by convolutional layers. To understand this, let’s look\n",
      "at an example: suppose a dense layer with 200 neurons sits on top of a convolutional\n",
      "layer that outputs 100 feature maps, each of size 7 × 7 (this is the feature map size, not\n",
      "the kernel size). Each neuron will compute a weighted sum of all 100 × 7 × 7 activa‐\n",
      "tions from the convolutional layer (plus a bias term). Now let’s see what happens if we\n",
      "replace the dense layer with a convolution layer using 200 filters, each 7 × 7, and with\n",
      "VALID padding. This layer will output 200 feature maps, each 1 × 1 (since the kernel\n",
      "is  exactly  the  size  of  the  input  feature  maps  and  we  are  using  VALID  padding).  In\n",
      "other words, it will output 200 numbers, just like the dense layer did, and if you look\n",
      "closely at the computations performed by a convolutional layer, you will notice that\n",
      "these numbers will be precisely the same as the dense layer produced. The only differ‐\n",
      "ence is that the dense layer’s output was a tensor of shape [batch size, 200] while the\n",
      "convolutional layer will output a tensor of shape [batch size, 1, 1, 200].\n",
      "\n",
      "To convert a dense layer to a convolutional layer, the number of fil‐\n",
      "ters in the convolutional layer must be equal to the number of units\n",
      "in  the  dense  layer,  the  filter  size  must  be  equal  to  the  size  of  the\n",
      "input feature maps, and you must use VALID padding. The stride\n",
      "may be set to 1 or more, as we will see shortly.\n",
      "\n",
      "Why is this important? Well, while a dense layer expects a specific input size (since it\n",
      "has one weight per input feature), a convolutional layer will happily process images of\n",
      "any  size24  (however,  it  does  expect  its  inputs  to  have  a  specific  number  of  channels,\n",
      "since each kernel contains a different set of weights for each input channel). Since an\n",
      "FCN  contains  only  convolutional  layers  (and  pooling  layers,  which  have  the  same\n",
      "property), it can be trained and executed on images of any size!\n",
      "\n",
      "23 “Fully Convolutional Networks for Semantic Segmentation,” J. Long, E. Shelhamer, T. Darrell (2015).\n",
      "\n",
      "24 There is one small exception: a convolutional layer using VALID padding will complain if the input size is\n",
      "\n",
      "smaller than the kernel size.\n",
      "\n",
      "Object Detection \n",
      "\n",
      "| \n",
      "\n",
      "473\n",
      "\n",
      "\fFor example, suppose we already trained a CNN for flower classification and localiza‐\n",
      "tion. It was trained on 224 × 224 images and it outputs 10 numbers: outputs 0 to 4 are\n",
      "sent  through  the  softmax  activation  function,  and  this  gives  the  class  probabilities\n",
      "(one per class); output 5 is sent through the logistic activation function, and this gives\n",
      "the objectness score; outputs 6 to 9 do not use any activation function, and they rep‐\n",
      "resent the bounding box’s center coordinates, and its height and width. We can now\n",
      "convert its dense layers to convolutional layers. In fact, we don’t even need to retrain\n",
      "it,  we  can  just  copy  the  weights  from  the  dense  layers  to  the  convolutional  layers!\n",
      "Alternatively, we could have converted the CNN into an FCN before training.\n",
      "\n",
      "Now suppose the last convolutional layer before the output layer (also called the bot‐\n",
      "tleneck layer) outputs 7 × 7 feature maps when the network is fed a 224 × 224 image\n",
      "(see the left side of Figure 14-25). If we feed the FCN a 448 × 448 image (see the right\n",
      "side  of  Figure  14-25),  the  bottleneck  layer  will  now  output  14  ×  14  feature  maps.25\n",
      "Since the dense output layer was replaced by a convolutional layer using 10 filters of\n",
      "size 7 × 7, VALID padding and stride 1, the output will be composed of 10 features\n",
      "maps, each of size 8 × 8 (since 14 - 7 + 1 = 8). In other words, the FCN will process\n",
      "the whole image only once and it will output an 8 × 8 grid where each cell contains 10\n",
      "numbers (5 class probabilities, 1 objectness score and 4 bounding box coordinates).\n",
      "It’s exactly like taking the original CNN and sliding it across the image using 8 steps\n",
      "per  row  and  8  steps  per  column:  to  visualize  this,  imagine  chopping  the  original\n",
      "image into a 14 × 14 grid, then sliding a 7 × 7 window across this grid: there will be 8\n",
      "×  8  =  64  possible  locations  for  the  window,  hence  8  ×  8  predictions.  However,  the\n",
      "FCN  approach  is  much  more  efficient,  since  the  network  only  looks  at  the  image\n",
      "once. In fact, You Only Look Once (YOLO) is the name of a very popular object detec‐\n",
      "tion architecture!\n",
      "\n",
      "25 This assumes we used only SAME padding in the network: indeed, VALID padding would reduce the size of\n",
      "\n",
      "the feature maps. Moreover, 448 can be neatly divided by 2 several times until we reach 7, without any round‐\n",
      "ing error. If any layer uses a different stride than 1 or 2, then there may be some rounding error, so again the\n",
      "feature maps may end up being smaller.\n",
      "\n",
      "474 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fFigure 14-25. A Fully Convolutional Network Processing a Small Image (left) and a\n",
      "Large One (right)\n",
      "\n",
      "You Only Look Once (YOLO)\n",
      "YOLO  is  an  extremely  fast  and  accurate  object  detection  architecture  proposed  by\n",
      "Joseph  Redmon  et  al.  in  a  2015  paper26,  and  subsequently  improved  in  201627\n",
      "(YOLOv2) and in 201828 (YOLOv3). It is so fast that it can run in realtime on a video\n",
      "(check out this nice demo).\n",
      "\n",
      "YOLOv3’s  architecture  is  quite  similar  to  the  one  we  just  discussed,  but  with  a  few\n",
      "important differences:\n",
      "\n",
      "26 “You Only Look Once: Unified, Real-Time Object Detection,” J. Redmon, S. Divvala, R. Girshick, A. Farhadi\n",
      "\n",
      "(2015).\n",
      "\n",
      "27 “YOLO9000: Better, Faster, Stronger,” J. Redmon, A. Farhadi (2016).\n",
      "\n",
      "28 “YOLOv3: An Incremental Improvement,” J. Redmon, A. Farhadi (2018).\n",
      "\n",
      "Object Detection \n",
      "\n",
      "| \n",
      "\n",
      "475\n",
      "\n",
      "\f• First, it outputs 5 bounding boxes for each grid cell (instead of just 1), and each\n",
      "bounding box comes with an objectness score. It also outputs 20 class probabili‐\n",
      "ties per grid cell, as it was trained on the PASCAL VOC dataset, which contains\n",
      "20 classes. That’s a total of 45 numbers per grid cell (5 * 4 bounding box coordi‐\n",
      "nates, plus 5 objectness scores, plus 20 class probabilities).\n",
      "\n",
      "• Second, instead of predicting the absolute coordinates of the bounding box cen‐\n",
      "ters, YOLOv3 predicts an offset relative to the coordinates of the grid cell, where\n",
      "(0, 0) means the top left of that cell, and (1, 1) means the bottom right. For each\n",
      "grid cell, YOLOv3 is trained to predict only bounding boxes whose center lies in\n",
      "that cell (but the bounding box itself generally extends well beyond the grid cell).\n",
      "YOLOv3 applies the logistic activation function to the bounding box coordinates\n",
      "to ensure they remain in the 0 to 1 range.\n",
      "\n",
      "• Third,  before  training  the  neural  net,  YOLOv3  finds  5  representative  bounding\n",
      "box  dimensions,  called  anchor  boxes  (or  bounding  box  priors):  it  does  this  by\n",
      "applying the K-Means algorithm (see ???) to the height and width of the training\n",
      "set  bounding  boxes.  For  example,  if  the  training  images  contain  many  pedes‐\n",
      "trians, then one of the anchor boxes will likely have the dimensions of a typical\n",
      "pedestrian. Then when the neural net predicts 5 bounding boxes per grid cell, it\n",
      "actually  predicts  how  much  to  rescale  each  of  the  anchor  boxes.  For  example,\n",
      "suppose  one  anchor  box  is  100  pixels  tall  and  50  pixels  wide,  and  the  network\n",
      "predicts, say, a vertical rescaling factor of 1.5 and a horizontal rescaling of 0.9 (for\n",
      "one of the grid cells), this will result in a predicted bounding box of size 150 × 45\n",
      "pixels. To be more precise, for each grid cell and each anchor box, the network\n",
      "predicts the log of the vertical and horizontal rescaling factors. Having these pri‐\n",
      "ors makes the network more likely to predict bounding boxes of the appropriate\n",
      "dimensions, and it also speeds up training since it will more quickly learn what\n",
      "reasonable bounding boxes look like.\n",
      "\n",
      "• Fourth, the network is trained using images of different scales: every few batches\n",
      "during  training,  the  network  randomly  chooses  a  new  image  dimension  (from\n",
      "330 × 330 to 608 × 608 pixels). This allows the network to learn to detect objects\n",
      "at  different  scales.  Moreover,  it  makes  it  possible  to  use  YOLOv3  at  different\n",
      "scales:  the  smaller  scale  will  be  less  accurate  but  faster  than  the  larger  scale,  so\n",
      "you can choose the right tradeoff for your use case.\n",
      "\n",
      "There are a few more innovations you might be interested in, such as the use of skip\n",
      "connections to recover some of the spatial resolution that is lost in the CNN (we will\n",
      "discuss this shortly when we look at semantic segmentation). Moreover, in the 2016\n",
      "paper,  the  authors  introduce  the  YOLO9000  model  that  uses  hierarchical  classifica‐\n",
      "tion: the model predicts a probability for each node in a visual hierarchy called Word‐\n",
      "Tree. This makes it possible for the network to predict with high confidence that an\n",
      "image represents, say, a dog, even though it is unsure what specific type of dog it is.\n",
      "\n",
      "476 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fSo I encourage you to go ahead and read all three papers: they are quite pleasant to\n",
      "read, and it is an excellent example of how Deep Learning systems can be incremen‐\n",
      "tally improved.\n",
      "\n",
      "Mean Average Precision (mAP)\n",
      "A very common metric used in object detection tasks is the mean Average Precision\n",
      "(mAP). “Mean Average” sounds a bit redundant, doesn’t it? To understand this met‐\n",
      "ric, let’s go back to two classification metrics we discussed in Chapter 3: precision and\n",
      "recall. Remember the tradeoff: the higher the recall, the lower the precision. You can\n",
      "visualize  this  in  a  Precision/Recall  curve  (see  Figure  3-5).  To  summarize  this  curve\n",
      "into a single number, we could compute its Area Under the Curve (AUC). But note\n",
      "that  the  Precision/Recall  curve  may  contain  a  few  sections  where  precision  actually\n",
      "goes up when recall increases, especially at low recall values (you can see this at the\n",
      "top left of Figure 3-5). This is one of the motivations for the mAP metric.\n",
      "\n",
      "Suppose the classifier has a 90% precision at 10% recall, but a 96% precision at 20%\n",
      "recall: there’s really no tradeoff here: it simply makes more sense to use the classifier\n",
      "at 20% recall rather than at 10% recall, as you will get both higher recall and higher\n",
      "precision.  So  instead  of  looking  at  the  precision  at  10%  recall,  we  should  really  be\n",
      "looking at the maximum precision that the classifier can offer with at least 10% recall.\n",
      "It would be 96%, not 90%. So one way to get a fair idea of the model’s performance is\n",
      "to  compute  the  maximum  precision  you  can  get  with  at  least  0%  recall,  then  10%\n",
      "recall,  20%,  and  so  on  up  to  100%,  and  then  calculate  the  mean  of  these  maximum\n",
      "precisions. This is called the Average Precision (AP) metric. Now when there are more\n",
      "than 2 classes, we can compute the AP for each class, and then compute the mean AP\n",
      "(mAP). That’s it!\n",
      "\n",
      "However,  in  an  object  detection  systems,  there  is  an  additional  level  of  complexity:\n",
      "what  if  the  system  detected  the  correct  class,  but  at  the  wrong  location  (i.e.,  the\n",
      "bounding box is completely off)? Surely we should not count this as a positive predic‐\n",
      "tion. So one approach is to define an IOU threshold: for example, we may consider\n",
      "that a prediction is correct only if the IOU is greater than, say, 0.5, and the predicted\n",
      "class is correct. The corresponding mAP is generally noted mAP@0.5 (or mAP@50%,\n",
      "or sometimes just AP50). In some competitions (such as the Pascal VOC challenge),\n",
      "this is what is done. In others (such as the COCO competition), the mAP is computed\n",
      "for  different  IOU  thresholds  (0.50,  0.55,  0.60,  …,  0.95),  and  the  final  metric  is  the\n",
      "mean of all these mAPs (noted AP@[.50:.95] or AP@[.50:0.05:.95]). Yes, that’s a mean\n",
      "mean average.\n",
      "\n",
      "Several YOLO implementations built using TensorFlow are available on github, some\n",
      "with pretrained weights. At the time of writing, they are based on TensorFlow 1, but\n",
      "by the time you read this, TF 2 implementations will certainly be available. Moreover,\n",
      "other object detection models are available in the TensorFlow Models project, many\n",
      "\n",
      "Object Detection \n",
      "\n",
      "| \n",
      "\n",
      "477\n",
      "\n",
      "\fwith pretrained weights, and some have even been ported to TF Hub, making them\n",
      "extremely easy to use, such as SSD29 and Faster-RCNN.30, which are both quite popu‐\n",
      "lar. SSD is also a “single shot” detection model, quite similar to YOLO, while Faster R-\n",
      "CNN is more complex: the image first goes through a CNN, and the output is passed\n",
      "to a Region Proposal Network (RPN) which proposes bounding boxes that are most\n",
      "likely to contain an object, and a classifier is run for each bounding box, based on the\n",
      "cropped output of the CNN.\n",
      "\n",
      "The  choice  of  detection  system  depends  on  many  factors:  speed,  accuracy,  available\n",
      "pretrained models, training time, complexity, etc. The papers contain tables of met‐\n",
      "rics, but there is quite a lot of variability in the testing environments, and the technol‐\n",
      "ogies evolve so fast that it is difficulty to make a fair comparison that will be useful for\n",
      "most people and remain valid for more than a few months.\n",
      "\n",
      "Great!  So  we  can  locate  objects  by  drawing  bounding  boxes  around  them.  But  per‐\n",
      "haps you might want to be a bit more precise. Let’s see how to go down to the pixel\n",
      "level.\n",
      "\n",
      "Semantic Segmentation\n",
      "In semantic segmentation, each pixel is classified according to the class of the object it\n",
      "belongs to (e.g., road, car, pedestrian, building, etc.), as shown in Figure 14-26. Note\n",
      "that different objects of the same class are not distinguished. For example, all the bicy‐\n",
      "cles on the right side of the segmented image end up as one big lump of pixels. The\n",
      "main difficulty in this task is that when images go through a regular CNN, they grad‐\n",
      "ually lose their spatial resolution (due to the layers with strides greater than 1): so a\n",
      "regular CNN may end up knowing that there’s a person in the image, somewhere in\n",
      "the bottom left of the image, but it will not be much more precise than that.\n",
      "\n",
      "29 “SSD: Single Shot MultiBox Detector,” Wei Liu et al. (2015).\n",
      "\n",
      "30 “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,” Shaoqing Ren et al.\n",
      "\n",
      "(2015).\n",
      "\n",
      "478 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fFigure 14-26. Semantic segmentation\n",
      "\n",
      "Just like for object detection, there are many different approaches to tackle this prob‐\n",
      "lem, some quite complex. However, a fairly simple solution was proposed in the 2015\n",
      "paper by Jonathan Long et al. we discussed earlier. They start by taking a pretrained\n",
      "CNN and turning into an FCN, as discussed earlier. The CNN applies a stride of 32 to\n",
      "the input image overall (i.e., if you add up all the strides greater than 1), meaning the\n",
      "last layer outputs feature maps that are 32 times smaller than the input image. This is\n",
      "clearly too coarse, so they add a single upsampling layer that multiplies the resolution\n",
      "by 32. There are several solutions available for upsampling (increasing the size of an\n",
      "image), such as bilinear interpolation, but it only works reasonably well up to ×4 or\n",
      "×8.  Instead,  they  used  a  transposed  convolutional  layer:31  it  is  equivalent  to  first\n",
      "stretching the image by inserting empty rows and columns (full of zeros), then per‐\n",
      "forming a regular convolution (see Figure 14-27). Alternatively, some people prefer to\n",
      "think  of  it  as  a  regular  convolutional  layer  that  uses  fractional  strides  (e.g.,  1/2  in\n",
      "Figure 14-27). The transposed convolutional layer can be initialized to perform some‐\n",
      "thing close to linear interpolation, but since it is a trainable layer, it will learn to do\n",
      "better during training.\n",
      "\n",
      "31 This type of layer is sometimes referred to as a deconvolution layer, but it does not perform what mathemati‐\n",
      "\n",
      "cians call a deconvolution, so this name should be avoided.\n",
      "\n",
      "Semantic Segmentation \n",
      "\n",
      "| \n",
      "\n",
      "479\n",
      "\n",
      "\fFigure 14-27. Upsampling Using a Transpose Convolutional Layer\n",
      "\n",
      "In a transposed convolution layer, the stride defines how much the\n",
      "input will be stretched, not the size of the filter steps, so the larger\n",
      "the stride, the larger the output (unlike for convolutional layers or\n",
      "pooling layers).\n",
      "\n",
      "TensorFlow Convolution Operations\n",
      "TensorFlow also offers a few other kinds of convolutional layers:\n",
      "\n",
      "• keras.layers.Conv1D creates a convolutional layer for 1D inputs, such as time\n",
      "\n",
      "series or text (sequences of letters or words), as we will see in ???.\n",
      "\n",
      "• keras.layers.Conv3D  creates  a  convolutional  layer  for  3D  inputs,  such  as  3D\n",
      "\n",
      "PET scan.\n",
      "\n",
      "• Setting the dilation_rate hyperparameter of any convolutional layer to a value\n",
      "of 2 or more creates an à-trous convolutional layer (“à trous” is French for “with\n",
      "holes”). This is equivalent to using a regular convolutional layer with a filter dila‐\n",
      "ted by inserting rows and columns of zeros (i.e., holes). For example, a 1 × 3 filter\n",
      "equal to [[1,2,3]] may be dilated with a dilation rate of 4, resulting in a dilated\n",
      "filter [[1, 0, 0, 0, 2, 0, 0, 0, 3]]. This allows the convolutional layer to\n",
      "\n",
      "480 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\fhave a larger receptive field at no computational price and using no extra param‐\n",
      "eters.\n",
      "\n",
      "• tf.nn.depthwise_conv2d() can be used to create a depthwise convolutional layer\n",
      "(but  you  need  to  create  the  variables  yourself).  It  applies  every  filter  to  every\n",
      "individual input channel independently. Thus, if there are fn filters and fn′ input\n",
      "channels, then this will output fn × fn′ feature maps.\n",
      "\n",
      "This solution is okay, but still too imprecise. To do better, the authors added skip con‐\n",
      "nections from lower layers: for example, they upsampled the output image by a factor\n",
      "of 2 (instead of 32), and they added the output of a lower layer that had this double\n",
      "resolution. Then they upsampled the result by a factor of 16, leading to a total upsam‐\n",
      "pling  factor  of  32  (see  Figure  14-28).  This  recovered  some  of  the  spatial  resolution\n",
      "that was lost in earlier pooling layers. In their best architecture, they used a second\n",
      "similar  skip  connection  to  recover  even  finer  details  from  an  even  lower  layer:  in\n",
      "short, the output of the original CNN goes through the following extra steps: upscale\n",
      "×2, add the output of a lower layer (of the appropriate scale), upscale ×2, add the out‐\n",
      "put  of  an  even  lower  layer,  and  finally  upscale  ×8.  It  is  even  possible  to  scale  up\n",
      "beyond the size of the original image: this can be used to increase the resolution of an\n",
      "image, which is a technique called super-resolution.\n",
      "\n",
      "Figure 14-28. Skip layers recover some spatial resolution from lower layers\n",
      "\n",
      "Once  again,  many  github  repositories  provide  TensorFlow  implementations  of\n",
      "semantic segmentation (TensorFlow 1 for now), and you will even find a pretrained\n",
      "instance  segmentation  model  in  the  TensorFlow  Models  project.  Instance  segmenta‐\n",
      "tion  is  similar  to  semantic  segmentation,  but  instead  of  merging  all  objects  of  the\n",
      "same  class  into  one  big  lump,  each  object  is  distinguished  from  the  others  (e.g.,  it\n",
      "identifies each individual bicycle). At the present, they provide multiple implementa‐\n",
      "tions  of  the  Mask  R-CNN  architecture,  which  was  proposed  in  a  2017  paper:  it\n",
      "extends  the  Faster  R-CNN  model  by  additionally  producing  a  pixel-mask  for  each\n",
      "bounding box. So not only do you get a bounding box around each object, with a set\n",
      "of  estimated  class  probabilities,  you  also  get  a  pixel  mask  that  locates  pixels  in  the\n",
      "bounding box that belong to the object.\n",
      "\n",
      "Semantic Segmentation \n",
      "\n",
      "| \n",
      "\n",
      "481\n",
      "\n",
      "\fAs you can see, the field of Deep Computer Vision is vast and moving fast, with all\n",
      "sorts of architectures popping out every year, all based on Convolutional Neural Net‐\n",
      "works. The progress made in just a few years has been astounding, and researchers\n",
      "are now focusing on harder and harder problems, such as adversarial learning (which\n",
      "attempts to make the network more resistant to images designed to fool it), explaina‐\n",
      "bility (understanding why the network makes a specific classification), realistic image\n",
      "generation (which we will come back to in ???), single-shot learning (a system that can\n",
      "recognize an object after it has seen it just once), and much more. Some even explore\n",
      "completely  novel  architectures,  such  as  Geoffrey  Hinton’s  capsule  networks32  (I  pre‐\n",
      "sented them in a couple videos, with the corresponding code in a notebook). Now on\n",
      "to the next chapter, where we will look at how to process sequential data such as time\n",
      "series using Recurrent Neural Networks and Convolutional Neural Networks.\n",
      "\n",
      "Exercises\n",
      "\n",
      "1. What are the advantages of a CNN over a fully connected DNN for image classi‐\n",
      "\n",
      "fication?\n",
      "\n",
      "2. Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels,\n",
      "a stride of 2, and SAME padding. The lowest layer outputs 100 feature maps, the\n",
      "middle one outputs 200, and the top one outputs 400. The input images are RGB\n",
      "images of 200 × 300 pixels. What is the total number of parameters in the CNN?\n",
      "If  we  are  using  32-bit  floats,  at  least  how  much  RAM  will  this  network  require\n",
      "when making a prediction for a single instance? What about when training on a\n",
      "mini-batch of 50 images?\n",
      "\n",
      "3. If your GPU runs out of memory while training a CNN, what are five things you\n",
      "\n",
      "could try to solve the problem?\n",
      "\n",
      "4. Why  would  you  want  to  add  a  max  pooling  layer  rather  than  a  convolutional\n",
      "\n",
      "layer with the same stride?\n",
      "\n",
      "5. When would you want to add a local response normalization layer?\n",
      "\n",
      "6. Can  you  name  the  main  innovations  in  AlexNet,  compared  to  LeNet-5?  What\n",
      "\n",
      "about the main innovations in GoogLeNet, ResNet, SENet and Xception?\n",
      "\n",
      "7. What is a Fully Convolutional Network? How can you convert a dense layer into\n",
      "\n",
      "a convolutional layer?\n",
      "\n",
      "8. What is the main technical difficulty of semantic segmentation?\n",
      "\n",
      "9. Build your own CNN from scratch and try to achieve the highest possible accu‐\n",
      "\n",
      "racy on MNIST.\n",
      "\n",
      "32 “Matrix Capsules with EM Routing,” G. Hinton, S. Sabour, N. Frosst (2018).\n",
      "\n",
      "482 \n",
      "\n",
      "| \n",
      "\n",
      "Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "\n",
      "\f10. Use transfer learning for large image classification.\n",
      "\n",
      "a. Create a training set containing at least 100 images per class. For example, you\n",
      "could classify your own pictures based on the location (beach, mountain, city,\n",
      "etc.),  or  alternatively  you  can  just  use  an  existing  dataset  (e.g.,  from  Tensor‐\n",
      "Flow Datasets).\n",
      "\n",
      "b. Split it into a training set, a validation set and a test set.\n",
      "\n",
      "c. Build the input pipeline, including the appropriate preprocessing operations,\n",
      "\n",
      "and optionally add data augmentation.\n",
      "\n",
      "d. Fine-tune a pretrained model on this dataset.\n",
      "\n",
      "11. Go through TensorFlow’s DeepDream tutorial. It is a fun way to familiarize your‐\n",
      "self with various ways of visualizing the patterns learned by a CNN, and to gener‐\n",
      "ate art using Deep Learning.\n",
      "\n",
      "Solutions to these exercises are available in ???.\n",
      "\n",
      "Exercises \n",
      "\n",
      "| \n",
      "\n",
      "483\n",
      "\n",
      "\fAbout the Author\n",
      "\n",
      "Aurélien Géron is a Machine Learning consultant. A former Googler, he led the You‐\n",
      "Tube video classification team from 2013 to 2016. He was also a founder and CTO of\n",
      "Wifirst from 2002 to 2012, a leading Wireless ISP in France; and a founder and CTO\n",
      "of  Polyconseil  in  2001,  the  firm  that  now  manages  the  electric  car  sharing  service\n",
      "Autolib’.\n",
      "\n",
      "Before this he worked as an engineer in a variety of domains: finance (JP Morgan and\n",
      "Société  Générale),  defense  (Canada’s  DOD),  and  healthcare  (blood  transfusion).  He\n",
      "published a few technical books (on C++, WiFi, and internet architectures), and was\n",
      "a Computer Science lecturer in a French engineering school.\n",
      "\n",
      "A few fun facts: he taught his three children to count in binary with their fingers (up\n",
      "to 1023), he studied microbiology and evolutionary genetics before going into soft‐\n",
      "ware engineering, and his parachute didn’t open on the second jump.\n",
      "\n",
      "Colophon\n",
      "\n",
      "The animal on the cover of Hands-On Machine Learning with Scikit-Learn and Ten‐\n",
      "sorFlow is the fire salamander (Salamandra salamandra), an amphibian found across\n",
      "most  of  Europe.  Its  black,  glossy  skin  features  large  yellow  spots  on  the  head  and\n",
      "back,  signaling  the  presence  of  alkaloid  toxins.  This  is  a  possible  source  of  this\n",
      "amphibian’s  common  name:  contact  with  these  toxins  (which  they  can  also  spray\n",
      "short distances) causes convulsions and hyperventilation. Either the painful poisons\n",
      "or the moistness of the salamander’s skin (or both) led to a misguided belief that these\n",
      "creatures not only could survive being placed in fire but could extinguish it as well.\n",
      "\n",
      "Fire salamanders live in shaded forests, hiding in moist crevices and under logs near\n",
      "the pools or other freshwater bodies that facilitate their breeding. Though they spend\n",
      "most of their life on land, they give birth to their young in water. They subsist mostly\n",
      "on a diet of insects, spiders, slugs, and worms. Fire salamanders can grow up to a foot\n",
      "in length, and in captivity, may live as long as 50 years.\n",
      "\n",
      "The fire salamander’s numbers have been reduced by destruction of their forest habi‐\n",
      "tat and capture for the pet trade, but the greatest threat is the susceptibility of their\n",
      "moisture-permeable skin to pollutants and microbes. Since 2014, they have become\n",
      "extinct in parts of the Netherlands and Belgium due to an introduced fungus.\n",
      "\n",
      "Many of the animals on O’Reilly covers are endangered; all of them are important to\n",
      "the world. To learn more about how you can help, go to animals.oreilly.com.\n",
      "\n",
      "The cover image is from Wood’s Illustrated Natural History. The cover fonts are URW\n",
      "Typewriter and Guardian Sans. The text font is Adobe Minion Pro; the heading font\n",
      "is Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.\n",
      "\n",
      "\f\n"
     ]
    }
   ],
   "source": [
    "print(text_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sentence = 'Hi, Good morning everyone, hope you all are enjoying NLP concept'\n",
    "\n",
    "sent =  gTTS(my_sentence, lang='en')\n",
    "sent.save('english.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Language not supported: ka",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m my_sentence_kan \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mಕಲಾಪದ ವೇಳೆ ಅಶಿಸ್ತಿನ ವರ್ತನೆ: ಅಮಾನತುಗೊಂಡ 141 ಸಂಸದರು ಸಂಸತ್ತಿನ ಚೇಂಬರ್, ಲಾಬಿ, ಗ್ಯಾಲರಿಗಳ ಪ್ರವೇಶಿಸದಂತೆ ಲೋಕಸಭೆ ಸಚಿವಾಲಯ ಸುತ್ತೋಲೆ\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m kan_sent \u001b[38;5;241m=\u001b[39m \u001b[43mgTTS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_sentence_kan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mka\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m kan_sent\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkannada.mp3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gtts/tts.py:145\u001b[0m, in \u001b[0;36mgTTS.__init__\u001b[0;34m(self, text, tld, lang, slow, lang_check, pre_processor_funcs, tokenizer_func)\u001b[0m\n\u001b[1;32m    143\u001b[0m     langs \u001b[38;5;241m=\u001b[39m tts_langs()\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlang \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m langs:\n\u001b[0;32m--> 145\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLanguage not supported: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m lang)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    147\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;28mstr\u001b[39m(e), exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: Language not supported: ka"
     ]
    }
   ],
   "source": [
    "my_sentence_kan = 'ಕಲಾಪದ ವೇಳೆ ಅಶಿಸ್ತಿನ ವರ್ತನೆ: ಅಮಾನತುಗೊಂಡ 141 ಸಂಸದರು ಸಂಸತ್ತಿನ ಚೇಂಬರ್, ಲಾಬಿ, ಗ್ಯಾಲರಿಗಳ ಪ್ರವೇಶಿಸದಂತೆ ಲೋಕಸಭೆ ಸಚಿವಾಲಯ ಸುತ್ತೋಲೆ'\n",
    "kan_sent = gTTS(my_sentence_kan, lang='ka')\n",
    "kan_sent.save('kannada.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29099792, 0.19260854, 0.36909396, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.19260854, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.36909396, 0.19260854, 0.19260854, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.23558789, 0.23558789,\n",
       "        0.23558789, 0.23558789, 0.36909396, 0.36909396],\n",
       "       [0.27299625, 0.18069342, 0.        , 0.        , 0.34626113,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.18069342, 0.34626113, 0.        , 0.34626113, 0.        ,\n",
       "        0.        , 0.18069342, 0.18069342, 0.        , 0.34626113,\n",
       "        0.34626113, 0.        , 0.        , 0.22101399, 0.22101399,\n",
       "        0.22101399, 0.22101399, 0.        , 0.        ],\n",
       "       [0.        , 0.19860274, 0.        , 0.38058058, 0.        ,\n",
       "        0.        , 0.38058058, 0.38058058, 0.38058058, 0.        ,\n",
       "        0.19860274, 0.        , 0.38058058, 0.        , 0.        ,\n",
       "        0.        , 0.19860274, 0.19860274, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.24291966, 0.24291966, 0.        , 0.        ],\n",
       "       [0.        , 0.18561477, 0.        , 0.        , 0.        ,\n",
       "        0.35569186, 0.        , 0.        , 0.        , 0.35569186,\n",
       "        0.18561477, 0.        , 0.        , 0.        , 0.35569186,\n",
       "        0.        , 0.18561477, 0.18561477, 0.35569186, 0.        ,\n",
       "        0.        , 0.35569186, 0.35569186, 0.22703351, 0.22703351,\n",
       "        0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,2), binary=True)\n",
    "tfidf.fit_transform(df['text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.51082562 1.         1.91629073 1.91629073 1.91629073 1.91629073\n",
      " 1.91629073 1.91629073 1.91629073 1.91629073 1.         1.91629073\n",
      " 1.91629073 1.91629073 1.91629073 1.91629073 1.         1.\n",
      " 1.91629073 1.91629073 1.91629073 1.91629073 1.91629073 1.22314355\n",
      " 1.22314355 1.22314355 1.22314355 1.91629073 1.91629073]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.idf_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

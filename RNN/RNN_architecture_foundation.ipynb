{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 3)                 27        \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31 (124.00 Byte)\n",
      "Trainable params: 31 (124.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# SimpleRNN (Batchsize, timesteps, no of features)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(3, input_shape=(4,5))) # SimpleRNN(Batch_size, (TimeSteps, no_of_features))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.8110293 , -0.6211981 , -0.18416983],\n",
       "       [-0.34915495, -0.08607161, -0.02857989],\n",
       "       [-0.52748   ,  0.23798114,  0.5137809 ],\n",
       "       [ 0.22792763, -0.6575132 ,  0.6429303 ],\n",
       "       [-0.2554695 , -0.7696267 ,  0.27624768]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.get_weights()[0].shape)\n",
    "model.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.32521558,  0.3259725 ,  0.88768053],\n",
       "       [ 0.94243145, -0.03446494, -0.33261847],\n",
       "       [-0.07783061,  0.9447509 , -0.3184152 ]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.get_weights()[1].shape)\n",
    "model.get_weights()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.get_weights()[2].shape)\n",
    "model.get_weights()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.7860352 ],\n",
       "       [-0.02757668],\n",
       "       [-0.8455278 ]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.get_weights()[3].shape)\n",
    "model.get_weights()[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.get_weights()[4].shape)\n",
    "model.get_weights()[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/pawankumarkc/Documents/vscode_workspace/MLAlgo/RNN/RNN_architecture_foundation.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/pawankumarkc/Documents/vscode_workspace/MLAlgo/RNN/RNN_architecture_foundation.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39;49mget_weights()[\u001b[39m5\u001b[39;49m]\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pawankumarkc/Documents/vscode_workspace/MLAlgo/RNN/RNN_architecture_foundation.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39mget_weights()[\u001b[39m5\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(model.get_weights()[5].shape)\n",
    "model.get_weights()[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1 -  Integer encoding method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "documents = ['Good Morning', 'Hope you are all doing well', \n",
    "             'We are upgrading ourself in advance deep learning', 'India won the match', 'Fight between Israel and Hamaz',\n",
    "             'America always support Israel', 'Australia won the match against pakistan', 'Microchip company opened in 1984', 'Kohli Kohli', \n",
    "             'Modi Ji jai ho', 'Chandrayaan 3 landed in south moon part']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(oov_token='<kumar>') #Out of vocablury specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<kumar>': 1,\n",
       " 'in': 2,\n",
       " 'are': 3,\n",
       " 'won': 4,\n",
       " 'the': 5,\n",
       " 'match': 6,\n",
       " 'israel': 7,\n",
       " 'kohli': 8,\n",
       " 'good': 9,\n",
       " 'morning': 10,\n",
       " 'hope': 11,\n",
       " 'you': 12,\n",
       " 'all': 13,\n",
       " 'doing': 14,\n",
       " 'well': 15,\n",
       " 'we': 16,\n",
       " 'upgrading': 17,\n",
       " 'ourself': 18,\n",
       " 'advance': 19,\n",
       " 'deep': 20,\n",
       " 'learning': 21,\n",
       " 'india': 22,\n",
       " 'fight': 23,\n",
       " 'between': 24,\n",
       " 'and': 25,\n",
       " 'hamaz': 26,\n",
       " 'america': 27,\n",
       " 'always': 28,\n",
       " 'support': 29,\n",
       " 'australia': 30,\n",
       " 'against': 31,\n",
       " 'pakistan': 32,\n",
       " 'microchip': 33,\n",
       " 'company': 34,\n",
       " 'opened': 35,\n",
       " '1984': 36,\n",
       " 'modi': 37,\n",
       " 'ji': 38,\n",
       " 'jai': 39,\n",
       " 'ho': 40,\n",
       " 'chandrayaan': 41,\n",
       " '3': 42,\n",
       " 'landed': 43,\n",
       " 'south': 44,\n",
       " 'moon': 45,\n",
       " 'part': 46}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(documents)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('good', 1),\n",
       "             ('morning', 1),\n",
       "             ('hope', 1),\n",
       "             ('you', 1),\n",
       "             ('are', 2),\n",
       "             ('all', 1),\n",
       "             ('doing', 1),\n",
       "             ('well', 1),\n",
       "             ('we', 1),\n",
       "             ('upgrading', 1),\n",
       "             ('ourself', 1),\n",
       "             ('in', 3),\n",
       "             ('advance', 1),\n",
       "             ('deep', 1),\n",
       "             ('learning', 1),\n",
       "             ('india', 1),\n",
       "             ('won', 2),\n",
       "             ('the', 2),\n",
       "             ('match', 2),\n",
       "             ('fight', 1),\n",
       "             ('between', 1),\n",
       "             ('israel', 2),\n",
       "             ('and', 1),\n",
       "             ('hamaz', 1),\n",
       "             ('america', 1),\n",
       "             ('always', 1),\n",
       "             ('support', 1),\n",
       "             ('australia', 1),\n",
       "             ('against', 1),\n",
       "             ('pakistan', 1),\n",
       "             ('microchip', 1),\n",
       "             ('company', 1),\n",
       "             ('opened', 1),\n",
       "             ('1984', 1),\n",
       "             ('kohli', 2),\n",
       "             ('modi', 1),\n",
       "             ('ji', 1),\n",
       "             ('jai', 1),\n",
       "             ('ho', 1),\n",
       "             ('chandrayaan', 1),\n",
       "             ('3', 1),\n",
       "             ('landed', 1),\n",
       "             ('south', 1),\n",
       "             ('moon', 1),\n",
       "             ('part', 1)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good Morning',\n",
       " 'Hope you are all doing well',\n",
       " 'We are upgrading ourself in advance deep learning',\n",
       " 'India won the match',\n",
       " 'Fight between Israel and Hamaz',\n",
       " 'America always support Israel',\n",
       " 'Australia won the match against pakistan',\n",
       " 'Microchip company opened in 1984',\n",
       " 'Kohli Kohli',\n",
       " 'Modi Ji jai ho',\n",
       " 'Chandrayaan 3 landed in south moon part']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[9, 10],\n",
       " [11, 12, 3, 13, 14, 15],\n",
       " [16, 3, 17, 18, 2, 19, 20, 21],\n",
       " [22, 4, 5, 6],\n",
       " [23, 24, 7, 25, 26],\n",
       " [27, 28, 29, 7],\n",
       " [30, 4, 5, 6, 31, 32],\n",
       " [33, 34, 35, 2, 36],\n",
       " [8, 8],\n",
       " [37, 38, 39, 40],\n",
       " [41, 42, 43, 2, 44, 45, 46]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Representing words with tokens\n",
    "sequences = tokenizer.texts_to_sequences(documents)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9, 10,  0,  0,  0,  0,  0,  0],\n",
       "       [11, 12,  3, 13, 14, 15,  0,  0],\n",
       "       [16,  3, 17, 18,  2, 19, 20, 21],\n",
       "       [22,  4,  5,  6,  0,  0,  0,  0],\n",
       "       [23, 24,  7, 25, 26,  0,  0,  0],\n",
       "       [27, 28, 29,  7,  0,  0,  0,  0],\n",
       "       [30,  4,  5,  6, 31, 32,  0,  0],\n",
       "       [33, 34, 35,  2, 36,  0,  0,  0],\n",
       "       [ 8,  8,  0,  0,  0,  0,  0,  0],\n",
       "       [37, 38, 39, 40,  0,  0,  0,  0],\n",
       "       [41, 42, 43,  2, 44, 45, 46,  0]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying zero padding (Pre or post)\n",
    "from keras.utils import pad_sequences\n",
    "sequences = pad_sequences(sequences, padding='post')\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study - IMDB movie review sentiment classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, Embedding, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (25000,) (25000,) (25000,)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data()\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32])\n",
      " list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95])\n",
      " list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113])\n",
      " ...\n",
      " list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 86527, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 14532, 325, 725, 134, 15271, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 11656, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 26094, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 17793, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 14492, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 17793, 5, 27, 710, 117, 74936, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 17793, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 17793, 7750, 5, 4241, 18, 4, 8497, 13164, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 11027, 4, 3586, 22459])\n",
      " list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 21469, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 40691, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 29455, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 11418, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 21213, 12, 38, 84, 80, 124, 12, 9, 23])\n",
      " list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 12815, 270, 14437, 5, 16923, 12255, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 16553, 21, 27, 9685, 6139, 5, 29043, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 85010, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 70907, 10755, 544, 5, 383, 1271, 848, 1468, 12183, 497, 16876, 8, 1597, 8778, 19280, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])]\n"
     ]
    }
   ],
   "source": [
    "print(x_train) #Dataset already has tokenised data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 13,\n",
       " 244,\n",
       " 6,\n",
       " 87,\n",
       " 337,\n",
       " 7,\n",
       " 628,\n",
       " 2219,\n",
       " 5,\n",
       " 28,\n",
       " 285,\n",
       " 15,\n",
       " 240,\n",
       " 93,\n",
       " 23,\n",
       " 288,\n",
       " 549,\n",
       " 18,\n",
       " 1455,\n",
       " 673,\n",
       " 4,\n",
       " 241,\n",
       " 534,\n",
       " 3635,\n",
       " 8448,\n",
       " 20,\n",
       " 38,\n",
       " 54,\n",
       " 13,\n",
       " 258,\n",
       " 46,\n",
       " 44,\n",
       " 14,\n",
       " 13,\n",
       " 1241,\n",
       " 7258,\n",
       " 12,\n",
       " 5,\n",
       " 5,\n",
       " 51,\n",
       " 9,\n",
       " 14,\n",
       " 45,\n",
       " 6,\n",
       " 762,\n",
       " 7,\n",
       " 17802,\n",
       " 1309,\n",
       " 328,\n",
       " 5,\n",
       " 428,\n",
       " 2473,\n",
       " 15,\n",
       " 26,\n",
       " 1292,\n",
       " 5,\n",
       " 3939,\n",
       " 6728,\n",
       " 5,\n",
       " 1960,\n",
       " 279,\n",
       " 13,\n",
       " 92,\n",
       " 124,\n",
       " 803,\n",
       " 52,\n",
       " 21,\n",
       " 279,\n",
       " 14,\n",
       " 9,\n",
       " 43,\n",
       " 6,\n",
       " 762,\n",
       " 7,\n",
       " 595,\n",
       " 15,\n",
       " 16,\n",
       " 28911,\n",
       " 23,\n",
       " 4,\n",
       " 1071,\n",
       " 467,\n",
       " 4,\n",
       " 403,\n",
       " 7,\n",
       " 628,\n",
       " 2219,\n",
       " 8,\n",
       " 97,\n",
       " 6,\n",
       " 171,\n",
       " 3596,\n",
       " 99,\n",
       " 387,\n",
       " 72,\n",
       " 97,\n",
       " 12,\n",
       " 788,\n",
       " 15,\n",
       " 13,\n",
       " 161,\n",
       " 459,\n",
       " 44,\n",
       " 4,\n",
       " 3939,\n",
       " 1101,\n",
       " 173,\n",
       " 21,\n",
       " 69,\n",
       " 8,\n",
       " 401,\n",
       " 22239,\n",
       " 4,\n",
       " 481,\n",
       " 88,\n",
       " 61,\n",
       " 4731,\n",
       " 238,\n",
       " 28,\n",
       " 32,\n",
       " 11,\n",
       " 32,\n",
       " 14,\n",
       " 9,\n",
       " 6,\n",
       " 545,\n",
       " 1332,\n",
       " 766,\n",
       " 5,\n",
       " 203,\n",
       " 73,\n",
       " 28,\n",
       " 43,\n",
       " 77,\n",
       " 317,\n",
       " 11,\n",
       " 4,\n",
       " 22228,\n",
       " 953,\n",
       " 270,\n",
       " 17,\n",
       " 6,\n",
       " 3616,\n",
       " 13,\n",
       " 545,\n",
       " 386,\n",
       " 25,\n",
       " 92,\n",
       " 1142,\n",
       " 129,\n",
       " 278,\n",
       " 23,\n",
       " 14,\n",
       " 241,\n",
       " 46,\n",
       " 7,\n",
       " 158]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218\n",
      "189\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train[0]))\n",
    "print(len(x_train[1]))\n",
    "print(len(x_train[5]))  \n",
    "#Utterance lengths are varying (different timestamp), so padding is required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding is required as we have different lenghts/timestamps for each review\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "x_train = pad_sequences(x_train, padding='post')\n",
    "y_test = pad_sequences(x_test, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2494\n",
      "2494\n",
      "2494\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train[0]))\n",
    "print(len(x_train[1]))\n",
    "print(len(x_train[5]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform padding and cap the data to 100, to reduce the complexity (Data is lost here) -- Not recommended in PROD\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=200)\n",
    "y_test = pad_sequences(x_test, padding='post', maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train[0]))\n",
    "print(len(x_train[1]))\n",
    "print(len(x_train[5]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RNN model - Approach 1 (Interger Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_1 (SimpleRNN)    (None, 32)                1088      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1121 (4.38 KB)\n",
      "Trainable params: 1121 (4.38 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(32, input_shape = (200, 1), return_sequences=False)) #200 max vocab size and 1 sequence, return_sequences=Output at the end\n",
    "model.add(Dense(1, activation='sigmoid')) #1 Neuron as the review is either positive or negative \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1088"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1*32+ 32*32+ 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*1 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "779/782 [============================>.] - ETA: 0s - loss: 0.6934 - accuracy: 0.4930"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type list).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/pawankumarkc/Documents/vscode_workspace/MLAlgo/RNN/RNN_architecture_foundation.ipynb Cell 36\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pawankumarkc/Documents/vscode_workspace/MLAlgo/RNN/RNN_architecture_foundation.ipynb#X50sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAdam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/pawankumarkc/Documents/vscode_workspace/MLAlgo/RNN/RNN_architecture_foundation.ipynb#X50sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(x_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(x_test, y_test))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m     97\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2 - Embedding method (Dense representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, None, 2)           20000     \n",
      "                                                                 \n",
      " simple_rnn_2 (SimpleRNN)    (None, 32)                1120      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21153 (82.63 KB)\n",
      "Trainable params: 21153 (82.63 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000,2))   #Vocab size = 10K(random), each vocab has 2 dimention. Means 2 numbers togather rep a word\n",
    "model.add(SimpleRNN(32, return_sequences=False)) # Many to one scenario. Sentiment analysis (Many words, one sentiment) specified by return_sequences=F\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type list).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/pawankumarkc/Documents/vscode_workspace/MLAlgo/RNN/RNN_architecture_foundation.ipynb Cell 42\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pawankumarkc/Documents/vscode_workspace/MLAlgo/RNN/RNN_architecture_foundation.ipynb#Y102sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAdam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/pawankumarkc/Documents/vscode_workspace/MLAlgo/RNN/RNN_architecture_foundation.ipynb#Y102sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(x_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(x_test, y_test))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m     97\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
